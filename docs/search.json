[
  {
    "objectID": "posts/2025-08-06-intro-diet-diversity/index.html",
    "href": "posts/2025-08-06-intro-diet-diversity/index.html",
    "title": "Measuring Dietary Diversity using DHS data",
    "section": "",
    "text": "Meeting dietary needs in early life is vital for human development and well-being. Child malnutrition before 24 months of age is particularly impactful on physical and cognitive health and human capital in later life stages1,2.\nThe main three indicators measuring dietary adequacy, developed by the World Health Organization, are minimum dietary diversity (MDD), minimum meal frequency (MMF), and minimum acceptable diet (MAD). Meal frequency is a proxy for under-nutrition and dietary diversity represents whether the child’s diet provides adequate macro-nutrients, while MAD indicates both MDD and MMF are met. For the purposes of this post, we will focus on minimum dietary diversity.\nStudies have shown that minimum dietary diversity, a standard measurement developed by the World Health organization, is strongly correlated with multiple dimensions of nutritional status3,4. Minimum dietary diversity is defined as the consumption of foods from 5 out of 8 food groups in the past 24 hours. In many countries, less than a quarter of children meet the standard diet diversity guidelines, and factors such as climate change and conflict threaten to reduce that prevalence even further5. Evidence already indicates that climate change may correlate with reduced dietary diversity6.\nThis post is designed to introduce researchers to available data for measuring dietary diversity in order to study trends over time and associations with climate change-related events."
  },
  {
    "objectID": "posts/2025-08-06-intro-diet-diversity/index.html#minimum-dietary-diversity-definition",
    "href": "posts/2025-08-06-intro-diet-diversity/index.html#minimum-dietary-diversity-definition",
    "title": "Measuring Dietary Diversity using DHS data",
    "section": "Minimum dietary diversity definition",
    "text": "Minimum dietary diversity definition\nChildren aged 6 to 23 months who were fed 5 out of the following 8 food groups in the past day meet the minimum dietary diversity (MDD) guidelines:\n\nBreastmilk\nGrains, white/pale starchy roots, tubers, porridge, and plantains\nLegumes and nuts\nDairy products (infant formula, milk, yogurt, cheese)\nFlesh foods (meat, fish, poultry and liver/organ meats)\nEggs\nVitamin A rich fruits and vegetables\nOther fruits and vegetables\n\nIn “Indicators for assessing infant and young child feeding practices”7, minimum dietary diversity was defined as having received 4 out of 7 food groups. In 2017 this definition was changed to 5 out of 8 food groups as breastmilk was included as an additional food group8. This change was made to eliminate disparities in the indicator for breastfeeding compared with non-breastfeeding children.” (DHS Guide to Statistics version 7.2)\n##Calculating the MDD indicator using R\nWe will use Ethiopia DHS surveys from 2000 to 2019 as an illustrative example, loading in an IPUMS DHS extract and bringing in the necessary libraries for our research example later.\n\n# Load in necessary libraries\nlibrary(ipumsr)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(janitor)\n\n# Load IPUMS DHS extract\ndhs_microdata &lt;- read_ipums_micro(\n  ddi = \"data/dhs/idhs_00050.xml\",\n  data_file = \"data/dhs/idhs_00050.dat.gz\",\n  verbose = FALSE\n)\n\n\n\nThe DHS questionnaires modify the standard questionnaire to include culturally-specific food groups, and the DHS guide to statistics instructs analysts to include the country-specific food types into the appropriate food groups above. For example, products made from teff (Ethiopia) should be included in grain foods.\nFirst, we recode the DHS diet variables into indicators for the food groups outlined in the WHO dietary diversity guidelines.\n\n#Create indicators for whether the youngest child consumed something from each of the 8 food groups in the past 24 hours\n\n#a)Breastmilk\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(breastmilk = case_when(BRSFEDUR==95 ~ 1))\n\n#b)Grains, cereals, tubers, porridge, and teff products\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(grains = case_when(MAFEDCEREAL24H==1 | MAFEDGRAIN24H==1 | MAFEDTUBER24H==1 | MAFEDPORR24H==1 | MAFEDTEFF24H==1 ~ 1))\n\n#c)Legumes and nuts\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(nuts_legumes = case_when(MAFEDLEGUM24H==1 | MAFEDNUTS24H==1 ~ 1))\n\n#d)Dairy products (infant formula, milk, yogurt, cheese)\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(dairy = case_when(MAFEDFORM24H==1 | MAFEDGENMILK24H==1 | MAFEDCHEESE24H==1 | MAFEDYOGURT24H==1 ~ 1))\n\n#e)Flesh foods (meat, fish, poultry and liver/organ meats)\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(meat = case_when(MAFEDMEAT24H==1 | MAFEDORGAN24H==1 | MAFEDFISH24H==1 | MAFEDPROTEIN24H == 1 | MAFEDFISH24H==1 | MAFEDBIRD24H ==1 ~ 1))\n\n#f)Eggs\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(eggs = case_when(MAFEDEGG24H==1 ~ 1))\n\n#g)Vitamin A rich fruits and vegetables\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(vita = case_when(MAFEDYELVEG24H==1 | MAFEDGRNVEG24H==1 | MAFEDVITAFRUIT24H==1 ~ 1))\n\n#h)Other fruits and vegetables\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(othfrtveg = case_when(MAFEDOFRTVEG24H==1 ~ 1))\n\n#Calculate the number of food groups consumed in the past 24 hours\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(foodgroups = sum(breastmilk, grains, nuts_legumes, dairy, meat, eggs, vita, othfrtveg, na.rm = TRUE))\n\n#Create an indicator for whether the child consumed foods from 5 or more food groups and assign to 1; zero otherwise\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(MDD = case_when(foodgroups &gt;= 5 ~ 1, foodgroups &lt; 5 ~ 0))"
  },
  {
    "objectID": "posts/2025-08-06-intro-diet-diversity/index.html#changes-over-time",
    "href": "posts/2025-08-06-intro-diet-diversity/index.html#changes-over-time",
    "title": "Measuring Dietary Diversity using DHS data",
    "section": "Changes over time",
    "text": "Changes over time\nProtein sources, notably eggs, were indicated together in the supplemental feeding module in DHS surveys. Starting in 2005, some surveys were fielded with animal protein sources (meat, poultry, or fish) separate from a variable to indicate consumption of eggs. From this point on, eggs were counted as its own food group for calculating MDD. This limits our ability to create a comparable indicator between samples before and after 2005. If a comparable indicator is central to the goal of your analysis, you could combine eggs and other protein sources into one indicator, and count the number of food groups out of 7. Bear in mind that a count out of seven food groups is no longer minimum dietary diversity as currently defined by the World Health Organization, but the estimates will be comparable over time if universe differences are reconciled, as we will address below."
  },
  {
    "objectID": "posts/2025-08-06-intro-diet-diversity/index.html#differences-in-variable-sub-population",
    "href": "posts/2025-08-06-intro-diet-diversity/index.html#differences-in-variable-sub-population",
    "title": "Measuring Dietary Diversity using DHS data",
    "section": "Differences in variable sub-population",
    "text": "Differences in variable sub-population\nThe skip logic pattern for feeding variables differs across surveys, and causes the variable universe (or sub-population of children in this variable) to differ. Variable universes are empirically tested by IPUMS DHS and can be found on the Universe tab (see this video tutorial about IPUMS DHS variable documentation for more information) for each integrated variable on their website. Because of their wider availability across IPUMS DHS samples, we will focus on the MAFED* variables. Below is an example of universe differences over time:\n\nEthiopia 2000: Children born in the 5 years before the survey to women age 15-49 whose last-born child under age 5 is still alive.\n\n\nEthiopia 2016: Children born in the 5 years before the survey to women age 15-49 with at least one surviving child under age 2 living with them.\n\nThe 2000 Ethiopia DHS survey includes questions on food consumption for the last-born surviving child up to age 5. In Ethiopia 2016, the mother is asked about the food consumption of her last born child under age 2, if that child is living with her. This will mean for both samples that children under 5 years old who were not last born will still have a valid response in the MAFED* variables. However, in Ethiopia 2016, the response for all children older than 2 years or not-youngest children is in reference to their youngest sibling. In Ethiopia 2000, this variable will contain information on last-born children for all children up to age 5.\nBecause older children are included in the skip logic in Ethiopia 2000, the sub-population of youngest children will differ between these two samples and will not give us a comparable indicator.\nThere is also a difference introduced in Ethiopia 2016 that the last-born children must be living with the mother.\nWe can easily impose a consistent universe across these samples by restricting only to the definition of MDD and analyzing only children aged 6 to 23 months living with their mother.\n\n#Keep only children under 2 years old\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  dplyr::filter(KIDCURAGE &lt; 2)\n#Keep only children who live with their mother\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  dplyr::filter(KIDLIVESWITH == 10)\n#Drop children who are not alive\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  dplyr::filter(KIDALIVE == 1)\n#Restrict to youngest born children\ndhs_microdata &lt;- dhs_microdata %&gt;%\n  group_by(SAMPLE, CASEID) %&gt;%\n  filter(row_number() == 1 | CASEID != lag(CASEID))\n\nThe code above should be applied to Ethiopia 2000 to address both the difference in age restriction and the co-residence restriction to align with Ethiopia 2016. We would also apply the same code to Ethiopia 2016 to not include records of children who are not last-born, not living with their mother, and/or not alive. Without addressing the universe difference, we may observe a spurious trend of dietary diversity if there is a systematic difference between the number of food groups older last-born children eat compared to last-born children age 6 to 23 months, or whether the inclusion of children not living with the mother in the 2000 survey might influence the estimate."
  },
  {
    "objectID": "posts/2025-08-06-intro-diet-diversity/index.html#creating-a-comparable-count-of-food-groups-over-time",
    "href": "posts/2025-08-06-intro-diet-diversity/index.html#creating-a-comparable-count-of-food-groups-over-time",
    "title": "Measuring Dietary Diversity using DHS data",
    "section": "Creating a comparable count of food groups over time",
    "text": "Creating a comparable count of food groups over time\nThe code below creates a modified count of food groups in which all protein sources are counted in one group so that the count of food groups is comparable between Ethiopia 2000 and subsequent surveys.\n\n#Create a variable that equals 1 if the child consumed any animal protein or egg in the past 24 hours\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(meateggs = case_when(MAFEDEGG24H==1 | MAFEDMEAT24H==1 | MAFEDORGAN24H==1 | MAFEDFISH24H==1 | MAFEDPROTEIN24H == 1 | MAFEDFISH24H==1 | MAFEDBIRD24H ==1 ~ 1))\n\n#Calculate the number of food groups the child consumed, with meat and eggs counting as one category.\n#The maximum number of food groups is seven.\ndhs_microdata &lt;- dhs_microdata %&gt;% \n  mutate(foodgroups_com = sum(breastmilk, grains, nuts_legumes, dairy, meateggs, vita, othfrtveg, na.rm = TRUE))\n\nAs a check for how significant the comparability issue could be, we can analyze cases in a sample that has separate indicators for meat and eggs. Upon closer inspection, the difference between the comparable count of food groups and the minimum dietary diversity definition does not differ greatly for Ethiopia 2005 - less than 1% of cases included children who had eaten both eggs and a different protein source. This may not be true in other contexts.\n\n#Load in the labelled library to create a helpfully-labelled cross tabulation\nlibrary(labelled)\n#Create a cross tabulation of the indicator for meat consumption and the indicator for egg consumption for Ethiopia 2005\ndhs_microdata %&gt;%\n  mutate(\n    MAFEDEGG24H = to_factor(MAFEDEGG24H),\n  ) %&gt;%\n  filter(SAMPLE == 23102) %&gt;%\n  tabyl(MAFEDEGG24H, meat, show_na = TRUE) %&gt;%\n  adorn_percentages(\"all\") %&gt;%\n  adorn_pct_formatting()\n#&gt;            MAFEDEGG24H    1   NA_\n#&gt;                     No 4.3% 91.0%\n#&gt;                    Yes 0.6%  3.9%\n#&gt;             Don't know 0.0%  0.1%\n#&gt;                Missing 0.0%  0.1%\n#&gt;  NIU (not in universe) 0.0%  0.0%"
  },
  {
    "objectID": "posts/2025-08-06-intro-diet-diversity/index.html#mapping-dietary-diversity-over-time",
    "href": "posts/2025-08-06-intro-diet-diversity/index.html#mapping-dietary-diversity-over-time",
    "title": "Measuring Dietary Diversity using DHS data",
    "section": "Mapping dietary diversity over time",
    "text": "Mapping dietary diversity over time\nUsing workflows described elsewhere in this blog (see our previous post as an example), we utilize spatially-harmonized shapefiles for Ethiopia to demonstrate comparable changes in indicators over time and across space. For more information on spatial harmonization, see another previous post. The shapefiles are available to download from the IPUMS DHS website.\nWe can use the comparable count of food groups consumed that we just calculated from DHS surveys conducted in Ethiopia in 2000, 2005, 2011, 2016, and 2019 to analyze patterns in dietary diversity over time in spatially-harmonized subnational regions. We’ll use the person-level weight PERWEIGHT to get weighted estimates representative at the region level and create a new dataframe called foodgroups_by_region.\n\n\nIn this post, we will map first level administrative boundaries. Second-level administrative areas have been predicted using GPS coordinates of DHS clusters, though the variable GEOLEV2 should be used only to connect contextual variables calculated using IPUMS International, and not to calculate estimates at the second-level unit level.\n\nfoodgroups_by_region &lt;- dhs_microdata %&gt;%\n  group_by(sample = as.factor(SAMPLE), geo = as.factor(GEO_ET2000_2019)) %&gt;%\n  summarise(average_food_groups_com = weighted.mean(foodgroups_com, w = PERWEIGHT, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n#Read in the spationally-harmonized Ethiopia shapefiles\nethiopia_shapefile &lt;- read_ipums_sf(\"data/gps/geo_et2000_2019.zip\")\n#&gt; options:        ENCODING=CP1252 \n#&gt; Reading layer `geo_et2000_2019' from data source \n#&gt;   `C:\\Users\\krist108\\AppData\\Local\\Temp\\RtmpENv9jB\\file4cac5b877ecf\\geo_et2000_2019.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 12 features and 3 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 32.99773 ymin: 3.404137 xmax: 48.00106 ymax: 14.89421\n#&gt; Geodetic CRS:  WGS 84\n\n#ATTACH SHAPEFILE TO FOOD GROUPS DATA BY REGION ----\nfoodgroups_by_region &lt;- foodgroups_by_region %&gt;% \n  mutate(DHSCODE = as.integer(geo))\ndhs_shapefile_merge &lt;- left_join(ethiopia_shapefile, foodgroups_by_region, join_by(DHSCODE), relationship=\"one-to-many\")\n\ndhs_shapefile_merge &lt;- subset(dhs_shapefile_merge, !is.na(sample))\n\nWe then use the ggplot2 library and the facet_wrap function to create a map of weighted average food groups consumed by young children by subnational regions of Ethiopia over time.\n\n\nCode\ndhs_shapefile_merge &lt;- dhs_shapefile_merge %&gt;%\n  mutate(\n    sample = to_factor(sample)\n  )\ndhs_shapefile_merge &lt;- dhs_shapefile_merge %&gt;%\n  mutate(\n    sample = case_when(\n    sample == 23101 ~ \"Ethiopia 2000\", \n    sample == 23102 ~ \"Ethiopia 2005\",\n    sample == 23103 ~ \"Ethiopia 2011\",\n    sample == 23104 ~ \"Ethiopia 2016\", \n    sample == 23105 ~ \"Ethiopia 2019\")\n  )    \n\nethiopia_maps &lt;- ggplot(data = dhs_shapefile_merge) +\n  geom_sf(aes(fill = average_food_groups_com)) + \n  facet_wrap(vars(sample)) +\n  scale_fill_viridis_c(direction = -1, name=\"Average food groups consumed\") +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.x = element_blank(),\n    axis.ticks.y = element_blank()\n  ) +\n  ggtitle(\"Regional average food groups consumed by children \\n aged 6 to 23 months over time in Ethiopia\") \n\nethiopia_maps\n\n\n\n\n\n\n\n\n\nNotice that the average count of food groups consumed differs vary little across subnational regions in 2000, but the estimates begin to decrease and diverge after 2005. The decline in dietary diversity in the easternmost region, Somali, by 2019 is particularly noticeable. One potential explanation is that a combination of erratic rainfall, internal displacement due to conflict, and high food prices in Somali and neighboring regions could have contributed to food scarcity leading up to this DHS survey9.\nCheck back soon for a related post on using dietary diversity indicators in a research example!"
  },
  {
    "objectID": "posts/2025-04-23-forecasting-pt1/index.html",
    "href": "posts/2025-04-23-forecasting-pt1/index.html",
    "title": "Anticipating the Future: Important terminology",
    "section": "",
    "text": "Climate and social scientists study past conditions to establish links between environmental conditions and human behavior and health. Once these patterns are identified, the interest may shift toward understanding what they imply about the evolution of our health outcomes as conditions change in the future.\nOf course, studying future outcomes is no easy task, as it’s impossible to make concrete observations of phenomena that haven’t happened yet! Still, we often have enough information to make informed judgments about what is likely to happen in the future.\nThis is the first in a series of posts that will discuss some of the complexities involved when attempting to anticipate future conditions. In this post, we’ll establish the foundational terminology that will be critical to understanding the concepts we introduce going forward."
  },
  {
    "objectID": "posts/2025-04-23-forecasting-pt1/index.html#scenarios",
    "href": "posts/2025-04-23-forecasting-pt1/index.html#scenarios",
    "title": "Anticipating the Future: Important terminology",
    "section": "Scenarios",
    "text": "Scenarios\nScenarios are the result of thought experiments or models that incorporate information from a small subset of a multitude of events that could occur. A scenario is a postulated sequence of events, which considers the possible evolution of the events under consideration. For instance, a scenario may consider the likely actions or decisions made by governments or individuals within specific countries or organizations.\nNotably, scenarios do not affirm that these actions or decisions will actually happen. Rather, they intend to illustrate the potential ramifications of risks and uncertainties and highlight the possible effects of government decisions and policy. Thus, scenarios can help decision makers think through the potential consequences of their decisions and consider alternatives that may improve outcomes.\nFor example, Lloyd’s of London is a market regulator for insurance providers and those that buy and sell insured risk in a global marketplace. Lloyd’s maintains a set of Realistic Disaster Scenarios (RDS) to stress test both individual syndicates and the market as a whole. The disasters within these scenarios include climate events (e.g., earthquakes, floods, and windstorms in very high value markets in the US, Japan, UK, and Europe) as well as terrorism and cyber attack events. If an earthquake occurs in the San Francisco area, for instance, Lloyd’s has estimated the losses for residential and commercial property insurance to be around $80 billion. The scenarios provide ways to ensure the stability of the reinsurance market and to reduce exposure by any one risk holder.\nThe United States government has a sea level rise tool, which provides a set of scenarios for the likely amount of sea level rise that will be experienced in different coastal areas. The five scenarios consider a variety of processes that could influence sea level across a wide range of future warming conditions—specifically, the amount of greenhouse gases emitted due to economic activity. They are defined by target values of Global Mean Sea Level Rise in 2100. These scenarios draw a direct connection between policy and action on climate mitigation and the amount of area along the US coastline which will likely be inundated by 2100.\nThe Famine Early Warning System Network (FEWS NET) used scenarios to estimate future food insecurity four months into the future through analysis of rainfall, harvests, commodity prices, seasonal labor demand, government policy, international aid, and other factors (see Figure 1). The scenarios relied on analysis of the current situation, assumptions about future changes, and the likely response to events by various actors. Updated every month, the scenarios provided members in the network guidance on appropriate responses to local shocks, like drought or conflict.\n\n\n\n\n\n\nFigure 1: FEWS NET projection period updates.\n\n\n\nPopulation scientists can employ scenarios when estimating future population counts as well. Typically, population scenarios are constructed by assuming constant, higher, or lower rates of core population processes, like fertility, migration, and mortality. Our World in Data has a tool for users to create population projections based on fertility scenarios drafted by the United Nations."
  },
  {
    "objectID": "posts/2025-04-23-forecasting-pt1/index.html#forecasts",
    "href": "posts/2025-04-23-forecasting-pt1/index.html#forecasts",
    "title": "Anticipating the Future: Important terminology",
    "section": "Forecasts",
    "text": "Forecasts\nUnlike scenarios, which are estimates based on assumptions about context and response to events in the future, forecasts are model predictions based on current and historical data. Forecasts are always evaluated for their accuracy and predictive ability using quantitative metrics. For example, the accuracy of a weather forecast can be evaluated by comparing the prediction to the observation dataset, allowing the forecast users to know whether the forecast was able to predict events that occurred.\nSubseasonal to seasonal (S2S) forecasts extend short-term weather forecasts from two weeks to two years into the future. While raw forecasts from S2S models are informative, calibration is necessary to correct systematic biases and quantify forecast uncertainty to facilitate applications such as scheduling agricultural activities or planning outdoor events. Most S2S forecasts can predict that it will be warm in summer and cool in winter, but being able to forecast significantly above or below average temperatures in a particular month is far more challenging.\n\n\n\n\nFigure 2. Seasonal temperature outlook for the United States, issued October 2024.\n\n\n\nPopulation scientists create population forecasts—predicted population counts—using various methods depending on the amount of input information available. The process of creating a population forecast is called population projection. There is some debate on whether population projections should be considered true forecasts, however, and most population scientists refer to their projections as “estimates” to showcase their professional caution.1,2"
  },
  {
    "objectID": "posts/2025-04-23-forecasting-pt1/index.html#predictions",
    "href": "posts/2025-04-23-forecasting-pt1/index.html#predictions",
    "title": "Anticipating the Future: Important terminology",
    "section": "Predictions",
    "text": "Predictions\nPredictions result from the expert analysis of information that integrates knowledge from multiple disciplines. The term “prediction” has the connotation of being less rigorous as it focuses on expert knowledge and is based on flexible assumptions. Predictions may also consider hypothetical conditions such as policy, politics or economic changes.\nForecasts, in contrast, often represent events that are likely to occur regardless of near-term government or individual actions, and are checked for accuracy once future conditions have been observed. However, there often is no direct way to determine the accuracy of a prediction due to incomplete information on the system and the lack of a specific model used in its creation.\nFor example, a government may decide to invest in subsidized fertilizer for smallholders, but this may not affect the price of commodities in the market in the current year given the impacts of broader macroeconomic forces and immediate supply and demand. Forecasts of drought impacts on crop production in the current year can’t be influenced by agricultural policies made by governments, since those investments take time to affect productivity. However, experts could still make predictions about the likely long-term impact of the investments."
  },
  {
    "objectID": "posts/2024-12-06-qualitative-methods/index.html",
    "href": "posts/2024-12-06-qualitative-methods/index.html",
    "title": "Incorporating Qualitative Methods into Spatial Health Research",
    "section": "",
    "text": "Understanding how extreme weather conditions impact people and communities is complex. Moreover, we’re not only interested in the overall impact of extreme weather on a population (for example, by identifying where droughts or heat waves occur), but also in which groups of people suffer disproportionately under these impacts. That is, when a drought occurs in a given community, who within that community experiences the most severe adverse outcomes?\nOf course, we are also concerned with why these differences emerge. As we’ve described in posts throughout this blog (for instance, here, here, and here), answering this question requires careful attention not only to the particular conditions of place and of persons within that place, but also to the technical details involved in spatial and temporal aggregation.\nIn other words, we must consider who a person is and how they live in their space, and we must also design measures that capture variability in exposures and conditions that make effective use of the data we have."
  },
  {
    "objectID": "posts/2024-12-06-qualitative-methods/index.html#measuring-person-level-factors-in-the-dhs",
    "href": "posts/2024-12-06-qualitative-methods/index.html#measuring-person-level-factors-in-the-dhs",
    "title": "Incorporating Qualitative Methods into Spatial Health Research",
    "section": "Measuring person-level factors in the DHS",
    "text": "Measuring person-level factors in the DHS\nIn the DHS, key measures that address an individual’s exposure are often their length of time in current residence and type of employment. These measures help to provide insight into what a person is used to dealing with and how sensitive their livelihoods are to environmental events.\n\n\nYou can use IPUMS DHS to browse groups of variables related to a given topic, like residency and employment.\nFor example, in an area prone to inconsistent rainfall, an established farmer who has lived there for the duration of their life is likely accustomed to this variability and less susceptible to a poor rainfall year than a farmer new to the community. The latter individual might face greater risks in the event of a poor rainy season given their lack of experience navigating or identifying those specific risks, normal to that place.\nHowever, both of these individuals are likely more susceptible to adverse impacts on their lifestyles from a poor rainfall year than individuals in the community with less dependence on rainfall to generate food or income. Experience interacts with employment, as well: a non-farmer who is a long-time resident of the community may identify poor rainy conditions in very different ways than a farmer, regardless of their experience navigating weather conditions in that community.\nThus, considering an individual’s time spent living in a place alongside how that individual lives and works in a place helps to provide important information about why individual experiences vary in response to extreme weather events."
  },
  {
    "objectID": "posts/2024-12-06-qualitative-methods/index.html#getting-help",
    "href": "posts/2024-12-06-qualitative-methods/index.html#getting-help",
    "title": "Incorporating Qualitative Methods into Spatial Health Research",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html",
    "href": "posts/2024-09-30-leaflet-pt2/index.html",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "",
    "text": "In our previous post, we introduced the {leaflet} package along with the GHS-SMOD data source for measuring population density. When we left off, we had loaded the GHS data and prepared it for mapping with leaflet.\nThis post will use the prepared data (ghs_smod) and the basemap we built in the last post to explore the functionality of leaflet in more depth.\nNeedless to say, if you haven’t yet taken a look at our previous post, you’ll want to check that out before continuing with this one!\n# Load relevant libraries\nlibrary(leaflet)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html#color-palettes",
    "href": "posts/2024-09-30-leaflet-pt2/index.html#color-palettes",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "Color palettes",
    "text": "Color palettes\nLeaflet’s palette functions depend on the type of data your raster contains. Because our data are categorical, we’ll use colorFactor() (categorical levels are often referred to as factors in R).\n\n\nFor numeric data, you might instead consider colorNumeric() or colorBin(). See the leaflet documentation for the available options.\nTo build a palette (which we call ghs_pal), we provide the set of colors that we want to use along with the domain of values that should be matched to each of these colors. This produces a palette function that takes a raster value as input and produces a corresponding color as output.\nIn our case, we’ll use an 8-color Inferno palette from the {colorspace} package. We then match each of these colors to one of the unique values in our raster:\n\n# Get color codes for the Inferno palette from colorspace\ncolors &lt;- colorspace::sequential_hcl(palette = \"inferno\", n = 8)\n\n# Map these colors to each unique raster value\nghs_pal &lt;- colorFactor(\n  colors,\n  c(10, 11, 12, 13, 21, 22, 23, 30),\n  na.color = \"transparent\" # Make missing values transparent\n)\n\nNow, we just provide our palette to the colors argument of our raster image layer:\n\nbasemap |&gt; \n  addRasterImage(ghs_smod[[1]], colors = ghs_pal)\n\n\n\n\n\n\nThis is starting to look more consistent with the style of our basemap.\nHowever, based on our color palette, areas of our raster that are coded as water are colored black. This is somewhat confusing, because our basemap uses black to represent land areas. (You can see in the Red Sea that the water transitions from gray to black at our raster boundary.) We can fix this by making the water areas of our raster transparent, so they show the basemap color instead.\nWe’ll also do the same for the “Very-low” density areas on land, making them show the black basemap color. This will help make the geographic context provided by our basemap more apparent and will help the data blend into the overall map.\n\n\n\n\n\n\nCaution\n\n\n\nNote that this conversion is intended for visualization purposes. For an analysis, we may not want to convert these values to NA!\n\n\nWe can use terra’s subst() to substitute these categories with missing values:\n\nghs_smod[[1]] &lt;- subst(ghs_smod[[1]], c(\"Water\", \"Very low\"), NA)\nghs_smod[[2]] &lt;- subst(ghs_smod[[2]], c(\"Water\", \"Very low\"), NA)\n\nFinally, let’s mask out the raster data outside of the Ethiopia borders to focus on the data we’re most interested in. We’ve downloaded 2016 Ethiopia boundary files from the DHS Program’s Spatial Data Repository and stored them in our data directory.\nFirst, we’ll load our data with {sf}:\n\net_border &lt;- st_read(\n  \"data/sdr_subnational_boundaries_2024-08-26/shps/sdr_subnational_boundaries.shp\",\n  quiet = TRUE\n) \n\nThen, we’ll dissolve away the internal boundaries to get a single country-level border and project to Web Mercator for consistency with our raster data.\n\n# Dissolve internal boundaries to get external border only\net_border &lt;- st_union(et_border)\n\n# Transform to Web Mercator for consistency with basemap and raster\net_border &lt;- st_transform(et_border, \"epsg:3857\")\n\nNow, we can mask out the raster values outside the country border with terra’s mask():\n\n# Mask out raster values outside of border\nghs_smod &lt;- mask(ghs_smod, vect(et_border))\n\nbasemap |&gt; \n  addRasterImage(ghs_smod[[1]], colors = ghs_pal)"
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html#legends",
    "href": "posts/2024-09-30-leaflet-pt2/index.html#legends",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "Legends",
    "text": "Legends\nOur map looks nice, but it’s hard to interpret without a legend. We can add one with addLegend().\nTo produce a legend, we again need to use our color palette to indicate the colors and labels we want to include in the legend. Since we’ve excluded water and very-low density areas from the map, we don’t want these to show in the legend. We’ll use our palette (ghs_pal) to generate the colors for the other density categories. (Recall that ghs_pal is a function that takes raster values in its domain and outputs the corresponding colors that we set earlier.)\nThen, we’ll manually input the desired labels that should correspond to these colors.\n\nlegend_colors &lt;- ghs_pal(c(12, 13, 21, 22, 23, 30))\nlegend_labels &lt;- c(\"Low\", \"Rural\", \"Suburban\", \"Semi-dense urban\", \"Dense urban\", \"Urban center\")\n\nNow, we provide this info to addLegend():\n\nbasemap |&gt; \n  addRasterImage(ghs_smod[[1]], colors = ghs_pal) |&gt; \n  addLegend(\n    title = \"GHS-SMOD Population Density\",\n    position = \"bottomright\",\n    colors = legend_colors,\n    labels = legend_labels,\n    opacity = 0.7\n  )"
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html#controlling-layer-visibility",
    "href": "posts/2024-09-30-leaflet-pt2/index.html#controlling-layer-visibility",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "Controlling layer visibility",
    "text": "Controlling layer visibility\nSo far, our map might as well be a static map. However, the utility of the interactive features of our map will be more apparent once we add more data to our map.\nIf we want to compare 2010 and 2020 data on the same map, for instance, we’ll need to add a toggle that allows us to selectively show and hide particular layers. To accomplish this, we’ll add layer group names to each of our map’s raster layers. We can then reference these names when we indicate which layers we want to be able to toggle on and off.\nTo add a layer toggle, we can use addLayersControl() along with the group names we’ve defined for each of our layers:\n\net_ghs_pop_map &lt;- basemap |&gt; \n  addRasterImage(\n    ghs_smod[[1]], \n    colors = ghs_pal,\n    group = \"GHS-SMOD 2010\" # Group name to reference 2010 layer\n  ) |&gt; \n  addRasterImage(\n    ghs_smod[[2]], \n    colors = ghs_pal,\n    group = \"GHS-SMOD 2020\" # Group name to reference 2020 layer\n  ) |&gt; \n  addLayersControl(\n    baseGroups = c(\"GHS-SMOD 2010\", \"GHS-SMOD 2020\"), # Specify group names here to add layer control toggle\n    options = layersControlOptions(collapsed = FALSE) # We don't want the toggle menu to be collapsed by default\n  ) |&gt; \n  addLegend(\n    title = \"GHSL Population Density\",\n    position = \"bottomright\",\n    colors = legend_colors,\n    labels = legend_labels,\n    opacity = 0.7\n  )\n\net_ghs_pop_map\n\n\n\n\n\n\nNow we can turn layers on and off to easily see the difference across the two epochs! Try it yourself by switching between the two radio buttons in the top right of the map pane. You can also zoom in to see how specific areas have changed."
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html#dhs-cluster-data",
    "href": "posts/2024-09-30-leaflet-pt2/index.html#dhs-cluster-data",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "DHS cluster data",
    "text": "DHS cluster data\nWe can also plot other types of data on the same map. Let’s load the 2016 DHS clusters for Ethiopia, which we’ve saved in our data directory.\n\n\nIf you don’t remember, we’ve demonstrated where you can find cluster GPS coordinates from the DHS Program previously.\n\n# Load 2016 Ethiopia cluster locations\net_clust &lt;- ipumsr::read_ipums_sf(\"data/ETGE71FL.zip\")\n\n# Remove missing GPS coordinates\net_clust &lt;- et_clust |&gt; \n  filter(!(LATNUM == 0 & LONGNUM == 0))"
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html#comparing-dhs-and-ghs",
    "href": "posts/2024-09-30-leaflet-pt2/index.html#comparing-dhs-and-ghs",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "Comparing DHS and GHS",
    "text": "Comparing DHS and GHS\nDHS surveys typically provide an indication of whether a given cluster is considered to be urban or rural. However, this definition varies across surveys, so it may be beneficial to identify external datasets (like GHS-SMOD) that can serve as a consistent proxy for urbanness when doing a comparative analysis.\nWe can see how well the 2016 Ethiopia definition of urbanness compares to the population density estimates from GHS-SMOD by plotting the cluster locations on top of our population density raster.\nAll we need to do is to add a new data layer to the map we created above. Since the DHS cluster data represent point locations, not raster data, we’ll need a different layer function. In this case, we’ll use addCircleMarkers(), which adds point markers to our map.\nWe just need to provide our et_clust data to get markers at their GPS locations. We’ll also adjust some of the size, color, and transparency specifications to make the map more legible:\n\n\nIf you want to add circles with a radius based on map units (like meters), use addCircles() instead.\n\net_ghs_pop_map |&gt; \n  addCircleMarkers(\n    data = et_clust,\n    fillColor = \"black\",\n    opacity = 1,\n    fillOpacity = 0.5,\n    radius = 7,\n    weight = 2\n  )\n\n\n\n\n\n\nThis shows us the cluster locations, but doesn’t show us which clusters are defined as urban and which are defined as rural. To accomplish this, we need another color palette.\nWe’ll use colorFactor() again to define a mapping between the \"U\" and \"R\" values used in the et_clust data to indicate urban and rural status:\n\nclust_pal &lt;- colorFactor(\n  palette = c(\"#ffa076\", \"#54cabe\"),\n  levels = c(\"U\", \"R\")\n)\n\n\n\n\n\n\n\nHex Codes\n\n\n\nOur colors here are represented as hexadecimal codes, which is a common format for representing colors digitally. Hex codes contain 6 digits prefixed with a # sign. The first two digits indicate the amount of red in the color, the second two the amount of green, and the last two the amount of blue. Optionally, you can include an additional pair of codes which adjust the transparency of the color.\nThere are countless resources online to help you interactively pick colors and identify the corresponding hex codes, so you don’t need to memorize how the codes are interpreted.\n\n\nNow when we provide our data to addCircleMarkers(), we can indicate that we want to color by a particular variable in our data—in this case, URBAN_RURA. We need to pass the values of the variable to our color palette to generate a color for each cluster coordinate. In this case, leaflet uses ~ notation to indicate that we’re providing a variable name from our data to the color argument (as opposed to a single color name, like \"white\").\n\net_ghs_pop_clust_map &lt;- et_ghs_pop_map |&gt; \n  addCircleMarkers(\n    data = et_clust,\n    fillColor = \"black\",\n    color = ~ clust_pal(URBAN_RURA), # Adjust color based on values of URBAN_RURA variable\n    opacity = 1,\n    fillOpacity = 0.5,\n    radius = 7,\n    weight = 2\n  )\n\nOf course, we also want to add another legend to communicate what these new colors mean:\n\net_ghs_pop_clust_map |&gt; \n  addLegend(\n    title = \"DHS Cluster Status\",\n    position = \"bottomright\",\n    colors = clust_pal(c(\"U\", \"R\")), # Get colors for each level using our palette function\n    values = c(\"U\", \"R\"),\n    labels = c(\"Urban\", \"Rural\"),\n    opacity = 0.7\n  )\n\n\n\n\n\n\nExamining the map, we can see that most cluster locations seem to align with the population density estimates from GHS-SMOD.\nIf you look around the map, you might notice some clusters unexpectedly labeled as urban or rural, but remember that the GPS point locations have been displaced from their true positions, so a direct overlay on the map may be misleading.\nStill, building interactive maps during data exploration can make it much easier to check that two data sources are consistent with one another, identify unexpected cases worth exploring further, or even generate new ideas about how certain variables may be related."
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html#popups",
    "href": "posts/2024-09-30-leaflet-pt2/index.html#popups",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "Popups",
    "text": "Popups\nIf you do identify clusters that seem to have unexpected urban/rural classifications, you may want to investigate the data values for that cluster more closely. However, it’s impossible to identify a specific cluster ID simply by looking at the map.\nOne solution is to add a popup, which will produce a text bubble when a spatial feature is clicked. We’ll use the popup argument with the DHSID variable name to indicate that we want our popups to contain the information contained in the DHSID variable of our et_clust data. Again, we use the ~ notation to indicate that DHSID is the name of a variable in our data, not a standalone value:\n\net_ghs_pop_map |&gt; \n  addCircleMarkers(\n    data = et_clust,\n    fillColor = \"black\",\n    color = ~ clust_pal(URBAN_RURA),\n    opacity = 1,\n    fillOpacity = 0.5,\n    radius = 7,\n    weight = 2,\n    popup = ~ DHSID # Add popup that shows the DHSID value for each point\n  ) |&gt; \n  addLegend(\n    title = \"DHS Cluster Status\",\n    position = \"bottomright\",\n    colors = clust_pal(c(\"U\", \"R\")),\n    values = c(\"U\", \"R\"),\n    labels = c(\"Urban\", \"Rural\"),\n    opacity = 0.7\n  )\n\n\n\n\n\n\nNow, when we click on a cluster point, we can see what the cluster ID is, allowing us to easily go back to our et_clust data and double check that cluster’s values. Give it a try by clicking on some of the points in the map above!"
  },
  {
    "objectID": "posts/2024-09-30-leaflet-pt2/index.html#getting-help",
    "href": "posts/2024-09-30-leaflet-pt2/index.html#getting-help",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 2",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-08-30-pop-density/index.html",
    "href": "posts/2024-08-30-pop-density/index.html",
    "title": "Measuring Population Density",
    "section": "",
    "text": "Researchers interested in understanding population distribution in the Global South are faced with a dilemma: data identifying where communities are and how many people live in them often does not exist. National censuses commonly report populations in aggregates, at province, district, or county levels—spatial resolutions much too coarse for studies of weather extremes or health.\nDatasets that estimate how and where many people live (population density layers) do exist, but differ in fundamental ways, and the choice of which dataset to use depends on the scale and scope of the research question. For instance, some datasets are better at identifying built-up areas, while others excel at measuring rural populations. Similarly, researchers who need to understand how many people live in peri-urban areas would not benefit from a dataset that uses a dichotomous urban/rural variable to define built-up areas.\nFurthermore, researchers studying change in population over time should be aware of comparability issues since some data providers update their methods regularly, making their datasets less comparable from year to year. Still, gridded population data are often the most effective tool for researchers to understand large-scale population distribution and explore human-environment relationships. Given the nuance of these datasets, how can we effectively include population density in our analyses?\nAs a start, this post will introduce several of the most widely-used population datasets:\nBelow, discuss each source’s history, input data layers, and methods used in its development. This can serve as a starting point for researchers to explore each dataset in more depth to determine which may be most appropriate for their specific research area."
  },
  {
    "objectID": "posts/2024-08-30-pop-density/index.html#global-rural-urban-mapping-project-grump",
    "href": "posts/2024-08-30-pop-density/index.html#global-rural-urban-mapping-project-grump",
    "title": "Measuring Population Density",
    "section": "Global Rural-Urban Mapping Project (GRUMP)",
    "text": "Global Rural-Urban Mapping Project (GRUMP)\nGRUMP is a series of global gridded population counts and densities created by Columbia University’s Centre for International Earth Science Information Network (CIESIN) in 2004, available for years 1990, 1995, and 2000.8 Using lightly-modeled dasymetric methods based on GPW population and NOAA night-time lights, it indicates the locations of urban settlements and delineates the spatial extents of urban areas at 30 arc-second (roughly 1 kilometer at the equator) resolution.6 In fact, it was the first global gridded population dataset to define urban areas and was widely used for two reasons:\n\nits urban footprint is based on stable-city lights which is an inclusive measure of urban areas\nthe dichotomous definition of the urban footprint (urban or rural) is simple for researchers to use9\n\nGRUMP’s popularity waned upon the creation of higher-resolution datasets that became available for more recent years, including the Global Human Settlement Layer (GHSL) created by the European Commission Joint Research Centre (JRC) in 2010."
  },
  {
    "objectID": "posts/2024-08-30-pop-density/index.html#global-human-settlement-layer-ghsl",
    "href": "posts/2024-08-30-pop-density/index.html#global-human-settlement-layer-ghsl",
    "title": "Measuring Population Density",
    "section": "Global Human Settlement Layer (GHSL)",
    "text": "Global Human Settlement Layer (GHSL)\nAvailable in 5-year intervals from 1975 to 2020, GHSL data products are provided at a spatial resolution of 3 arc-seconds (roughly 100 meters at the equator). GHSL utilizes Landsat imagery (prior to 2014) and Sentinel-2 composite imagery (2014-forward) in an image classification method called Symbolic Machine Learning (SML) to map urban land cover and classify population and land area into seven classes along an urban-rural continuum.9,10\nThe GHSL population data product is made up of three datasets: GHS-POP (population size), GHS-BUILT (built-up areas), and GHS-SMOD (a Degree of Urbanization model grid that delineates settlement types using GHS-POP and GHS-BUILT).11\nGHSL has been widely used to study the interactions between weather extremes, health, and populations. For example, Pinchoff and colleagues12 used GHS-BUILT to examine the impact of urbanicity on health in Tanzania. McGranahan and colleagues13 used GHS-SMOD to calculate urbanization rates in low-elevation coastal zones and estimate the effect of urbanization on weather extremes in deltaic regions. In another study, the authors examined spatial accessibility to healthcare in sub-Saharan Africa (SSA) using GHS-SMOD.14"
  },
  {
    "objectID": "posts/2024-08-30-pop-density/index.html#landscan",
    "href": "posts/2024-08-30-pop-density/index.html#landscan",
    "title": "Measuring Population Density",
    "section": "LandScan",
    "text": "LandScan\nWhile it is a lower resolution data product, LandScan is another prominent global population distribution dataset that provides ambient (24-hour average) population distribution at 30 arc-seconds (roughly 1 kilometer at the equator) resolution, developed in 1998 by the Oak Ridge National Laboratory in Tennessee.15,16\nIn contrast to GRUMP and GHSL—which provide data at 5-year intervals—LandScan is available annually from 2000 through 2022. LandScan utilizes night-time light satellite imagery along with spatial information about elevation (Digital Elevation Models), slope (Digital Terrain Models), land cover, and populated place vector data within a multivariable dasymetric model that uses machine learning to assign the likelihood of population occurrence to each cell.\nLandScan differs from other datasets in that it provides ambient population distribution, depicting not only where people sleep but where they travel, work, and socialize over a 24-hour period. While this may be an important distinction for some researchers, it sets the dataset apart from others, making it less comparable. Researchers should also take caution when comparing multiple versions of LandScan data—regular updates to the input data and distribution algorithm can cause comparability issues between versions."
  },
  {
    "objectID": "posts/2024-08-30-pop-density/index.html#worldpop",
    "href": "posts/2024-08-30-pop-density/index.html#worldpop",
    "title": "Measuring Population Density",
    "section": "WorldPop",
    "text": "WorldPop\nWorldPop is another global gridded population distribution dataset that provides annual estimates from 2000 through 2020; however, WorldPop is provided at a higher resolution of 3 arc-seconds (roughly 100 meters at the equator). Developed by the University of Southampton in 2013, WorldPop uses Sentinel-1 and Sentinel-2 imagery in combination with spatial information on impervious surfaces (SRTM) and slope (DEM) in a dasymetric model with machine learning to estimate population distribution.17\nWorldPop was created by combining three continental-scale population datasets:\n\nAfriPop (developed in 2009)\nAsiaPop (developed in 2012)\nAmeriPop (developed in 2013)\n\nWorldPop’s population estimates are fractional (not integer), which can produce difficulties in interpretation since individual persons obviously cannot be divided up over an area. Its prediction model is good at estimating activity space with the caveat that it can overestimate population density in peri-urban areas while under-estimating population densities in urban centers.\n\n\n\nTable 1: Comparison of Gridded Population Datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Urban-Rural Mapping Project (GRUMP)\nGlobal Human Settlement Layer (GHSL)\nLandScan\nWorldPop\n\n\n\n\nSpatial Resolution\n30 arc-seconds (~1 km)\n3 arc-seconds (~100m)\n30 arc-seconds (~1km)\n3 arc-seconds (~100m)\n\n\nTemporal Availability\n1990, 1995, 2000\n1975-2020 (5-year intervals)\n2000-2022 (annual)\n2000-2020 (annual)\n\n\nOrganization\nCIESIN, Columbia University\nEuropean Commission\nOak Ridge National Laboratory\nUniversity of Southampton\n\n\nAncillary Data\n\nDMSP-OLS night-time light imagery\nTactical pilotage charts\n\n\nSentinel-2 composite and Landsat imagery\nTopography (DEM & SRTM)\nRoad surfaces (OpenStreetMap)\n\n\nDMSP-OLS night-time light imagery\nAdvanced Very High Resolution Radiometry (AVHRR) satellite imagery\nBuilding characteristics (DEM)\nSlope (NIMA Digital Terrain Elevation)\nGlobal Land Cover Characteristics Database\nPopulated places vector (NIMA VMAP)\n\n\nSentinel-1 & Sentinel-2 combination: ESA CCI land cover 300m annual global land cover time-series\nImpervious surface (SRTM)\nSlope (DEM)\n\n\n\nMethods\nArea-weighted reallocation\nDasymetric modeling with Symbolic Machine Learning (SML) is used to combine built-up areas (GHS-BUILT) and population size (GHS-POP) to create a settlement model (GHS-SMOD) based on Degree of Urbanization (DoU), which includes seven classes along an urban-rural continuum.\nDasymetric modeling with machine learning (ML). Likelihood of population occurrence in a particular cell is modeled based on probability coefficients, including roads (weighted by distance from cells to roads), slope (weighted by favorable slope categories), land cover (weighted by type and applying exclusions), and night-time lights (weighted by frequency).\nDasymetric modeling with machine learning (ML). A random forest prediction model is used to create a weight layer.\n\n\nAdvantages\nThe first of its kind, GRUMP was the ideal population distribution dataset used by researchers prior to 2010.\nHigh resolution. Very accurate in areas with higher development. Not a dichotomous urban/rural variable, it allows re searchers to identify peri-urban areas.\nDepicts ambient (24-hour average) population distribution, which captures not only where people sleep but where they travel, work, and socialize. Data available annually.\nHigh resolution. Population density raster data allows researchers to define urban, peri-urban, and rural areas based on density percentages that are appropriate for that area.\n\n\nLimitations\nData only available at 5-year intervals and not updated for recent years. Since it is based on proportional reallocation, it relies on accurate subnational population projections and a reliable nigh-time lights dataset.\nLow degrees of accuracy in rural areas. Data only available at 5-year intervals.\nNot comparable with other datasets due to its ambient nature. Regular updates to distribution algorithms makes it inadvisable to compare different versions of the dataset.\nPopulation estimates are fractional (not integer), but people cannot exist over an entire area. Potential overestimates of population density in peri-urban areas and underestimates in urban centers."
  },
  {
    "objectID": "posts/2024-08-30-pop-density/index.html#getting-help",
    "href": "posts/2024-08-30-pop-density/index.html#getting-help",
    "title": "Measuring Population Density",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html",
    "href": "posts/2024-08-01-ndvi-data/index.html",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "",
    "text": "Update\n\n\n\nSince we originally released this post, we now recommend using VIIRS when doing contemporary NDVI research. See our VIIRS post for more information about how to adapt the contents discussed in this post for use with VIIRS files.\nIn our previous post, we introduced NDVI as a remote sensing tool that can be used to study the effects of environmental conditions on malnourishment and other health outcomes. In this post, we’ll show how to access and load NDVI data using R."
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html#earthdata-search",
    "href": "posts/2024-08-01-ndvi-data/index.html#earthdata-search",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "Earthdata Search",
    "text": "Earthdata Search\nWe’ll use NASA’s Earthdata Search web interface to select and download data. Earthdata Search provides access to NASA’s immense archive of satellite data.\nBefore you can access MODIS register for access to Earthdata Search, you’ll need to register for a free account.\n\nFind data\nOnce you’ve registered for an account, you can open up the Earthdata Search interface, which should look something like this:\n\n\n\nFrom this interface, you can search for data across many NASA products.\n\nSelecting a product\nAs mentioned above, we’ll use data from MODIS for this post. MODIS provides several different NDVI products that vary in their temporal and spatial resolution. Each product has a specific code used to reference it (see the previous link for a table including these codes).\nSome products are provided at the monthly level. If you don’t need finer temporal resolution, it’s preferable to download the monthly data provided directly by NASA. This is because NASA performs additional quality assurance steps when producing monthly data, including adding corrections for pixels obscured due to cloud cover and other similar tweaks. NASA’s processing will generally be better than any preprocessing we would do on our own.\nOf course, for more finely-tuned temporal analyses (e.g. start-of-season analyses), it may be more appropriate to download data from one of the 16-day MODIS products.\nEither way, you can search for a particular product by typing its code in the search bar in the top left of the Earthdata Search interface, as shown below.\n\n\n\nIn our case, we’ve selected the MOD13Q1 product, which provides data at a 250 meter resolution in 16-day increments.\nYou can also select a geographic region of interest using the polygon drawing tools on the right side of the screen. For instance, to identify data for Kenya, we can draw a polygon around the country. Selecting a region will highlight the different areas, or tiles, for which we can download data.\nIn the case of Kenya, the country spans 4 different MODIS tiles. To obtain full coverage of the entire country, we’ll need to download each tile and stich them together.\n\n\n\nTo narrow down further, you can filter the time range of data by entering start and end dates on the left side of the interface.\nOnce you’ve selected the area and time of interest, you can click the Download button to proceed to the download page, as shown below.\n\n\n\n\n\n\nDownload\nDepending on how many tiles and time increments you’ll be downloading, you’ll notice that the estimated data storage size will be quite large. This is both a product of the resolution of this data as well as the fact that we must download full tiles of data. That is, we can’t download data just for our region of interest alone; instead, we have to download data for the MODIS tile(s) that contain that region and then crop the resulting data.\n\nData storage concerns\nData storage can therefore be a significant barrier with working with long time series of NDVI data. This is why most institutions and researchers who work extensively with raster data have dedicated server space to use when running raster operations.\nHowever, if your area of interest is small enough, it may be possible to build up a dataset by downloading and cropping data to that area incrementally, preventing you from needing to store large image files indefinitely.\nThere are also cloud-based solutions available, Google Earth Engine and Amazon Web Services. Google Earth Engine in particular has become a popular interface that combines an interactive map and documentation with a JavaScript library that allows you to write scripts for geoprocessing on Google’s servers. We won’t discuss these options extensively here, but if you begin working on more data-intensive spatial projects, it may be worth exploring what they have to offer.\n\n\nDownloaded files\nAfter clicking the Download link, you should be redirected to a download page that shows the individual files that fit your selection criteria. The page will also include a download.sh script that will perform a batch download. This can be useful if you’re downloading data for many files at once. See the Earthdata documentation for more details on how to use this script to automate the download of many files.\nYou’ll get a separate HDF file for each tile and time increment. Fortunately, {terra} is able to load HDF files, so we don’t need to learn any new R tools yet. However, because each tile represents a different spatial extent, we need to use a new technique to combine them into a single source, called a mosaic.\n\n\n    © Robert J. Hijmans et al. (GPL &gt;=3)"
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html#nasa-image-files",
    "href": "posts/2024-08-01-ndvi-data/index.html#nasa-image-files",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "NASA image files",
    "text": "NASA image files\nNASA provides its files in HDF format. Each HDF file is identified by its tile code (e.g. h21v08) and its timestamp code (e.g. A2014001). To mosaic our tiles, we’ll need to create a separate raster stack for each tile across all of our timestamps.\n\n\nHDF stands for Hierarchical Data Format, yet another common raster data file format.\nFirst, we’ll list all our files, which we’ve saved in a directory we’ve named data_local/MOD13Q1:\n\nfiles &lt;- list.files(\"data_local/MOD13Q1\", full.names = TRUE)\n\nYou’ll notice that each file contains the tile code and timestamp in the file name.\n\nfiles\n#&gt; [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v08.061.2021246163834.hdf\"\n#&gt; [2] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v09.061.2021246163703.hdf\"\n#&gt; [3] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v08.061.2021246164307.hdf\"\n#&gt; [4] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v09.061.2021246155251.hdf\"\n#&gt; [5] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v08.061.2021260011612.hdf\"\n#&gt; [6] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v09.061.2021260011337.hdf\"\n#&gt; [7] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v08.061.2021260023343.hdf\"\n#&gt; [8] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v09.061.2021260023455.hdf\"\n\nUltimately, we’ll need to group each of these files such that we can create a single raster stack for each tile. The layers in that stack should correspond to the data’s timestamps. This will produce a time series of NDVI data for each spatial region which can later be combined with the adjacent regions.\nTo achieve this, we’ll need to leverage the tile and timestamp codes that NASA provides in its file names."
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html#regular-expressions",
    "href": "posts/2024-08-01-ndvi-data/index.html#regular-expressions",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "Regular expressions",
    "text": "Regular expressions\nRight now, all our files are in one large list. But because they represent different spatial extents, we can’t load them all at once with terra:\n\nrast(files)\n#&gt; Error: [rast] extents do not match\n\nInstead, we need to first organize our files by their tile codes. We could go ahead and manually store the codes for later access like so:\n\ntile_codes &lt;- c(\"h21v08\", \"h21v09\", \"h22v08\", \"h22v09\")\ntimestamps &lt;- c(\"2014001\", \"2014209\")\n\nHowever, this wouldn’t produce a very flexible workflow: if we ever decided to perform a similar analysis for different times or spatial regions, we’d have to modify these codes manually.\nAn alternative approach would be to dynamically extract the codes using regular expressions. The term regular expressions refers to a specific syntax designed to identify and extract sequences of characters in a set of text.\nWe’ll use the {stringr} package to demonstrate the basics of regular expressions:\n\nlibrary(stringr)\n\n\n\n    © RStudio, Inc. (MIT) \nAt their simplest, regular expressions allow you to extract specific characters from a string:\n\n# str_detect() tells us whether a search pattern exists in the target string\n\n# \"b\" does exist in the string\nstr_detect(\"abc123\", \"b\")\n#&gt; [1] TRUE\n\n# \"z\" does not exist in the string:\nstr_detect(\"abc123\", \"z\")\n#&gt; [1] FALSE\n\nHowever, regular expressions may also include special characters, which allow you to search more flexibly. For instance, [ brackets allow us to define a character class, which will detect whether any of the enclosed characters is present.\n\nstr_detect(\"abc123\", \"[ahz]\")\n#&gt; [1] TRUE\n\nWe can use a similar approach to search for any numeric digits in our string:\n\nstr_detect(\"abc123\", \"[0-9]\")\n#&gt; [1] TRUE\n\nCurly braces can be used to detect a certain number of sequential characters. For instance, to determine whether the string contains 3 consecutive digits:\n\nstr_detect(\"abc123\", \"[0-9]{3}\")\n#&gt; [1] TRUE\n\nThis returns TRUE, but if we searched for a longer sequence, we’d no longer be able to detect a match for the pattern:\n\nstr_detect(\"abc123\", \"[0-9]{4}\")\n#&gt; [1] FALSE\n\nThe special characters ^ and $ represent the start and end of the string, respectively. For instance, to determine if the string starts with a digit:\n\nstr_detect(\"abc123\", \"^[0-9]\")\n#&gt; [1] FALSE\n\nOr to determine if it ends with a digit:\n\nstr_detect(\"abc123\", \"[0-9]$\")\n#&gt; [1] TRUE\n\nThere are far more available options in the regular expression language, and we won’t be able to cover them all here. The stringr package includes a helpful introduction to help you get started.\nIn our case, we can use the basic syntax we just introduced to extract the tile code from our file names:\n\n# Extract h00v00 tile code pattern\ntile_codes &lt;- unique(str_extract(files, \"h[0-9]{2}v[0-9]{2}\"))\n\ntile_codes\n#&gt; [1] \"h21v08\" \"h21v09\" \"h22v08\" \"h22v09\"\n\nImportantly, if we ever download new files for different tiles, this code will still extract their tile codes without any modification (that is, unless NASA changes their file naming conventions unexpectedly).\nRegular expressions therefore allow us to implement a generalized approach that is more robust to future changes in our analysis."
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html#an-iterative-r-workflow",
    "href": "posts/2024-08-01-ndvi-data/index.html#an-iterative-r-workflow",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "An iterative R workflow",
    "text": "An iterative R workflow\nNext, we need to group the files so that all files for a single tile are placed together. That way, all the timestamps for each tile will be loaded into a raster stack with a single spatial extent. Each layer will correspond to a different timestamp.\nBecause we have multiple tiles and multiple files for each tile, we will likely need to apply the same set of processing operations many times over. While we could copy and paste our code to group each tile’s files, this would quickly become tedious. Furthermore, hard-coding our file groups would make it harder to make modifications if we were to add more files to our analysis or adapt our code to work with a different region.\nInstead, we can build an iterative workflow that will automatically apply our processing steps for each tile.\n\nThe purrr package\nWe’ll use the {purrr} package to build our workflow. The purrr package provides a concise syntax for iterating through multiple inputs. This is primarily facilitated by purrr’s map() function.\n\n\n    © RStudio, Inc. (GPL-3) \n\n\n\n\n\n\nIterating in base R\n\n\n\nYou may already be familiar with the for loop or with lapply(), both of which are options for iterating in base R (lapply() is most similar to purrr’s map()).\nIf you’re more comfortable with these tools, the code we introduce below can certainly be adapted for a for loop or lapply() workflow.\n\n\nmap() allows us to iterate by providing two things:\n\nThe list (or vector) of objects we want to iterate over\nA function describing the operation we want to do for each element in the list\n\nFor instance, take this list of letters:\n\nl &lt;- list(\n  c(\"A\", \"B\"),\n  c(\"C\", \"D\", \"E\"), \n  \"F\"\n)\n\nl\n#&gt; [[1]]\n#&gt; [1] \"A\" \"B\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"C\" \"D\" \"E\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"F\"\n\nLet’s say we want to find out the length of each element in our list. Many R functions are vectorized, so we might think to simply use length() on our list:\n\n\nVectorized functions automatically operate on each individual element of a vector. For instance, nchar() can count characters in a single string or for each string in a vector of strings.\n\nlength(l)\n#&gt; [1] 3\n\nHowever, this gives us the length of our input list l, which isn’t what we want. Instead, we can use purrr to map the length() function to each element in our list l:\n\nlibrary(purrr)\n\n# For each element in `l`, apply the `length()` function\nmap(l, length)\n#&gt; [[1]]\n#&gt; [1] 2\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 3\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 1\n\n\n\n\n\n\n\nAnonymous function syntax\n\n\n\nWe can also provide the function in R’s anonymous function syntax, which we introduced previously. For instance:\n\nmap(l, function(x) length(x))\n\n# Or, for R 4.1+\nmap(l, \\(x) length(x))\n\nIf you want to modify other arguments of the function being mapped, you’ll need to use anonymous function syntax instead of the function name itself.\n\n\nWe see that map() returns a list containing the result we get when we apply the length() function to each individual element in l.\nWe can do a similar process with our raster tiles. For each tile code, we want to detect the files that contain that tile code in their name. This way, we can group those files together.\nHere, we use str_detect() to detect the presence of the input tile_code string in our set of files. For each tile code, we’ll get a set of logical values indicating the position of the files that contain that code.\n\n# For each tile code, identify the file indexes that contain that code\nmap(\n  tile_codes,\n  function(code) str_detect(files, code)\n)\n#&gt; [[1]]\n#&gt; [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n#&gt; \n#&gt; [[2]]\n#&gt; [1] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n#&gt; \n#&gt; [[3]]\n#&gt; [1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n#&gt; \n#&gt; [[4]]\n#&gt; [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n\nWe can use these logical values to subset the file names that correspond to each tile code. Remember that we’ll have multiple files for each tile because we’re working with multiple timestamps:\n\ntiles &lt;- map(\n  tile_codes,\n  function(code) files[str_detect(files, code)]\n)\n\ntiles\n#&gt; [[1]]\n#&gt; [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v08.061.2021246163834.hdf\"\n#&gt; [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v08.061.2021260011612.hdf\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v09.061.2021246163703.hdf\"\n#&gt; [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v09.061.2021260011337.hdf\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v08.061.2021246164307.hdf\"\n#&gt; [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v08.061.2021260023343.hdf\"\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v09.061.2021246155251.hdf\"\n#&gt; [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v09.061.2021260023455.hdf\"\n\n\n\n\n\n\n\nAt this point we can load each of our tiles into a separate raster stack using terra’s rast(). Each of these files contains several layers with different measures. We can peek at the layers by loading the first file. We’re only interested in the layer of NDVI values, which we see is the first layer:\n\nnames(rast(files[1]))\n#&gt;  [1] \"\\\"250m 16 days NDVI\\\"\"                     \n#&gt;  [2] \"\\\"250m 16 days EVI\\\"\"                      \n#&gt;  [3] \"\\\"250m 16 days VI Quality\\\"\"               \n#&gt;  [4] \"\\\"250m 16 days red reflectance\\\"\"          \n#&gt;  [5] \"\\\"250m 16 days NIR reflectance\\\"\"          \n#&gt;  [6] \"\\\"250m 16 days blue reflectance\\\"\"         \n#&gt;  [7] \"\\\"250m 16 days MIR reflectance\\\"\"          \n#&gt;  [8] \"\\\"250m 16 days view zenith angle\\\"\"        \n#&gt;  [9] \"\\\"250m 16 days sun zenith angle\\\"\"         \n#&gt; [10] \"\\\"250m 16 days relative azimuth angle\\\"\"   \n#&gt; [11] \"\\\"250m 16 days composite day of the year\\\"\"\n#&gt; [12] \"\\\"250m 16 days pixel reliability\\\"\"\n\nBecause data at this resolution can be memory-intensive, we can load only the layer of NDVI data by using the lyrs argument to rast():\n\nrast(files[1], lyrs = 1)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 4800, 4800, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 231.6564, 231.6564  (x, y)\n#&gt; extent      : 3335852, 4447802, 0, 1111951  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; source      : MOD13Q1.A2014001.h21v08.061.2021246163834.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI \n#&gt; varname     : MOD13Q1.A2014001.h21v08.061.2021246163834 \n#&gt; name        : \"250m 16 days NDVI\"\n\nHowever, we’ll be loading multiple files (one for each timestamp) into each raster stack for each tile. Therefore, we have to select the first layer of each raster file. The lyrs argument considers all the files together when loading them. That is, lyrs = 1 doesn’t refer to the first layer of each file. Instead, it will extract the very first layer of the first file only.\nFortunately, each file is organized identically, so we know that NDVI will always be the first layer. Because there are 12 layers in each file, we just need to load every 12th layer. We can easily build a sequence that contains the correct layer numbers:\n\n# Maximum layer value will be the first layer in the very last file being loaded\nmax_lyrs &lt;- 12 * length(tiles[[1]])\n\n# Create sequence in increments of 12 up to the maximum layer index needed\nndvi_layers &lt;- seq(1, max_lyrs, by = 12)\n\nndvi_layers\n#&gt; [1]  1 13\n\nNow, we can again map() over our file groups to load each set of files, selecting out only the NDVI layers:\n\n# Load NDVI layers for each set of files corresponding to a particular tile\nndvi_tiles &lt;- map(\n  tiles, \n  function(x) rast(x, lyrs = ndvi_layers)\n)\n\nWe see that we now have a list of 4 separate raster stacks, because we loaded each entry in tiles separately:\n\nndvi_tiles\n#&gt; [[1]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 231.6564, 231.6564  (x, y)\n#&gt; extent      : 3335852, 4447802, 0, 1111951  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; sources     : MOD13Q1.A2014001.h21v08.061.2021246163834.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt;               MOD13Q1.A2014209.h21v08.061.2021260011612.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt; varnames    : MOD13Q1.A2014001.h21v08.061.2021246163834 \n#&gt;               MOD13Q1.A2014209.h21v08.061.2021260011612 \n#&gt; names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#&gt; \n#&gt; [[2]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 231.6564, 231.6564  (x, y)\n#&gt; extent      : 3335852, 4447802, -1111951, 0  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; sources     : MOD13Q1.A2014001.h21v09.061.2021246163703.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt;               MOD13Q1.A2014209.h21v09.061.2021260011337.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt; varnames    : MOD13Q1.A2014001.h21v09.061.2021246163703 \n#&gt;               MOD13Q1.A2014209.h21v09.061.2021260011337 \n#&gt; names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#&gt; \n#&gt; [[3]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 231.6564, 231.6564  (x, y)\n#&gt; extent      : 4447802, 5559753, 0, 1111951  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; sources     : MOD13Q1.A2014001.h22v08.061.2021246164307.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt;               MOD13Q1.A2014209.h22v08.061.2021260023343.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt; varnames    : MOD13Q1.A2014001.h22v08.061.2021246164307 \n#&gt;               MOD13Q1.A2014209.h22v08.061.2021260023343 \n#&gt; names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#&gt; \n#&gt; [[4]]\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 231.6564, 231.6564  (x, y)\n#&gt; extent      : 4447802, 5559753, -1111951, 0  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; sources     : MOD13Q1.A2014001.h22v09.061.2021246155251.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt;               MOD13Q1.A2014209.h22v09.061.2021260023455.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#&gt; varnames    : MOD13Q1.A2014001.h22v09.061.2021246155251 \n#&gt;               MOD13Q1.A2014209.h22v09.061.2021260023455 \n#&gt; names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\"\n\nTo confirm that each raster stack comes from a different tile, we note that each has a different spatial extent:\n\nmap(ndvi_tiles, ext)\n#&gt; [[1]]\n#&gt; SpatExtent : 3335851.559, 4447802.078667, 0, 1111950.519667 (xmin, xmax, ymin, ymax)\n#&gt; \n#&gt; [[2]]\n#&gt; SpatExtent : 3335851.559, 4447802.078667, -1111950.519667, 0 (xmin, xmax, ymin, ymax)\n#&gt; \n#&gt; [[3]]\n#&gt; SpatExtent : 4447802.078667, 5559752.598333, 0, 1111950.519667 (xmin, xmax, ymin, ymax)\n#&gt; \n#&gt; [[4]]\n#&gt; SpatExtent : 4447802.078667, 5559752.598333, -1111950.519667, 0 (xmin, xmax, ymin, ymax)"
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html#crop-tiles",
    "href": "posts/2024-08-01-ndvi-data/index.html#crop-tiles",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "Crop tiles",
    "text": "Crop tiles\nNext, we’ll crop our tiles to our area of interest (in this case, using the Kenya national borders). This will reduce the size of our files going forward and speed up any subsequent processing.\nTo crop the tiles to our area of interest, we’ll need country borders. We’ll load the integrated boundaries from IPUMS and dissolve them (with st_union()) to get the national border.\n\nke_borders &lt;- ipumsr::read_ipums_sf(\"data/geo_ke1989_2014.zip\") |&gt; \n  st_make_valid() |&gt; # Fix minor border inconsistencies\n  st_union()\n\nWe need to make sure our borders are in the same coordinate system as our raster before we can crop to its extent, so we’ll extract the CRS from one of our rasters and use it with st_transform() to convert our borders’ CRS. (We’ve introduced st_transform() in the past.)\n\n\n\n\n\n\nRaster projection\n\n\n\nIn general, if you’re working with both raster and vector data, it’s best to project the vector data to match your raster data rather than the reverse.\nProjecting a raster distorts the cells, but because rasters necessarily are represented as a uniform grid, the values in the new cells must be resampled from the original grid, as the new cell locations may overlap with multiple input cell locations. This adds an additional later of uncertainty to the data.\n\n\n\n# Obtain CRS of NDVI raster data\nndvi_crs &lt;- crs(ndvi_tiles[[1]])\n\n# Transform borders to same CRS as NDVI\nke_borders &lt;- st_transform(ke_borders, crs = ndvi_crs)\n\nNow, we can use our borders to crop each tile, removing the values outside of the border area. Note that we still need to use map() because our rasters are still separate elements in the ndvi_tiles list:\n\nndvi_tiles &lt;- map(\n  ndvi_tiles, \n  function(x) crop(x, ke_borders)\n)\n\nNow we’re ready to join our tiles together!"
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html#mosaic-tiles",
    "href": "posts/2024-08-01-ndvi-data/index.html#mosaic-tiles",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "Mosaic tiles",
    "text": "Mosaic tiles\nWe can mosaic our cropped tiles together with terra’s mosaic() function. Typically mosaic() only takes two rasters.\n\n# Mosaic two of our tiles together\nndvi_mosaic &lt;- mosaic(ndvi_tiles[[1]], ndvi_tiles[[3]])\n\nHowever, we need to mosaic all 4 tiles to cover Kenya in its entirety:\n\n\nShow plot code\nndvi_pal &lt;- list(\n  pal = c(\n    \"#fdfbdc\",\n    \"#f1f4b7\",\n    \"#d3ef9f\",\n    \"#a5da8d\",\n    \"#6cc275\",\n    \"#51a55b\",\n    \"#397e43\",\n    \"#2d673a\",\n    \"#1d472e\" \n  ),\n  values = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1)\n)\n\nggplot() +\n  layer_spatial(ndvi_mosaic[[1]]) +\n  layer_spatial(st_simplify(ke_borders, dTolerance = 1000), fill = NA) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 100000000),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    title = \"NDVI: Kenya\",\n    subtitle = \"January 1-16, 2014\",\n    fill = \"NDVI\",\n    caption = \"Source: NASA MOD13Q1\"\n  ) +\n  theme_dhs_map()\n\n\n\n\n\n\n\n\n\nWe could manually mosaic all the tiles together, but purrr actually provides a more efficient solution. The reduce() function allows us to collapse a set of objects stored in a list by repeatedly applying the same operation to each pair of list elements.\nThat is, reduce() allows us to mosaic the first two entries in our ndvi_tiles list, then mosaic that output with the next entry in the list, and so on. We simply need to provide the list of our raster tiles and the function that we want to use to collapse the list. In this case, we want to mosaic() the elements in the list together into a single raster output.\n\nke_ndvi &lt;- reduce(ndvi_tiles, mosaic)\n\nNext, we’ll rescale the NDVI values, which should range from -1 to 1:\n\nke_ndvi &lt;- ke_ndvi / 100000000\n\n\n\nShow plot code\nggplot() +\n  layer_spatial(ke_ndvi[[1]]) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    title = \"NDVI: Kenya\",\n    subtitle = \"January 1-16, 2014\",\n    fill = \"NDVI\",\n    caption = \"Source: NASA MOD13Q1\"\n  ) +\n  theme_dhs_map()\n\n\n\n\n\n\n\n\n\nNotice that no time information is attached to our output raster:\n\ntime(ke_ndvi)\n#&gt; [1] NA NA\n\nThe time information is stored in a A0000000 format string in the file name. The first 4 digits after the A contain the data year, and the next 3 contain the day of the year.\nWe can again use a regular expression to extract the timestamps using this pattern:\n\n# Extract 7-digit sequence following an \"A\"\ntimestamps &lt;- unique(stringr::str_extract(files, \"(?&lt;=A)[0-9]{7}\"))\n\ntimestamps\n#&gt; [1] \"2014001\" \"2014209\"\n\n\n\n\"(?&lt;=A)\" is referred to as a lookbehind assertion. It will match elements of a string only if they follow the text in the assertion (in this case, \"A\"). For more about assertions, check out this cheatsheet.\nThese timestamps are in year + day of year format. We can parse this format using {lubridate}, using \"yj\" format.\n\n\n    © Garrett Grolemund, Hadley Wickham (GPL &gt;= 2) \n\nLearn more about the different codes used to specify time formats here.\n\n\n# Parse time format and attach to our output raster\ntime(ke_ndvi) &lt;- unique(lubridate::parse_date_time(timestamps, \"yj\"))\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that by manually setting the time after combining our tiles together, we may miss the possibility that certain tiles may have been recorded at a different time than others. MODIS data are fairly reliable time-wise, so these files do all share the same timestamps, but when combining files, it’s always worth confirming that each raster was recorded for the expected time range.\n\n\nNow, we finally have a full set of NDVI data for our area of interest. We have full spatial coverage for Kenya, and we’ve got our timestamps represented in distinct layers of our raster:\n\nke_ndvi\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 4662, 3795, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 231.6564, 231.6564  (x, y)\n#&gt; extent      : 3769512, 4648648, -520300.2, 559681.8  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; source      : spat_2020fd5ebe_8224_SDnwk7fjh8CwF05.tif \n#&gt; names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#&gt; min values  :             -0.2000,             -0.2000 \n#&gt; max values  :              0.9995,              0.9995 \n#&gt; time        : 2014-01-01 to 2014-07-28 UTC\n\n\n\nShow plot code\nlibrary(patchwork)\n#&gt; \n#&gt; Attaching package: 'patchwork'\n#&gt; The following object is masked from 'package:terra':\n#&gt; \n#&gt;     area\n\nke_ndvi_mask &lt;- mask(ke_ndvi, vect(ke_borders))\n\np1 &lt;- ggplot() +\n  layer_spatial(ke_ndvi_mask[[1]]) +\n  layer_spatial(ke_borders, fill = NA) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    subtitle = \"January 1-16, 2014\",\n    fill = \"NDVI\"\n  ) +\n  theme_dhs_map(show_scale = FALSE) +\n  theme_dhs_base()\n\np2 &lt;- ggplot() +\n  layer_spatial(ke_ndvi_mask[[2]]) +\n  layer_spatial(ke_borders, fill = NA) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    subtitle = \"July 28-August 12, 2014\",\n    fill = \"NDVI\"\n  ) +\n  theme_dhs_map() +\n  theme_dhs_base()\n\np1 + p2 +\n  plot_layout(guides = \"collect\", ncol = 2) +\n  plot_annotation(\n    title = \"Seasonal NDVI Comparison: Kenya\",\n    caption = \"Source: NASA MOD13Q1\"\n  ) &\n  theme(legend.position='bottom')"
  },
  {
    "objectID": "posts/2024-08-01-ndvi-data/index.html#getting-help",
    "href": "posts/2024-08-01-ndvi-data/index.html#getting-help",
    "title": "An Iterative Workflow for Loading NDVI Data in R",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-06-11-climate-reproduction/index.html",
    "href": "posts/2024-06-11-climate-reproduction/index.html",
    "title": "Incorporating Environmental and Reproductive History Data",
    "section": "",
    "text": "There have been heroic efforts to produce thorough, high-quality, publicly available data on weather extremes, which has provided new opportunities for researchers interested in extreme weather demography. The wealth of environmental resources we now have provide solid, well-organized, and fine-grained data on temperatures and rainfall and how these conditions vary over time. What is more challenging is getting the personal histories from surveys to line up with those data.\nIf researchers are interested in the relationship between extreme weather and reproductive issues such as contraceptive use, sexual activity, and fertility preferences, data analysis can be relatively straightforward. Questions answered at the time of the survey give us this current information. We know where respondents live at the time of the survey and when the survey took place, so space and time are easily addressed.\nIf we want to unravel how extreme weather might influence reproductive events, such as pregnancies, we need to look back across time at what happened, when, and where. This post addresses some common limitations and issues that can be tricky to work with when aligning extreme weather events and reproductive health data longitudinally.\nSome of our focus below is on analyzing pregnancy history data collected prior to DHS Phase 8. For surveys from DHS Phase 8 forward (released after 2023), when pregnancy histories were routinely collected, analyzing reproductive events over time may be more straightforward. But any analysis of reproductive events using longitudinal data must grapple with issues related to migration, defining the period at risk of fetal loss, and making sound choices about covariates, all topics discussed in this post."
  },
  {
    "objectID": "posts/2024-06-11-climate-reproduction/index.html#defining-the-window-of-observation",
    "href": "posts/2024-06-11-climate-reproduction/index.html#defining-the-window-of-observation",
    "title": "Incorporating Environmental and Reproductive History Data",
    "section": "Defining the window of observation",
    "text": "Defining the window of observation\nThe first step is establishing the window of observation, that is, the time period when we know a woman is living at her current location. In the absence of detailed migration histories, we can begin this observation period with the exact month and year she moved to her current place of residence (when such detail is available). Alternatively, we can choose a standard reference period, such as the first January following the year she moved to her location. Since the most commonly available DHS migration variable covers “Years lived in place of residence” (V104/RESIDEINTYR), this last approach is often the most practical. Although we do not analyse births or pregnancies before that starting point, these unobserved events are used when creating information about the woman, such as parity or years since last birth/pregnancy outcome. For women who reported “always” living in their current location, the full reproductive history can be analyzed (i.e., events from age 15 until the time of interview).\nOnce the window of observation is constructed for each individual, and the sample selected to include only those observations, then environmental data can be merged on to the unit of analysis (i.e., woman or woman month)."
  },
  {
    "objectID": "posts/2024-06-11-climate-reproduction/index.html#defining-the-timing-of-a-pregnancy",
    "href": "posts/2024-06-11-climate-reproduction/index.html#defining-the-timing-of-a-pregnancy",
    "title": "Incorporating Environmental and Reproductive History Data",
    "section": "Defining the timing of a pregnancy",
    "text": "Defining the timing of a pregnancy\nA particularly salient period during which extreme weather can affect reproduction is the time between conception and the end of a pregnancy. When working with live births, including data from the widely available DHS birth histories, the conception month is usually calculated by subtracting 9 months from the date of birth.\nWhen working with pregnancy history data, the situation is sometimes more complicated. DHS samples with pregnancy histories always report the year and month a pregnancy ended. For this blog post, we focus on the 22 standard DHS surveys that collected pregnancy history data before (the current) Phase 8 survey. The Phase 8 approach is very different than the pre-Phase 8 approach and as a result data analysis approaches will need to be different. Strategies to use both the pre-phase 8 and the phase 8 data are useful to allow for longer-term investigations of the impact of extreme weather on pregnancy.\nFor the pre-Phase 8 data, the variable names relating to pregnancy history are sample-specific. In general, country- and year-specific variable names in DHS women’s surveys begin with the letter s followed by the number of the relevant question on the questionnaire. Thus, for example, on the women’s questionnaire for the 2012-2013 Pakistan sample, question number 226 records the month and year that each pregnancy ended. Accordingly, variables s226m_XX and s226y_XX respectively report “Month of pregnancy lost” and “Year of pregnancy lost,” with XX referencing the number of the pregnancy, from the most recent (01) to the twentieth most recent.\nFiguring out the date of conception for pregnancies in pre-Phase 8 pregnancy histories can be difficult. Some of these surveys, such as the Pakistan 2012-13 sample, directly asked how long the pregnancy lasted. For many of the samples with early pregnancy histories, particularly from Eastern Europe and East Asia, this direct information about pregnancy duration (and thus conception date) is lacking.\nIn such cases, establishing the month of conception based on the month of miscarriage, abortion or stillbirth requires imputation. Knowing the month of conception is required for investigating extreme weather impacts on pregnancy outcomes. One approach uses information derived from the five-year calendar data.21 In those data one can observe the month a woman said she was pregnant and the month when the pregnancy ended. By taking the difference, we can get an estimate of how long pregnancies lasted on average before the specific outcome in a specific culture. We can also calculate the mean and standard deviation for pregnancy outcomes for a given calendar sample and use those statistics as a structure for imputation. Note that reports from the calendar data may be sensitive to context—for example, the extent to which stillbirths are called miscarriages and vice versa. In addition, the length of gestation before abortion may vary by abortion legislation, which obviously varies across time and space. Thus, no clear rules of thumb, like “pregnancies ending in a live birth usually last 9 months,” can be stated for pregnancies ending in miscarriage, stillbirth, or abortion. Such estimates for terminated pregnancies should be context-specific (e.g., access to pregnancy tests) and empirically grounded.\nResults from calendar-based imputed pregnancy duration data for one study are shown in Table 1, which details the pregnancy outcome timing from Armenian and Tajik DHS data.21 Only 65% of reported pregnancies ended in a live birth in Armenia during the five years preceding the survey (2015-16) and 84% in Tajikistan (2017). To impute the timing of conceptions ending in stillbirth, miscarriage, or abortion, the authors applied a normal (Gaussian) distribution using the mean and standard deviation observed in the five-year calendar data within the specific DHS data set.\n\nTable 1: Gestation by pregnancy outcome\n\n\n\n\n\n\n\n\n\n\nLive Birth\nStillbirth\nMiscarriage\nAbortion\n\n\n\n\nArmenia, 2015-2016\n\n\n\n\n\n\nMean\n8.56\n6.77\n2.57\n2.24\n\n\nMedian\n9\n7\n2\n2\n\n\nSD\n1.35\n1.89\n0.88\n0.8\n\n\nMin.\n2\n2\n1\n1\n\n\nMax.\n10\n9\n7\n9\n\n\nNumber of pregnancies\n2,084\n17\n305\n783\n\n\nTajikistan, 2017\n\n\n\n\n\n\nMean\n8.63\n6.61\n2.22\n1.9\n\n\nMedian\n9\n7\n2\n2\n\n\nSD\n1.28\n1.93\n1.18\n1.02\n\n\nMin.\n1\n2\n1\n1\n\n\nMax.\n10\n9\n9\n9\n\n\nNumber of pregnancies\n6,890\n70\n538\n715"
  },
  {
    "objectID": "posts/2024-06-11-climate-reproduction/index.html#selecting-appropriate-covariates",
    "href": "posts/2024-06-11-climate-reproduction/index.html#selecting-appropriate-covariates",
    "title": "Incorporating Environmental and Reproductive History Data",
    "section": "Selecting appropriate covariates",
    "text": "Selecting appropriate covariates\nA final consideration when working with retrospective longitudinal data is which variables can be used as covariates. Unless information comes in the form of histories, where we can track changes in a characteristic for an individual over time, we should only use information that does not change. Examples of non-changing variables are birth cohort and ethnicity/nationality.\nThe DHS provides a wealth of household and individual-level information that is unfortunately problematic when analyzing the effects of extreme weather over time. This is generally unsatisfying, since we know socioeconomic factors play an important role in health outcomes. But ascribing back characteristics—such as household wealth quintile, woman’s employment status, her contraceptive knowledge, or her decision-making power at the time the survey—to the respondent earlier in time is risking a few problems. First, such ascription introduces error into the data, whereby a characteristic is assigned at a time for which it may not be true. Although it may seem relevant that the person eventually ended up with that characteristic, assigning the characteristic before it is true introduces estimation inconsistencies due to “anticipatory analyses”.22\nWhile assuming that current characteristics held earlier is never completely reliable, error could be minimized by restricting analysis to recent pregnancies—for example, those occurring in the three years before the survey or focusing on only the most-recent and second-most-recent pregnancies (indexed _01 and _02), if they occurred recently.\nOne option for gaining some measure of socioeconomic status that varies over time is to construct time-varying covariate variables. For example, from the variable that tells us the woman’s completed number of years of education (V133, or EDYRTOTAL in IPUMS DHS), we can produce a reasonable estimate of her last year in school. We can code the following years according to the highest level of education the respondent achieved. Similarly, we do not have partnership histories in DHS, but we do know the woman’s age at her first marriage or coresident union (V511, or AGEFRSTMAR in IPUMS DHS). From this information, we can construct a time-varying covariate for whether the woman had ever entered a co-resident union."
  },
  {
    "objectID": "posts/2024-06-11-climate-reproduction/index.html#getting-help",
    "href": "posts/2024-06-11-climate-reproduction/index.html#getting-help",
    "title": "Incorporating Environmental and Reproductive History Data",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html",
    "href": "posts/2024-04-15-chirts-metrics/index.html",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "",
    "text": "In our previous technical post, we showed how to reduce a daily time series of environmental data for an entire country into a single digestible metric that can be attached to DHS survey data for use in an analysis.\nHowever, as we mentioned then, a long-run average isn’t always the most appropriate way to aggregate raster values across time. As an example, imagine we were interested in the impacts of extreme heat on child birth weight. The proximate conditions in the several months preceding each child’s birth are likely to be more consequential for that child’s health than the average conditions over many years.1"
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#dhs-boundaries",
    "href": "posts/2024-04-15-chirts-metrics/index.html#dhs-boundaries",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "DHS boundaries",
    "text": "DHS boundaries\nFor this series, we’ll use the 2012 DHS sample for Mali. We won’t be working with the survey data until our next post, but we will be using the integrated administrative boundary files from IPUMS in our maps.\nYou can download the boundary data directly from IPUMS DHS by clicking the shapefile link under the Mali section of this table. We’ve placed this shapefile in the data/gps directory within our project.\nPreviously, we unzipped the shapefile and loaded it with st_read() from the sf package. However, since IPUMS often distributes shapefiles in zip archives, ipumsr provides read_ipums_sf() as a way to read a shapefile directly without manually extracting the compressed files. We’ll use this convenience to load our boundary data into an sf object:\n\nml_borders &lt;- read_ipums_sf(\"data/gps/geo_ml1995_2018.zip\")\n\nml_borders\n#&gt; Simple feature collection with 8 features and 3 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -12.23888 ymin: 10.14781 xmax: 4.267383 ymax: 25.00108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 8 × 4\n#&gt;   CNTRY_NAME ADMIN_NAME    DHSCODE                                      geometry\n#&gt;   &lt;chr&gt;      &lt;chr&gt;           &lt;int&gt;                                 &lt;POLYGON [°]&gt;\n#&gt; 1 Mali       Kayes               1 ((-9.330095 15.50158, -9.320187 15.50138, -9…\n#&gt; 2 Mali       Ségou               4 ((-3.962041 13.50099, -3.963767 13.50299, -3…\n#&gt; 3 Mali       Mopti               5 ((-0.7440053 15.06439, -0.9481987 14.88898, …\n#&gt; 4 Mali       Tombouctou          6 ((-0.005279168 21.87488, -0.006277 21.87317,…\n#&gt; 5 Mali       Bamako              9 ((-7.932848 12.68226, -7.932015 12.68209, -7…\n#&gt; 6 Mali       Sikasso             3 ((-4.472905 12.71992, -4.472977 12.71908, -4…\n#&gt; 7 Mali       Koulikoro           2 ((-9.076561 15.50138, -9.000042 15.50046, -8…\n#&gt; 8 Mali       Gao and Kidal       7 ((-0.4509285 15.0843, -0.450753 15.0864, -0.…\n\nOur boundary file includes borders for individual administrative units within Mali. However, it’s often useful to have a single external border for spatial operations and mapping.\nTo combine internal borders, we can use st_union() from {sf}. However, in this case, we first need to simplify our file so that st_union() works properly.\n\n\n\n\n\n\nTip\n\n\n\nFor some spatial files, small misalignments may cause problems for certain spatial operations. Often, you’ll notice these issues in your maps if errant lines or points appear in unexpected places. You can check whether a file is topologically valid with st_is_valid().\n\n\nWe use st_make_valid() to correct some of these issues. Then, we can combine our internal geometries with st_union() and simplify our external border slightly.\n\n# Validate internal borders\nml_borders_neat &lt;- st_make_valid(ml_borders)\n\n# Collapse internal borders to get single country border\nml_borders_out &lt;- ml_borders_neat |&gt; \n  st_union() |&gt; \n  st_simplify(dTolerance = 1000) |&gt; \n  st_as_sf()\n\nNow, we have both detailed internal borders as well as a single country border."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#chirts",
    "href": "posts/2024-04-15-chirts-metrics/index.html#chirts",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "CHIRTS",
    "text": "CHIRTS\nFor our temperature data, we’ll use the Climate Hazards Center Infrared Temperature with Stations (CHIRTS) dataset.2 CHIRTS provides daily estimates for several temperature metrics at a 0.05° (~5 kilometer) raster resolution. CHIRTS provides estimates of the following measures:\n\nDaily maximum air temperature (2 meters above ground)\nDaily minimum air temperature (2 meters above ground)\nDaily average relative humidity\nDaily average heat index\n\nThe most appropriate metric will depend on the nature of your research. For instance, relevant temperature metrics for studying the effects of heat on the human body are likely different from those used for studying agricultural productivity.\nFor health research, it’s also worth considering the specifics of the population of interest, as they may employ adaptive strategies or be at increased risk of heat exposure due to common pre-existing conditions or lifestyle features.3\nSince this post focuses specifically on R techniques (and not on methodological considerations), we’ll keep it simple and use daily maximum air temperature.\nThere are two ways to go about obtaining the CHIRTS data: either via manual download or via the {chirps} R package.\n\nManual download\nCHIRTS data for Africa can be downloaded directly from the Climate Hazards Center.\nData are distributed as NetCDF files, a common format for distributing scientific raster data structures. NetCDF files contain metadata about the file contents (for instance, about the temporal or spatial dimensions of the data), which will be useful later when we aggregate data to the monthly level.\nYou’ll notice that the files—each of which contains a full year’s worth of data for the entire continent of Africa—contain 3.3 Gigabytes of data apiece. For this demonstration, we’ll therefore only download a single year of data.\n\n\n\n\n\n\nClimatological Normals\n\n\n\nWhen dealing with environmental data, it’s often necessary to have a long time series of data to establish a stable climatological normal, or baseline, to which to compare current observations. 30-year normals are commonly used, but their use has recently been questioned due to the acceleration of extreme weather frequency.4\nBecause of the space and time required to obtain data to calculate normals at a high resolution, we won’t be creating normals from CHIRTS in this post, but you may see them in the literature.\n\n\nWe’re working with the 2012 Mali sample from IPUMS DHS for this example, so we’ll download the Tmax.2012.nc file from the CHC listing. We’ve placed this file in the data directory.\nFortunately, we don’t need to learn any new tools to handle this file, as support for NetCDF is already built into {terra}. We can easily load the raster with rast():\n\nml_chirts &lt;- rast(\"data/Tmax.2012.nc\")\n\nIn this post we’ll be particularly interested in the temporal dimension of our raster data. Typically, a raster stack will represent time in layers, where each layer represents the recorded values at a particular point in time. terra’s SpatRaster objects have a built-in representation of time, which can be accessed with the time() function:\n\ntime(ml_chirts)\n#&gt;   [1] \"2012-01-01\" \"2012-01-02\" \"2012-01-03\" \"2012-01-04\" \"2012-01-05\"\n#&gt;   [6] \"2012-01-06\" \"2012-01-07\" \"2012-01-08\" \"2012-01-09\" \"2012-01-10\"\n#&gt;  [11] \"2012-01-11\" \"2012-01-12\" \"2012-01-13\" \"2012-01-14\" \"2012-01-15\"\n....\n\nWe can see that the temporal information contained in the NetCDF file was automatically included when we loaded this raster into R. Each of these dates correspond to a layer in the SpatRaster. This temporal representation will become useful when we aggregate temperature to the monthly level.\n\nCrop CHIRTS raster\nDownloading data from the CHC provides a raster for the entire African continent. We can greatly speed up our future processing by cropping this raster to our area of interest using the Mali borders that we loaded above.\nFirst, we’ll add a 10 kilometer buffer around the country border so that we retain the CHIRTS data just outside of the country as well. That way, if any DHS clusters fall in the border regions of the country, we will still be able to calculate temperature metrics in their general vicinity.\nWe’ve covered buffering in the past if you need to refresh your memory on this process.\n\n# Transform to UTM 29N coordinates, buffer, and convert back to WGS84\nml_borders_buffer &lt;- ml_borders_out |&gt; \n  st_transform(crs = 32629) |&gt; \n  st_buffer(dist = 10000) |&gt;\n  st_transform(crs = 4326)\n\nFinally, we can crop the CHIRTS raster to our buffered border region with terra’s crop():\n\nml_chirts &lt;- crop(ml_chirts, ml_borders_buffer, snap = \"out\")\n\n\n\n\nAccess via the chirps package\nYou may recall from our previous technical post that CHC data can be obtained via the {chirps} package in R. This package provides access to CHIRTS data as well.\nYou can obtain CHIRTS from the chirps package by providing a spatial boundary representing the area of interest for which the CHIRTS raster data should be obtained. You’ll also need to specify a temporal range and temperature variable.\nIn this case, we’ll use the buffered administrative borders for Mali that we downloaded earlier, specify the 2012 time range, and select the \"Tmax\" variable:\n\nml_chirts2 &lt;- chirps::get_chirts(\n  vect(ml_borders_buffer), \n  dates = c(\"2012-01-01\", \"2012-12-31\"),\n  var = \"Tmax\"\n)\n\n\n\nWe convert our administrative borders to a terra SpatVector object with vect() because this is the spatial structure expected by get_chirts().\nIn contrast to the NetCDF files provided when downloading CHIRTS data directly, obtaining data via the chirps package does not provide any temporal metadata:\n\ntime(ml_chirts2)\n#&gt;   [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n#&gt;  [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n#&gt;  [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n....\n\nSince we know that we have a full year of data, we can construct the temporal metadata manually. We’ll use the {lubridate} package to make this a little easier:\n\n# Convert strings to Date objects specifying year-month-day (ymd) format:\nstart &lt;- lubridate::ymd(\"2012-01-01\")\nend &lt;- lubridate::ymd(\"2012-12-31\")\n\n# Set time as a daily sequence of dates for all of 2012\ntime(ml_chirts2) &lt;- seq(start, end, by = \"days\")\n\ntime(ml_chirts2)\n#&gt;   [1] \"2012-01-01\" \"2012-01-02\" \"2012-01-03\" \"2012-01-04\" \"2012-01-05\"\n#&gt;   [6] \"2012-01-06\" \"2012-01-07\" \"2012-01-08\" \"2012-01-09\" \"2012-01-10\"\n#&gt;  [11] \"2012-01-11\" \"2012-01-12\" \"2012-01-13\" \"2012-01-14\" \"2012-01-15\"\n....\n\n\n\n\n\n\n\nCaution\n\n\n\nManually attaching time units works in this case because we know the CHIRTS data have no gaps. However, there’s no built-in check to ensure that you’re attaching the correct date to each layer of the raster stack, so you’ll want to be sure you know what time units are truly represented by each layer before assigning them manually.\n\n\nAt this point, we should have a daily raster for the region around Mali. We can take a peek by mapping the temperature distribution on a single day of our CHIRTS time series:\n\n\nWe’re not going to explicitly demonstrate how we produce our maps in this post since some are fairly complicated to set up. If you’re curious, you can peek at the collapsed code blocks to see how each of our maps are produced.\n\n\nShow plot functions\n# Add map scale bar and update guides\ntheme_dhs_map &lt;- function(show_scale = TRUE, continuous = TRUE) {\n  if (show_scale) {\n    scale &lt;- annotation_scale(\n      aes(style = \"ticks\", location = \"br\"), \n      text_col = \"#999999\",\n      line_col = \"#999999\",\n      height = unit(0.2, \"cm\")\n    )\n  } else {\n    scale &lt;- NULL\n  }\n  \n  if (continuous) {\n    guide &lt;- guides(\n      fill = guide_colorbar(draw.llim = FALSE, draw.ulim = FALSE)\n    )\n  } else {\n    guide &lt;- guides(\n      fill = guide_colorsteps(draw.llim = FALSE, draw.ulim = FALSE)\n    )\n  }\n  \n  list(scale, guide)\n}\n\n# Define custom palette functions so we can easily reproduce\n# color schemes across our maps in this post\nchirts_palettes &lt;- function() {\n  list(\n    main = c(\"#bad3e8\", \"#ffd3a3\", \"#da5831\", \"#872e38\"),\n    diff = c(\"#5B3794\", \"#8F4D9F\", \"#B76AA8\", \"#D78CB1\", \"#F1B1BE\", \"#F8DCD9\")\n  )\n}\n\n# Continuous fill scale for a selected palette\nscale_chirts_c &lt;- function(pal = \"main\", ...) {\n  pal &lt;- chirts_palettes()[[pal]]\n  colorRampPalette(pal, ...)\n}\n\n# ggplot2 layer for continuous scale for selected palette\nscale_fill_chirts_c &lt;- function(pal = \"main\", na.value = NA, ...) {\n  pal &lt;- scale_chirts_c(pal)\n  ggplot2::scale_fill_gradientn(colors = pal(256), na.value = na.value, ...)\n}\n\n# ggplot2 layer for binned scale for selected palette\nscale_fill_chirts_b &lt;- function(pal = \"main\", na.value = NA, ...) {\n  pal &lt;- scale_chirts_c(pal)\n  ggplot2::scale_fill_stepsn(colors = pal(256), na.value = na.value, ...)\n}\n\n\n\n\nShow plot code\n# Split raster into two at the country border to allow for differential \n# highlighting\nr_in &lt;- mask(ml_chirts[[1]], ml_borders_out, inverse = FALSE)\nr_out &lt;- mask(ml_chirts[[1]], ml_borders_out, inverse = TRUE)\n\n# Plot\nggplot() +\n  layer_spatial(r_in, alpha = 0.9, na.rm = TRUE) +\n  layer_spatial(r_out, alpha = 0.3, na.rm = TRUE) +\n  layer_spatial(ml_borders_neat, fill = NA, color = \"#eeeeee\") +\n  layer_spatial(ml_borders_out, fill = NA, color = \"#7f7f7f\", linewidth = 0.5) +\n  labs(\n    title = \"Daily Maximum Temperature: Mali\",\n    subtitle = \"January 1, 2012\",\n    fill = \"Temperature (°C)\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  ) +\n  scale_fill_chirts_c(limits = c(18, 50)) +\n  theme_dhs_map()"
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#average-monthly-temperature",
    "href": "posts/2024-04-15-chirts-metrics/index.html#average-monthly-temperature",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "1. Average monthly temperature",
    "text": "1. Average monthly temperature\nFor our first metric, we’ll take a simple approach and calculate an average monthly temperature.\nPreviously, we introduced terra’s mean() method, which allows you to calculate a single mean across all layers of a SpatRaster object. In this case, we also want to calculate a mean, but we need to adapt our approach so we can do so for each month independently.\nEnter terra’s tapp() function. tapp() allows you to apply a function to groups of raster layers. You can indicate which layers should be grouped together with the index argument. For instance, to independently calculate the mean of the first three layers and the second three layers, we could use the following index:\n\ntapp(\n  ml_chirts[[1:6]], # Select the first 6 layers to demo\n  fun = mean, \n  index = c(1, 1, 1, 2, 2, 2)\n)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 \n#&gt; source(s)   : memory\n#&gt; names       :       X1,       X2 \n#&gt; min values  : 20.32146, 21.20080 \n#&gt; max values  : 35.77333, 35.86034\n\nNotice that this produces an output SpatRaster with 2 layers: the first represents the mean of the first 3 layers in the input, and the second represents the mean of the next 3 layers.\nHowever, manually identifying the index layers for each month of the year would be tedious and error-prone. Not only would we have to type out code to handle 366 days of data, but we would also have to contend with the fact that months vary in length. And depending on your time frame and region of interest, you may also need to account for leap years or entirely different calendars!\nFortunately, terra provides us with an easier way. Because we have a time component to our data, we can use the temporal metadata already attached to our raster as the index. For instance, to aggregate by month, simply use index = \"months\":\n\nml_chirts_mean &lt;- tapp(ml_chirts, fun = mean, index = \"months\")\n\nml_chirts_mean\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 12  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n#&gt; source(s)   : memory\n#&gt; names       :      m_1,      m_2,      m_3,      m_4,      m_5,      m_6, ... \n#&gt; min values  : 20.13009, 21.27224, 27.66812, 29.93578, 29.76501, 26.91157, ... \n#&gt; max values  : 35.44701, 38.60472, 40.72202, 42.85499, 44.10707, 47.46994, ... \n#&gt; time (mnts) : Jan to Dec\n\nAs expected, we now have 12 layers in our output SpatRaster (see the dimensions component of the output above). Each layer contains the average daily maximum temperature (in degrees Celsius) for the given month, as shown below.\n\n\nShow plot code\nlibrary(patchwork)\n\n# Helper to split raster layers into a list for small-multiple panel mapping\nsplit_raster &lt;- function(r) {\n  purrr::map(seq_len(nlyr(r)), function(i) r[[i]])\n}\n\n# Function to build individual panels for a small-multiple map using \n# continuous color scheme\nchirts_panel_continuous &lt;- function(x, \n                                    panel_title = \"\",\n                                    show_scale = TRUE,\n                                    ...) {\n  r_in &lt;- mask(x, ml_borders_out, inverse = FALSE)\n  \n  ggplot() + \n    layer_spatial(r_in, alpha = 0.9, na.rm = TRUE) +\n    layer_spatial(ml_borders_neat, fill = NA, color = \"#eeeeee\") +\n    layer_spatial(ml_borders_out, fill = NA, color = \"#7f7f7f\") +\n    labs(subtitle = panel_title, fill = \"Temperature (°C)\") +\n    scale_fill_chirts_c(...) +\n    theme_dhs_map(show_scale = show_scale) +\n    theme(\n      axis.text.x = element_blank(), \n      axis.text.y = element_blank(),\n      plot.subtitle = element_text(hjust = 0.5, size = 12),\n      panel.grid = element_blank()\n    )\n}\n\n# Split raster by layer\nr &lt;- split_raster(ml_chirts_mean)\n\n# Show scale only on final panel\nshow_scale &lt;- c(rep(FALSE, length(r) - 1), TRUE)\n\n# Panel labels\nmonths &lt;- c(\"January\", \"February\", \"March\", \"April\", \n            \"May\", \"June\", \"July\", \"August\",\n            \"September\", \"October\", \"November\", \"December\")\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_continuous(x, y, z, limits = c(18, 50))\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Average Monthly Temperature: Mali 2012\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\n\n\nNote that the CHC does provide a CHIRTS product that has been pre-aggregated to the monthly level. For projects relying on average monthly temperature, this is likely a better option than manually aggregating more fine-grained CHIRTS data.\nHowever, the advantage to working with daily CHIRTS data is that we have the flexibility to calculate our own custom monthly temperature metrics that aren’t necessarily provided out-of-the-box. We’ll demonstrate one in the next section."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#days-above",
    "href": "posts/2024-04-15-chirts-metrics/index.html#days-above",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "2. Days above a temperature threshold",
    "text": "2. Days above a temperature threshold\nAverage temperature may be a straightforward monthly temperature metric, but it doesn’t do a very good job of capturing acute temperature anomalies. When it comes to human health, evidence suggests that temperatures above certain thresholds are associated with physiological impairments, though the precise threshold depends on several factors.3 Regardless, these extreme events may be masked when we average across an entire month.\nTo explore this possibility, we’ll calculate the proportion of days in each month that exceed a certain raw temperature threshold. We’ll use 35°C as our threshold, which represents a value near the upper end of Mali’s temperature distribution1 and is similar to other commonly used (though occasionally questionable) thresholds.3\n\n\nTypically, you would want a more thorough justification of your threshold temperature. This post is not intended to represent a real analysis, so we select 35°C for the purposes of demonstration.\n\nLogical raster operations\nWe can exploit the fact that terra supports logical operations on SpatRaster objects to help us calculate this metric. For instance, we can compare the entire raster to a set value with the familiar &gt; operator:\n\nml_bin &lt;- ml_chirts &gt; 35\n\nThis produces a binary raster, where each pixel in each layer receives a logical value based on whether it is above or below 35 degrees Celsius (note that the min values and max values for each layer are now TRUE or FALSE):\n\nml_bin\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 366  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n#&gt; source      : spat_d3403e6edae9_54080_GdaPwXHptgIOPSx.tif \n#&gt; varname     : Tmax (Climate Hazards Center Tmax) \n#&gt; names       : Tmax_1, Tmax_2, Tmax_3, Tmax_4, Tmax_5, Tmax_6, ... \n#&gt; min values  :  FALSE,  FALSE,  FALSE,  FALSE,  FALSE,  FALSE, ... \n#&gt; max values  :   TRUE,   TRUE,   TRUE,   TRUE,   TRUE,   TRUE, ... \n#&gt; time (days) : 2012-01-01 to 2012-12-31\n\nWe can use this to count the number of days above the 35°C threshold for each pixel in our raster. We simply need to sum the binary raster layers within each month to count the number of days that exceed the threshold at each pixel location.\n\n\nWhen treating logical values as numeric, TRUE is treated as a 1 and FALSE is treated as a 0.\nHowever, we also need to account for the fact that not all months contain the same number of days. We therefore produce a proportion of each month’s days that meet our temperature threshold by dividing by the number of days in each month.\nFor binary data, this turns out to be the same as calculating a mean, so we can use a similar approach as we did when calculating average monthly temperature. The difference is that we now provide our binary raster (ml_bin) to the tapp() function:\n\nml_prop &lt;- tapp(ml_bin, fun = mean, index = \"months\")\n\nOnce again, we end up with a SpatRaster with 12 layers. However, in this case, each raster pixel reflects the proportion of days above 35° in that month.\n\n\nShow plot code\n# Function to build individual panels for a small-multiple map using \n# binned color scheme\nchirts_panel_binned &lt;- function(x, \n                                panel_title = \"\",\n                                fill_label = \"\",\n                                show_scale = TRUE,\n                                ...) {\n  r_in &lt;- mask(x, ml_borders_out, inverse = FALSE)\n  \n  ggplot() + \n    layer_spatial(r_in, alpha = 0.9, na.rm = TRUE) +\n    layer_spatial(ml_borders_neat, fill = NA, color = \"#eeeeee\") +\n    layer_spatial(ml_borders_out, fill = NA, color = \"#7f7f7f\") +\n    labs(subtitle = panel_title, fill = fill_label) +\n    scale_fill_chirts_b(...) +\n    theme_dhs_map(show_scale = show_scale) +\n    theme(\n      axis.text.x = element_blank(), \n      axis.text.y = element_blank(),\n      plot.subtitle = element_text(hjust = 0.5, size = 12),\n      panel.grid = element_blank(),\n      legend.ticks = element_blank()\n    )\n}\n\n# Split raster by layer\nr &lt;- split_raster(ml_prop)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    n.breaks = 8, \n    limits = c(0, 1), \n    fill_label = \"Proportion of Days Above 35°C\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Proportion of Days Above Threshold\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\n\n\nThis approach reveals a bit more detail about the consistency of the temperature exposure during certain months. As we can see, some months are spent almost entirely above 35°C across the country. This continual exposure may be more strongly related to health than the averages we calculated in the section above."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#heatwaves-consecutive-days-above-a-temperature-threshold",
    "href": "posts/2024-04-15-chirts-metrics/index.html#heatwaves-consecutive-days-above-a-temperature-threshold",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "3. Heatwaves: Consecutive days above a temperature threshold",
    "text": "3. Heatwaves: Consecutive days above a temperature threshold\nContinual exposure to high temperatures likely has a more pronounced health impact than isolated hot days. Accordingly, most heatwave definitions attempt to identify sequences of days that meet certain temperature criteria.\nWe can build upon our previous temperature metric to produce a simple heatwave definition that identifies all days that belong to a sequence of 3+ days in a row that all meet the 35°C threshold used earlier.\nHow should we go about identifying sequences of days? To simplify things, let’s pull out the daily CHIRTS values for a single pixel in our raster to use as an example:\n\n# Extract values from a single pixel of CHIRTS data\npx1 &lt;- as.numeric(ml_chirts[1, 1])\n\npx1\n#&gt;   [1] 22.08196 22.25259 24.63318 25.46410 25.11430 24.69748 25.71723 27.64781\n#&gt;   [9] 25.00261 22.27929 24.83206 22.42412 24.87811 28.53550 27.20606 26.75240\n#&gt;  [17] 22.12105 22.98229 20.55801 19.10251 20.74286 22.91010 22.37615 22.23903\n....\n\nAs we demonstrated above, it’s easy to identify the layers that are above a given threshold:\n\npx1_bin &lt;- px1 &gt; 35\n\npx1_bin\n#&gt;   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt;  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt;  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n....\n\nBut how do we extract sequences from this vector? One option is to exploit the features of run-length encoding. Run-length encoding represents a vector of values as a sequence of runs of a single value and the length of that run.\nWe can use the rle() function from base R to convert to run-length encoding:\n\npx1_rle &lt;- rle(px1_bin)\n\npx1_rle\n#&gt; Run Length Encoding\n#&gt;   lengths: int [1:23] 81 1 16 1 30 11 3 3 7 1 ...\n#&gt;   values : logi [1:23] FALSE TRUE FALSE TRUE FALSE TRUE ...\n\nThe output shows us that the px1_bin vector starts with a run of 81 FALSE values, then has a run of 1 TRUE value, then 16 FALSE values, and so on. As you can see, run-length encoding provides us both with information about the values and the sequencing of our input vector.\nIf we define a heatwave as any sequence of 3+ days above the temperature threshold, each heatwave will be represented by entries with a value of TRUE (days that exceeded the threshold) and a length (number of days in a row) of 3 or more. We can use logical operations to identify whether each run is a heatwave or not:\n\n# Identify days that were above threshold and belonged to a 3+ day sequence\nis_heatwave &lt;- px1_rle$values & (px1_rle$lengths &gt;= 3)\n\nis_heatwave\n#&gt;  [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n#&gt; [13] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\nSumming this vector will give us the number of unique heatwave events during the year for this sample pixel:\n\nsum(is_heatwave)\n#&gt; [1] 8\n\nWe could also get the proportion of days that are within those heatwaves by summing the lengths of all the heatwave events and dividing by the number of total days:\n\n# Extract lengths for each heatwave event\nheatwave_lengths &lt;- px1_rle$lengths[is_heatwave]\n\nheatwave_lengths\n#&gt; [1]  11   3   4 101   4   8   3   7\n\n# Proportion of days belonging to a heatwave\nsum(heatwave_lengths) / length(px1)\n#&gt; [1] 0.3852459\n\n\nCustom functions\nIf we want to calculate the proportion of heatwave days across our entire raster, we can again return to tapp(). However, there’s no built-in function (like mean) that we can use to do the processing we walked through above.\nInstead, we must provide our own custom function to indicate the processing that tapp() should perform on each set of layers. To define a function, we use the function() keyword along with several arguments.\n\n\n\n\n\n\nNote\n\n\n\nA function’s arguments are the input parameters that the user can set when calling the function.\n\n\nAt its simplest, all this requires is copying the code we’ve already written above (we’ve consolidated the code into single lines in some cases):\n\nprop_heatwave &lt;- function(temps) {\n  # Convert to RLE of days above threshold\n  bin_rle &lt;- rle(temps &gt;= 35)\n  \n  # Identify heatwave events based on sequence length\n  is_heatwave &lt;- bin_rle$values & (bin_rle$lengths &gt;= 3)\n  \n  # Count heatwave days and divide by total number of days\n  sum(bin_rle$lengths[is_heatwave]) / length(temps)\n}\n\nNote that we do the exact same processing as we did in our step-by-step walkthrough. The only difference is that instead of using the px1 variable (which contains the values for a single pixel), we instead use a more general argument, which we call temps. temps stands in for any arbitrary input vector that the function user can provide (we’ve named it temps to help make it clear that this should be a vector of temperature values). This means that we can easily run the same calculation on different vectors:\n\n# Extract 2 pixels for demonstration\npx1 &lt;- as.numeric(ml_chirts[1, 1])\npx2 &lt;- as.numeric(ml_chirts[1, 2])\n\n# Calculate heatwave proportions for each pixel\nprop_heatwave(px1)\n#&gt; [1] 0.3852459\n\nprop_heatwave(px2)\n#&gt; [1] 0.3852459\n\n\nWriting a more flexible function\nRight now, the user of the function has no way to modify the temperature threshold or sequence length used in the heatwave definition, because those values (35 and 3) are hard-coded into our function.\nIf we move these values to the function arguments, we allow the user to decide what values these parameters should take. Here is a modified version of prop_heatwave() that does this:\n\nprop_heatwave &lt;- function(temps, thresh, n_seq) {\n  # Convert to RLE of days above threshold\n  bin_rle &lt;- rle(temps &gt;= thresh)\n  \n  # Identify heatwave events based on sequence length\n  is_heatwave &lt;- bin_rle$values & (bin_rle$lengths &gt;= n_seq)\n  \n  # Count heatwave days and divide by total number of days\n  sum(bin_rle$lengths[is_heatwave]) / length(temps)\n}\n\nOur function now has a thresh argument and an n_seq argument. Where we previously would have compared our input temps vector to the threshold of 35, we now compare it to the value the user provides to thresh. Similarly, where we previously would have used sequences of length 3 or more, we now use the sequence length the user provides to n_seq.\nUsing the values of 35 and 3 produces the same heatwave proportion as above:\n\nprop_heatwave(px1, thresh = 35, n_seq = 3)\n#&gt; [1] 0.3852459\n\nBut now we can easily change our inputs to calculate modified heatwave definitions. For instance, to find the proportion of days in 4+ day heatwaves of at least 37°C:\n\nprop_heatwave(px1, thresh = 37, n_seq = 4)\n#&gt; [1] 0.3114754\n\nThis example demonstrates how function arguments can be used to produce a more flexible function that can be easily applied across a range of input parameters. Building functions in this way has a bit of an up-front cost, but it often pays for itself by making your future analysis much more robust and scalable.\n\n\n\nScaling up\nNow that we have a function to calculate our heatwave definition, we can provide it to tapp() with our desired temperature threshold and sequence parameters.\n\n# Apply our custom heatwave counter to each month\nml_heatwave_prop &lt;- tapp(\n  ml_chirts, \n  fun = function(x) prop_heatwave(x, thresh = 35, n_seq = 3), \n  index = \"months\"\n)\n\n\n\n\n\n\n\nAnonymous Function Syntax\n\n\n\nOur fun argument above is written in anonymous function syntax. It may look a little complex, but remember that tapp() expects you to provide a function to its fun argument. Whichever function you provide should be a function of the vector of values for each pixel in the input raster.\nfunction(x) prop_heatwave(x, ...) indicates that we want to provide each of these vectors x to our prop_heatwave() function (as the temps argument). The thresh and n_seq arguments are fixed across all pixels. (In this case, x is just a placeholder to reference the vector inputs to our function. We could just as easily use another name, but x is traditional and concise.)\nWhy didn’t we use this syntax in previous sections? Well, it turns out that fun = mean was simply a shorthand. Writing fun = function(x) mean(x) would have also worked!\n\n\nLet’s see what the distribution of heatwave days looks like under our latest definition:\n\n\nShow plot code\n# Split raster by layer\nr &lt;- split_raster(ml_heatwave_prop)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    n.breaks = 8, \n    limits = c(0, 1),\n    fill_label = \"Proportion of days in a heatwave\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Proportion of heatwave days\",\n    subtitle = \"Heatwaves as sequences of 3+ days of 35°C\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\n\n\nBecause we’ve built a flexible prop_heatwave() function, we can easily calculate a different heatwave definition. For heatwaves of 4+ days of 37°C+, for instance:\n\nml_heatwave_prop2 &lt;- tapp(\n  ml_chirts, \n  fun = function(x) prop_heatwave(x, thresh = 37, n_seq = 4), \n  index = \"months\"\n)\n\n\n\nShow plot code\n# Split raster by layer\nr &lt;- split_raster(ml_heatwave_prop2)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    n.breaks = 8, \n    limits = c(0, 1),\n    fill_label = \"Proportion of days in a heatwave\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Proportion of heatwave days\",\n    subtitle = \"Heatwaves as sequences of 4+ days of 37°C\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\n\n\nIt can be difficult to see the differences side-by-side, but we can subtract the output rasters to easily examine the differences between the two definitions:\n\nml_heatwave_diff &lt;- ml_heatwave_prop2 - ml_heatwave_prop\n\n\n\nShow plot code\n# Convert 0 values to NA for transparency\nNAflag(ml_heatwave_diff) &lt;- 0\n\n# Split raster into layers\nr &lt;- split_raster(ml_heatwave_diff)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    pal = \"diff\", \n    n.breaks = 8, \n    limits = c(-1, 0),\n    fill_label = \"Difference in proportion of days in heatwave\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Difference in heatwave proportion metrics\",\n    subtitle = \"37°C 4+ days vs. 35°C 3+ days heatwave definitions\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\n\n\nAs we can see, modifying our definition does indeed produce meaningful differences in the prevalence of heatwave days in various parts of the country. Which definition is appropriate will depend on your particular research questions and aims. However, building custom functions into your workflow will make it much easier to quickly compare different definitions, facilitating sensitivity analyses and robustness checks."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#getting-help",
    "href": "posts/2024-04-15-chirts-metrics/index.html#getting-help",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html",
    "href": "posts/2024-02-23-rainfall-insights/index.html",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "",
    "text": "One barrier to research on extreme weather and health is that many surveys—including the The Demographic and Health Surveys Program (DHS)—do not collect detailed information on local environmental conditions in survey areas. Fortunately, organizations like IPUMS DHS are now taking steps to use DHS enumeration cluster coordinates to link a limited set of environmental variables—including monthly and long-term average precipitation—to individual and household records within the DHS.1\nHowever, in many cases, these pre-calculated time series may not meet researchers’ needs because they reflect only average conditions and not the experience of the specific surveyed households. To obtain a more nuanced understanding of a survey response’s environmental context, some researchers may choose to calculate and link environmental variables themselves. We introduced some methods for doing so in our recent post, where we demonstrated how to integrate raw precipitation data from the Climate Hazards Center InfraRed Precipitation with Station Dataset (CHIRPS) with survey data from IPUMS DHS.\nThis post aims to serve as a guide for new researchers wading into research on the health impacts of extreme weather events. While we focus on rainfall, it is important to note that there are significant interactions between rainfall and other variables such as temperature and vegetation. Furthermore, rainfall alone may be a crude measure; for instance, Grace et al. (2021) highlight multiple and nuanced ways in which precipitation can impact health.2 Nevertheless, this blog post outlines key concepts to consider when thinking through the health impacts of precipitation patterns, selecting a data source, and operationalizing precipitation data. We draw examples from recent publications that utilize DHS data; however, their inclusion does not constitute an endorsement of the publications themselves."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#impacts-of-rainfall-on-human-health",
    "href": "posts/2024-02-23-rainfall-insights/index.html#impacts-of-rainfall-on-human-health",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Impacts of rainfall on human health",
    "text": "Impacts of rainfall on human health\nRainfall can impact human health in both direct and indirect ways, with important consequences when linking precipitation data to survey responses. For instance, researchers interested in indirect mechanisms may need to incorporate a time lag between extreme weather events and survey response dates, since the impact of rainfall events in one period may not appear until later.3\nBelow, we outline several of the most common pathways through which precipitation impacts health outcomes.\n\nDisasters and precipitation extremes\nDirect and immediate impacts on human health\nExtreme rainfall can result in flooding or landslides, which may lead to death or injury. Furthermore, extreme events often contribute to indirect health impacts through the other pathways described below.\n\n\nAgriculture and nutrition\nIndirect impacts on human health\nRainfall directly impacts crop growth and agricultural productivity, which then affects food security and nutrition. It takes time for plants to grow, so the impacts on health and well-being may not be immediate.\nPrecipitation is not the only variable that can be used to explore these pathways; Normalized Difference Vegetation Index (NDVI) can also be used as a proxy for food security.2\n\n\nInfectious diseases\nIndirect impacts on human health\nRainfall can impact the transmission of enteric disease like cholera (through its effects on sanitation practices, water quality, and hygiene) as well as vector-borne diseases like malaria and dengue (through its effects on water supply and vector habitats).\nIdeally, researchers would be able to explore these pathways through more proximate measures of transmission risks like disease incidence. However, in resource-poor settings these data may not be available at fine temporal and spatial resolutions.\n\n\nCrime and violence\nIndirect impacts on human health\nRainfall shocks may be linked to crime and violence through their impacts on economic activities, stress, and competition for limited resources. For instance, rainfall impacts on economic security may lead to herder and farmer conflicts."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#choosing-a-rainfall-data-source",
    "href": "posts/2024-02-23-rainfall-insights/index.html#choosing-a-rainfall-data-source",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Choosing a rainfall data source",
    "text": "Choosing a rainfall data source\nThe specific research question and estimation strategy should play a central role in determining the most suitable rainfall data source. Researchers have various factors to consider when making this choice, including the available time span4,5, the manner in which data were collected (for instance, accurate ground-based station data versus satellite data),6 and the extent of coverage for the geographic location of interest.7\n\nData accuracy\nIt’s important for researchers to recognize the trade-offs between data accuracy and coverage. In many cases, more accurate products will have more limited geographic coverage, so researchers with a small area of interest may be able to use a more accurate data product.\n\n\nTemporal resolution\nResearchers must also consider the temporal resolution of the data (e.g. hourly, daily, monthly, etc.). An appropriate temporal resolution is likely to depend on the characteristics of the survey data that will be joined to the environmental data. For instance, it may not be necessary to use hourly precipitation data when linking to the DHS, since survey response dates are often provided only at the monthly level. However, it is always possible to aggregate fine-grained data to a larger temporal scale, and fine-grained data may provide more flexibility in the way aggregation is carried out.\n\n\nSpatial resolution\nThe majority of rainfall data is available in raster format, wherein precipitation data are stored in a grid of cells, each with a particular precipitation value. If the grid cells are very large and encompass many DHS clusters, there won’t be enough variation to exploit in an analysis. On the other hand, if the grid cells are less than 10 kilometers across (a plausible buffer size around DHS cluster coordinates), then their values will need to be aggregated within each DHS cluster region.\n\n\nData accessibility\nOf course, data availability and accessibility often drive data source decisions. Researchers may opt to use data that they or collaborators have previously used or have easy access to in order to reduce the difficulty of setting up an analysis with a new source.\n\nComparison of Selected Precipitation Data Sources\n\n\nName\nFinest Resolution\nGeo. Range\nTemp. Range\nTime Step\nRainfall Data Source\n\n\n\n\nCHIRPS\n0.05°\n50°S-50°N (all longitudes)\n1981-near present\nDaily, pentad, dekad, monthly, 2-monthly, 3-monthly, annual\nWeather station records and geostationary satellite observations\n\n\nCRU TS\n0.5°\nAll land except Antarctica\n1901-2022\nMonthly\nWeather station records\n\n\nUDEL-TS\n0.5°\n89.75°N-89.75°S, 0.25°E-359.75°E\n1901-2014\nMonthly\nWeather station records\n\n\nGPCC\n0.25°\n90.0°N-90.0°S, 0.0°E-360.0°E\n1891-2019\nMonthly\nWeather station records\n\n\nERA5\n0.25°\nGlobal\n1940-present\nHourly, monthly\nReanalysis (model estimates from satellite data assimilation)"
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#measuring-rainfall",
    "href": "posts/2024-02-23-rainfall-insights/index.html#measuring-rainfall",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Measuring rainfall",
    "text": "Measuring rainfall\nScholars frequently aggregate total rainfall over an interval (for instance, to calculate annual, seasonal, or monthly precipitation summaries). Additionally, researchers often generate long-term rainfall averages for use in identifying anomalies and extreme rainfall events.3 It is imperative that researchers justify their choice of temporal aggregation.\nIn this section, we summarize several publications that highlight different techniques for measuring rainfall.\n\nTotal precipitation\nStudies that simply use raw precipitation totals are able to test whether additional rainfall has a negative or positive impact on the outcome of interest.8\n\nTotal precipitation during the growing season\nSome studies restrict their focus to a region’s growing season to better assess the impacts of environmental conditions on agricultural production and—indirectly—nutrition and food security.9\n\n\nTotal rainy season precipitation\nSpecific details of the geographical area of interest may inform the manner in which data are aggregated. For instance, Randell, Gray, and Shayo (2022) take region-specific rainfall patterns into account in research on Tanzania, where depending on the region there might be one (Msimu) or two rainy seasons (Masika and Vuli) during the year.10\n\n\n\nRainfall variability and anomalies\nMeasures of rainfall variability consider how rainfall at a specific time and location compares to the long-term average rainfall in that same location. However, several measures of variability exist, and information should be provided to explain how the reference period was chosen and how results change if the reference period changes.\n\nAnnual variability\n\nRainfall deviation percentile can be used to quantify how a specific rainfall event compares to historical data for that same location. For instance, in Epstein et al. (2023), the 50th percentile represents a year with median rainfall levels compared to 29 previous years; numbers closer to 0 represent drier than average years and closer to 1 represent wetter years.11\nRainfall Z-scores quantify how many standard deviations a rainfall event differs from the mean. Negative Z-scores indicate below-average rainfall while positive Z-scores indicate above-average rainfall.12\nStandardized Precipitation Index (SPI) is similar to the Z-score but first corrects for the skew found in rainfall distributions by transforming the data using a gamma distribution.5\n\n\n\nSeasonal variability\nRainfall is often seasonal, and depending on the health pathway of interest, researchers should think critically about how seasons may impact their analyses. Rainfall during the dry season is likely to have a different effect than rainfall in the wet season, for instance.13\n\nRandell et al. (2021) calculate Z-scores for 2015 monsoon rainfall based on monsoon rainfalls during 1980-2015 reference period.14\nOmiat and Shively (2020) calculate deviation during the main rainfall season of the survey year and of the previous year.15\nAbiona (2017) calculates the percentage deviation from the mean agricultural season average. Specifically, this was generated using the natural logarithm of the current agricultural season minus the 30-year historical average for the same locality.4\n\n\n\nExtreme weather variability during specific windows in the respondent’s life course\nEnvironmental analyses should be tied to survey responses during specific exposure windows to determine the impact of extremes.3 For instance, research on the impact of precipitation on weight at birth may want to focus on precipitation anomalies during the months of gestation. Or, if gestational length is unknown, researchers may choose to use anomalies during the 12 months prior to birth. It is often necessary to make approximations like this based on the available data.\n\nIn studying in-utero rainfall variability, Le and Nguyen (2021) use “the deviation of the nine-month in-utero rainfall from the long run average of total rainfall during those nine months”.6 In this case, the long-run average was based on data from 1981-2018. The authors further dichotomized their results in order to more easily summarize the impact of wet versus dry shocks.\n\n\n\n\nPrecipitation extremes: Floods and droughts\nIt is uncommon for researchers to use rainfall data alone to define a flood event. Instead, an indicator for flood is generated based on precipitation anomalies. Consequently, we recommend using the term extreme rainfall rather than flood event when relying solely on rainfall data.\nResearchers often use rainfall data alone to identify droughts, though in these instances they are really capturing meteorological droughts.16 Depending on the type of drought of interest, researchers may want to use a combination of temperature, rainfall, and other data sources to identify droughts associated with low soil moisture (agricultural droughts), low ground water, or surface runoff. As with all environmental metrics, researchers should think critically about time scales when operationalizing drought; a drought lasting 1 month is likely to have a very different effect than a drought lasting 6 or 12 months.\nSpecific drought indices are also available to identify drought conditions. The Standardized Precipitation Evaporation Index (SPEI) takes into account rainfall as well as evaporation, and the Palmer Drought Severity Index (PDSI) includes rainfall, evapotranspiration, and runoff.\nBelow we highlight some ways researchers using DHS data have used rainfall data to quantify the impacts of extreme precipitation. These examples illustrate that there are a wide range of cutoffs used to identify droughts. While this makes comparisons difficult, it’s appropriate that drought should be defined specific to the place, time, and water use patterns of a given region.\n\nExtreme rainfall: rainfall deviation ≥ 90th percentile17\nFlood: rainfall deviation &gt; 75th percentile4\nDrought: rainfall deviation is &lt; 25th percentile4\nDrought: binary variable for rainfall ≤ 15th percentile18\nDrought: SPI values &lt; -1.55\nDrought: classified as ordinal categorical variable:19\n\nSevere (≤ 10 percentile)\nMild/moderate (&gt; 10th percentile to ≤ 30th percentile)\nNone (&gt; 30th percentile)"
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#context-matters",
    "href": "posts/2024-02-23-rainfall-insights/index.html#context-matters",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Context matters",
    "text": "Context matters\n\nHow might the same shock have different effects in different regions?\nStudies that span geographic areas with heterogeneous climatic zones and other characteristics often stratify or interact rainfall with regional characteristics. This is because the same rainfall event most likely does not have the same impact on outcomes in areas with different geographic features (for instance, in arid versus non arid areas,20 rural versus urban areas,21 or on individuals with different livelihoods22).\nThis highlights the most important point when working with environmental data: no one-size-fits-all approach exists. The manner in which data are selected, operationalized, and analyzed must be consistently informed by the physical and cultural specifics of the geographic region under consideration.\nWe hope that these drops of wisdom are a good starting point for your research."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#getting-help",
    "href": "posts/2024-02-23-rainfall-insights/index.html#getting-help",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html",
    "href": "posts/2024-02-03-climate-theory/index.html",
    "title": "Developing Robust Conceptual Models in Research on Extreme Weather and Health",
    "section": "",
    "text": "Unpacking the relationship between weather extremes and health requires not only technical skills, but also a strong theoretical foundation. Extreme weather, health, and their relationships are all complex. Researchers begin by conceptualizing—for their specific questions—1) the nature of the extreme weather event, 2) the details of the health outcome, and 3) the temporal and spatial relationships between extreme weather and the health phenomena of interest.\nBeyond these three elements, contextual factors are crucial for identifying the complex causal pathways among weather extremes and health. Figure 1 illustrates how the pathway from an extreme weather event, its manifestation in the environment, and its impact on various health outcomes is embedded within larger contexts. As suggested by the gray box on the right, individuals will be impacted by extreme weather in unique ways based on their demographic characteristics and social determinants that affect their lived experiences. The gray box on the left illustrates how environmental and institutional contexts will also influence relationships between extreme weather events and health. Contexts influence the impact of exposures on individuals as well as their mitigation and adaptation strategies in response to extreme weather risks."
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html#building-a-conceptual-model",
    "href": "posts/2024-02-03-climate-theory/index.html#building-a-conceptual-model",
    "title": "Developing Robust Conceptual Models in Research on Extreme Weather and Health",
    "section": "Building a conceptual model",
    "text": "Building a conceptual model\nBelow, we provide the steps in creating a tailored conceptual model for research on extreme weather and health.\n\n1. Assessing the temporal and spatial nature of extreme weather risk\nRisks arising from weather extremes vary in their temporal and spatial scales. In terms of timing, risks may be acute or protracted. Acute risks arise when one-time exposure to an extreme weather event is sufficient to trigger health outcomes, for example, when flooding makes water unsafe for drinking. Protracted risks, in contrast, arise from sustained exposure to extreme weather, such as the inhalation of dust over the course of enduring dry periods. The impact of extreme weather is also spatially variable. While some events affect wide areas, others’ scope is limited to discrete locations. Determining the type of risk depends on both the nature of the extreme weather event and the health outcome studied. Identifying the temporal and spatial dimensions of risk provide the basis for decisions concerning temporal aggregation of environmental data, as well as the length of expected lags between initial exposure to an extreme weather event and the manifestation of the health outcome.\n\n\n2. Incorporating social and behavioral context\nCertain extreme weather events are known to have deleterious effects on humans. For example, exposure to extremely high temperatures during the first trimester of pregnancy increases the risk of low birth-weight babies.2 Basic biological truths such as this are insufficient, however, to fully assess the impact of extreme weather on individual health because individuals have dramatically different access to resources, opportunities, and constraints. A pregnant person who lives in an air-conditioned house will experience extreme temperatures very differently than a person who is houseless. Demographic characteristics, such as age, gender, wealth, and partnership status matter when assessing the impacts of weather extremes.\nFurthermore, the characteristics of societies (e.g., rural versus urban, gender roles, livelihoods) matter. Consider rising temperatures in nomadic versus other types of communities.3 For both, temperature change may make it harder to access water. In nomadic communities, the result could be men driving herds further from the community and thus staying away longer, while in other communities, the result could be that children tasked with collecting water find the task more physically demanding. Who and how individuals are affected by water shortages will vary depending on community norms and roles. Community characteristics will also affect the instigation and nature of mitigation and adaptation efforts. While analyses that map the nature of extreme weather over time provide a useful starting point, they cannot fully capture the individual- and community-specific impacts of extreme weather.3\n\n\n3. Understanding environmental and institutional contexts\nEnvironmental and institutional contexts are also influential in shaping causal pathways between weather extremes and health. The physical environment, such as altitude and the demarcation of seasons, can provide some level of protection from some extreme weather events. For instance, a small reduction in annual rainfall can have a devastating effect in an arid location where there is a close correspondence between a rainy season and agricultural production.4 The same reduction in rainfall might have only a minor impact in a location where annual precipitation is diffused throughout the year and is less closely tied to the agricultural season. Understanding such differences is important for selecting accurate measurements. In the former location, the rain shortfall might best be measured through the length of the rainy season. In the latter location, considering rainfall deviation from a monthly or annual average could be more appropriate.4\nInstitutional contexts include the built environment, such as the presence of roads, wells, and irrigation systems, as well as organizations designed to facilitate human capabilities, such as political, healthcare, and education systems. Local governments’ capacities to intervene to protect populations faced with risks of extreme weather can reduce negative health outcomes. Healthcare systems affect the accessibility of treatment (both physically and financially), while education systems provide individuals with the knowledge of when and how to respond to weather events. Extreme weather and health research that fails to capture these contextual effects can lead to faulty conclusions."
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html#conclusion",
    "href": "posts/2024-02-03-climate-theory/index.html#conclusion",
    "title": "Developing Robust Conceptual Models in Research on Extreme Weather and Health",
    "section": "Conclusion",
    "text": "Conclusion\nTheorizing the core relationship among specific extreme weather events and specific health outcomes is the first step in developing a conceptual model but is insufficient to fully capture complex causal pathways. Individual characteristics, social determinants of health, and environmental and institutional environments are also critical. Informed and clearly identified spatial and temporal measurement of environmental exposures is necessary, and these exposures should be thoughtfully, appropriately, and explicitly linked to the particular health outcomes of interest. Community-focused expertise and stakeholder engagement is vital to fully understand and incorporate how broader contexts interact with local circumstances to uniquely influence how weather extremes are experienced.\n\nFor more information\nTo learn more about the technical modeling implications of conceptual models of extreme weather and health, see Dorélien and Grace (2023).4\nTo see a specific model of the impact of weather extremes on women’s reproductive health in Africa, see Grace (2017).3\nFor further delineation of the elements in models of extreme weather and health (in the U.S. context), see Balbus et al. (2016).5"
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html#getting-help",
    "href": "posts/2024-02-03-climate-theory/index.html#getting-help",
    "title": "Developing Robust Conceptual Models in Research on Extreme Weather and Health",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html",
    "href": "posts/2024-02-01-getting-started-with-r/index.html",
    "title": "An Introduction to R",
    "section": "",
    "text": "The IPUMS DHS Spatial Analysis and Health Research Hub is designed to introduce spatial data concepts for researchers who work with population health survey data.\nTo do so, the blog will provide both conceptual content describing ways in which spatial data can be appropriately included in population research as well as technical content demonstrating how to implement some of these approaches using statistical software.\nTechnical content will rely primarily on the R programming language. Not only is R one of the most popular platforms for data analysis around the world, but its open-source nature aligns well with IPUMS values. R is freely available for Windows, MacOS, and a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux)."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html#getting-started-with-r",
    "href": "posts/2024-02-01-getting-started-with-r/index.html#getting-started-with-r",
    "title": "An Introduction to R",
    "section": "Getting started with R",
    "text": "Getting started with R\nTo download R, visit the Comprehensive R Archive Network (CRAN) and choose the appropriate download link for your operating system.\n\n\n\n\n\n\nR Version Requirements\n\n\n\nThis blog is designed to work with R 4.1.0 and later. The 4.1.0 release included several notable updates that are used in this blog, including the introduction of the native pipe operator |&gt;.\nIf you’ve previously downloaded R, you’ll need to update your R version to run some of the code presented in this blog.\n\n\nThere are countless free resources available for learning R, from foundational knowledge to niche topics. Here are a few of our favorite resources to help you get started:\n\nR for Data Science for beginners\nAdvanced R for a deeper dive\nRSpatial and Geocomputation with R for analysis with spatial data\nggplot2 for data visualization\nMastering Shiny for interactive applications\nR Markdown: The Definitive Guide for producing annotated code, word documents, presentations, web pages, and more\nR-bloggers for regular news and tutorials\n\nAdditionally, you can always get help on Stack Overflow and—for packages hosted on GitHub—their GitHub issues page. No matter what question you have, you’re unlikely to be the first person to encounter it, so it’s always worth checking to see whether your problem has been solved in the past."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html#essential-packages",
    "href": "posts/2024-02-01-getting-started-with-r/index.html#essential-packages",
    "title": "An Introduction to R",
    "section": "Essential packages",
    "text": "Essential packages\nSeveral packages will come into frequent use on this blog.\n\nipumsr\nThe {ipumsr} package contains functions that make it easier to load IPUMS data into R.\n\n\n    © IPUMS (MPL-2.0) \nFor IPUMS DHS, the most relevant of these is read_ipums_micro(), which allows you to load a fixed-width data file along with associated metadata.\nipumsr also contains functions to interact with variable metadata after loading, like ipums_var_info(), ipums_val_labels(), and so on.\nAs mentioned above, ipumsr also contains client tools for interacting with the IPUMS API. This allows users to request and download IPUMS data entirely within their R environment. While not available for IPUMS DHS, users of supported IPUMS collections can learn more from the API workflows introduced on the ipumsr website\n\n\ntidyverse\nThe {tidyverse} package actually refers to a family of related packages. Installing tidyverse will actually install each of these component packages:\n\n\n    © RStudio, Inc. (MIT) \n\n{ggplot2} for data visualization\n{dplyr} for data manipulation\n{tidyr} for data tidying\n{readr} for data import\n{purrr} for functional programming\n{tibble} for tibbles (a modern re-imagining of data frames)\n{stringr} for string manipulation\n{forcats} for factor handling\n\nIt’s possible to call library(tidyverse) to load all the packages in the tidyverse collection, but in most cases it’s best to individually load the specific packages you need for a given R script. This allows you and other users to more easily identify the specific packages required to run your code. It also makes your code more accessible—other users won’t have to have the entire tidyverse collection installed to run your code, only the specific packages that are actually required.\nIn general, this blog will follow the tidyverse style guide where possible. So-called “tidy” conventions are designed with the express purpose of making code and console output more human readable.\n\n\n\n\n\n\nTip\n\n\n\nSometimes, human readability imposes a performance cost: in our experience, IPUMS DHS datasets are small enough that this is not an issue. For larger datasets, we recommend exploring the {data.table} package instead.\n\n\n\n\nsf\n\n\n    © Edzer Pebesma (GPL-2 | MIT) \n{sf}, which stands for simple features, is the main R package for working with spatial vector data.\nsf represents spatial data in a “tidy” format that resembles those used by tidyverse packages mentioned above. sf objects contain tabular data along with a record of the geometry associated with each individual record.\nThis format makes it easy to perform spatial operations, attach spatial information to non-spatial data sources (like DHS surveys), and generate maps.\n\n\nterra\n\n\n    © Robert J. Hijmans et al. (GPL &gt;=3) \n{terra} provides a general framework for working with spatial data in both raster and vector format.\nWhile sf provides an alternative approach to working with vector data, terra’s raster handling stands alone and provides robust methods to quickly index, aggregate, and manipulate raster data.\nBecause of its speed and simplicity, terra has superseded the long-lived raster package, which is being retired. You may still see online resources that reference the raster package, but we suggest relying only on terra."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html#getting-help",
    "href": "posts/2024-02-01-getting-started-with-r/index.html#getting-help",
    "title": "An Introduction to R",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest Posts",
    "section": "",
    "text": "Harnessing CHC-CMIP6 Climate Scenario Data to Explore the Future\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Dietary Diversity using DHS data\n\n\n\n\n\n\nNutrition\n\n\nIntegrated geography\n\n\nComparable indicators\n\n\nTrends\n\n\nterra\n\n\nsf\n\n\n\nAn introduction to constructing comparable dietary diversity indicators. This post breaks away from the series on future conditions, with the first post of two on dietary diversity.\n\n\n\n\n\nAug 6, 2025\n\n\nDevon Kristiansen, Rebecca Luttinen\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating the now and predicting the future: Fertility rate estimation and population projection\n\n\n\n\n\n\nProjections\n\n\nPopulation\n\n\nFertility\n\n\nMethodology\n\n\nR\n\n\n\nUse the DHS to estimate fertility levels and project future population levels \n\n\n\n\n\nJun 3, 2025\n\n\nRebecca Luttinen, Elizabeth Heger Boyle\n\n\n\n\n\n\n\n\n\n\n\n\nAnticipating the Future: Important terminology\n\n\n\n\n\n\nProjections\n\n\nScenarios\n\n\nForecasts\n\n\nPredictions\n\n\nPopulation\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nPart 1 in our series exploring climate and population projections in health research \n\n\n\n\n\nApr 23, 2025\n\n\nMolly Brown, Rebecca Luttinen\n\n\n\n\n\n\n\n\n\n\n\n\nFrom MODIS to VIIRS: The Latest Source for NDVI Data\n\n\n\n\n\n\nNDVI\n\n\nVIIRS\n\n\nMODIS\n\n\nNASA\n\n\nAgriculture\n\n\nFood production\n\n\nImporting Data\n\n\nR\n\n\nReproducible workflows\n\n\nterra\n\n\nrhdf5\n\n\nstringr\n\n\npurrr\n\n\nggspatial\n\n\n\nUpdate your NDVI workflows with NASA’s newest instruments\n\n\n\n\n\nMar 31, 2025\n\n\nFinn Roberts, Rebecca Luttinen\n\n\n\n\n\n\n\n\n\n\n\n\nIncorporating Qualitative Methods into Spatial Health Research\n\n\n\n\n\n\nQualitative methods\n\n\nExtreme weather\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nSpatiotemporal research is enhanced when guided by local knowledge, perspective, and expertise. \n\n\n\n\n\nDec 6, 2024\n\n\nKathryn Grace\n\n\n\n\n\n\n\n\n\n\n\n\nUsing QGIS for Spatial Data Analysis\n\n\n\n\n\n\nGIS\n\n\nQGIS\n\n\nOpen-source software\n\n\nCHIRTS\n\n\nGeoprocessing\n\n\n\nStandalone GIS software can provide an interactive alternative to code-based analysis workflows \n\n\n\n\n\nOct 25, 2024\n\n\nAnkit Sikarwar\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Leaflet for Exploratory Data Analysis: Part 2\n\n\n\n\n\n\nCartography\n\n\nPopulation density\n\n\nUrbanization\n\n\nInteractivity\n\n\nLongitudinal data\n\n\nR\n\n\nleaflet\n\n\nterra\n\n\nsf\n\n\n\nExploring customization options for leaflet maps\n\n\n\n\n\nSep 30, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Leaflet for Exploratory Data Analysis: Part 1\n\n\n\n\n\n\nCartography\n\n\nPopulation density\n\n\nUrbanization\n\n\nLongitudinal data\n\n\nR\n\n\nleaflet\n\n\nterra\n\n\nsf\n\n\n\nAn introduction to interactive mapping using the leaflet package\n\n\n\n\n\nSep 13, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring Population Density\n\n\n\n\n\n\nPopulation\n\n\nPopulation density\n\n\nUrbanization\n\n\nData sources\n\n\nGRUMP\n\n\nGHSL\n\n\nLandScan\n\n\nWorldPop\n\n\nResearch concepts\n\n\n\nAn introduction to gridded data sources used to measure human presence across the globe \n\n\n\n\n\nAug 30, 2024\n\n\nGrace Cooper\n\n\n\n\n\n\n\n\n\n\n\n\nAggregation Methods for NDVI Data\n\n\n\n\n\n\nNDVI\n\n\nSeasonality\n\n\nClimatological normals\n\n\nAgriculture\n\n\nExtreme weather\n\n\nFood production\n\n\nR\n\n\nTime series\n\n\nterra\n\n\nggspatial\n\n\nsf\n\n\n\nWe use data from two regions to illustrate approaches to processing NDVI data \n\n\n\n\n\nAug 16, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nAn Iterative Workflow for Loading NDVI Data in R\n\n\n\n\n\n\nNDVI\n\n\nAgriculture\n\n\nExtreme weather\n\n\nFood production\n\n\nImporting Data\n\n\nR\n\n\nReproducible workflows\n\n\nTime series\n\n\nstringr\n\n\npurrr\n\n\nterra\n\n\nggspatial\n\n\nsf\n\n\n\nUse purrr and terra to efficiently integrate NDVI data across multiple spatial extents\n\n\n\n\n\nAug 1, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nIntroducing NDVI as a Tool for Population Health Research\n\n\n\n\n\n\nExtreme weather\n\n\nNDVI\n\n\nFood production\n\n\nAgriculture\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nSatellite imagery can be used in conjunction with place-specific research methods to assess food production and its impacts \n\n\n\n\n\nJul 2, 2024\n\n\nJude Mikal\n\n\n\n\n\n\n\n\n\n\n\n\nIncorporating Environmental and Reproductive History Data\n\n\n\n\n\n\nExtreme weather\n\n\nReproductive history\n\n\nLongitudinal data\n\n\nTime series\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nUnderstanding the challenges when using longitudinal data with DHS surveys \n\n\n\n\n\nJun 11, 2024\n\n\nSunnee Billingsley\n\n\n\n\n\n\n\n\n\n\n\n\nTime-specific temperature exposures for DHS survey respondents\n\n\n\n\n\n\nCHIRTS\n\n\nTemperature\n\n\nHeat\n\n\nLongitudinal analysis\n\n\nTime series\n\n\nMultilevel modeling\n\n\nExtreme weather\n\n\nterra\n\n\nlme4\n\n\ntidyr\n\n\nR\n\n\n\nFine-tune an analysis by incorporating temporal information from a DHS survey with longitudinal environmental data \n\n\n\n\n\nMay 31, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nFlexible Workflows with CHIRTS Temperature Data\n\n\n\n\n\n\nCHIRTS\n\n\nTemperature\n\n\nHeat\n\n\nExtreme weather\n\n\nTime series\n\n\nReproducible workflows\n\n\nFunctional programming\n\n\nterra\n\n\nsf\n\n\nggspatial\n\n\nR\n\n\n\nGeneralize your code to more easily aggregate time-varying environmental data \n\n\n\n\n\nApr 15, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Spatially Harmonized Geography in IPUMS DHS\n\n\n\n\n\n\nIntegrated geography\n\n\nSpatial harmonization\n\n\nLongitudinal analysis\n\n\nMethodology\n\n\nResearch concepts\n\n\nCartography\n\n\n\nA demonstration of data and techniques to handle shifting geographic boundaries in longitudinal research \n\n\n\n\n\nApr 9, 2024\n\n\nSula Sarkar, Devon Kristiansen, Miriam King\n\n\n\n\n\n\n\n\n\n\n\n\nDroplets of Insights for Integrating DHS and Rainfall Data\n\n\n\n\n\n\nExtreme weather\n\n\nPrecipitation\n\n\nCHIRPS\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nSome factors to consider when obtaining and preparing precipitation data for use with DHS surveys \n\n\n\n\n\nMar 7, 2024\n\n\nAudrey Dorélien, Molly Brown\n\n\n\n\n\n\n\n\n\n\n\n\nAttaching CHIRPS Precipitation Data to DHS Surveys\n\n\n\n\n\n\nCHIRPS\n\n\nPrecipitation\n\n\nExtreme weather\n\n\nAgriculture\n\n\nterra\n\n\nsf\n\n\nggspatial\n\n\nR\n\n\n\nRemotely sensed precipitation data can provide important context for understanding the health outcomes reported in the DHS \n\n\n\n\n\nFeb 4, 2024\n\n\nFinn Roberts, Matt Gunther\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping Robust Conceptual Models in Research on Extreme Weather and Health\n\n\n\n\n\n\nExtreme weather\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nIntroducing the theoretical foundations for effective research on weather extremes and health \n\n\n\n\n\nFeb 3, 2024\n\n\nElizabeth Heger Boyle, Kathryn Grace, Audrey Dorélien, Devon Kristiansen\n\n\n\n\n\n\n\n\n\n\n\n\nObtaining Data from IPUMS DHS\n\n\n\n\n\n\nTips\n\n\nImporting data\n\n\nR\n\n\n\nIdentify and download data from the IPUMS DHS website and load it into R \n\n\n\n\n\nFeb 2, 2024\n\n\nFinn Roberts, Matt Gunther\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to R\n\n\n\n\n\n\nTips\n\n\nR\n\n\n\nGetting started with R and RStudio\n\n\n\n\n\nFeb 1, 2024\n\n\nFinn Roberts, Matt Gunther\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog-post-workflow.html",
    "href": "blog-post-workflow.html",
    "title": "Blog Post Workflow",
    "section": "",
    "text": "To work on the blog, you’ll need the following:\n\nR\nRStudio\nGit\nA few particular R packages\n\nIf you’re working on your own machine, follow the instructions below to get this software installed.\nThe MPC also hosts a server instance of R and RStudio, which you can access using your UMN login credentials. If you prefer to work on the server, then R, RStudio, and Git will already be installed. However, you will still need to install the indicated R packages and configure your Git setup (if you haven’t done so before), so we still suggest reading through these instructions.\n\n\n\n\nTo install R, visit the Comprehensive R Archive Network (CRAN) and choose the appropriate download link for your operating system.\n\n\n\nRStudio is an interactive development environment (IDE) for R that provides an interface and code editing tools that make it much, much easier to write and edit R code.\nThe DHS Research Hub is organized as an RStudio Project, which requires RStudio. You can download RStudio here.\n\n\n\nWhen you’ve got RStudio set up, install these R packages by running the following in the RStudio console:\n\n# For writing blog posts\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\n# For help setting up Git\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf you have trouble installing any of these packages, try to install them in a fresh RStudio session (in the RStudio toolbar, select Session ‣ Restart R).\n\n\n\n\n\n\n\n\nThe DHS Research Hub blog materials exist in both a public and private format. When you’re working on the blog, you’ll be working on the private site, which is hosted on the UMN Enterprise GitHub server at https://github.umn.edu/mpc/dhs-research-hub.\nThe UMN GitHub server is accessible only to people affiliated with UMN. By working in this environment, we can develop and edit new posts without having to worry about them becoming visible on the public site prematurely. Also, it allows us to store files necessary for certain posts (e.g., IPUMS data files) without making those files publicly visible. Members of the blog “admin” team will be responsible for migrating completed posts to the public version of the site.\nSince you’re affiliated with UMN, you automatically have access to an account on UMN GitHub. To initialize an account, visit https://github.umn.edu and log in with your University Internet ID and password.\n\n\n\nTo interact with GitHub, you’ll need to install Git on your local machine. Git is the version control software that allows us to track the blog’s history and manage line-by-line changes to files as we edit new posts.\n\n\nMacOS comes with Git already installed. To confirm, you can check its location by running the following in the Mac Terminal or the Terminal in the R Studio window (typically in the tab next to the Console tab).\n\n\nTerminal\n\nwhich git\n\nYou can also check the version you have installed by running\n\n\nTerminal\n\ngit --version\n\nIf for some reason Git is not installed, use the Install Git using Homebrew instructions to install it.\n\n\n\nUsers of other operating systems should download the appropriate git for their operating system.\n\n\n\n\nNext, we’ll link RStudio and Git. This adds a new tab to your RStudio interface where you can see your files being tracked by Git in a convenient point-and-click format.\nIn the RStudio toolbar, select Tools ‣ Global Options and locate the Git/SVN tab. Ensure that the box shown below is checked, and then enter the location of the executable file. To find the git executable\n\non MacOS: run which git in Terminal\non Windows: look for git.exe in your file explorer (most likely in Program Files)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we’ll provide Git with the username and email associated with our UMN GitHub account. We’ll also need to generate a Personal Access Token (PAT) for our account. The PAT functions like a password that allows you to interact with your UMN GitHub account from R and the command line.\nFirst, set the username and email address associated with your UMN GitHub account. Git commands need to be run in the RStudio Terminal.\n\n\n\n\n\n\nImportant\n\n\n\nNote that the RStudio Terminal is not the same thing as the RStudio Console, which executes R code alone!\nYou can find the RStudio Terminal by clicking on the Terminal tab next to the Console tab.\n\n\nIn the RStudio Terminal, I’d run:\n\n\nTerminal\n\ngit config --global user.name \"Finn Roberts\"\ngit config --global user.email robe2037@umn.edu\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t necessarily need to store your credentials to use Git, but if you don’t, you’ll have to enter them in a popup window each time you interact with GitHub from R or the command line.\n\n\n\n\nNext, create a Personal Access Token (PAT) for your account. This functions like a password.\nRun the following in your RStudio console to launch a webpage where you can configure a new PAT:\n\nusethis::create_github_token(host = \"https://github.umn.edu\")\n\nYou can leave the default boxes checked and click the green Generate Token button. This should display a long string of digits—this is your new PAT. Don’t close this page yet! Return to your RStudio console and run:\n\ngitcreds::gitcreds_set(\"https://github.umn.edu\")\n\nThis will prompt you to enter a new token. Follow the instructions to copy and paste the PAT you just generated in your browser and press Enter. From now on, RStudio and Git will be able to access your UMN GitHub account automatically.\n\n\n\n\n\n\nNote\n\n\n\nIf you have a personal GitHub account at https://github.com you could repeat this process substituting \"https://github.com\" for \"https://github.umn.edu\", and Git will automatically choose the right credentials based on the repository associated with your project.\n\n\n\n\n\n\nNow that we have Git configured, we can download (or clone) a copy of the blog materials from UMN GitHub.\nOpen RStudio and navigate to File ‣ New Project, then select Version Control:\n\n\n\n\n\n\n\n\n\nChoose Git to clone the project from a GitHub repository:\n\n\n\n\n\n\n\n\n\nOn the next menu page, enter the address for the UMN GitHub repository: https://github.umn.edu/mpc/dhs-research-hub/\nHit the tab key and the project directory name should populate automatically. If not, enter dhs-research-hub as the directory name:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to clone the private version of the repository (the one located at github.umn.edu), not the public version (located at github.com).\n\n\nIn the third field, choose the location where you would like to store the blog materials on your computer. This can be anywhere that makes sense with your personal file organization approach.\n\n\nWhen choosing a place to save this project, do not save to a network drive. This seems to cause RStudio to crash!\nFinally, click Create Project. After a short bit, RStudio will relaunch and open the new project. If you adjust the windows to show the Files (left) and Git (right) tabs, you should see something like this:\n\n\n\nYou have now downloaded a copy of the DHS Research Hub blog to your computer!\nMoreover, because you’ve connected these files to a GitHub repository, the RStudio Project will now keep track of changes you make to the files in this folder, and it will prompt you to upload your changes back to GitHub: as you add, edit, or delete files, a list of changes will appear in the Git tab."
  },
  {
    "objectID": "blog-post-workflow.html#set-up-r-and-rstudio",
    "href": "blog-post-workflow.html#set-up-r-and-rstudio",
    "title": "Blog Post Workflow",
    "section": "",
    "text": "To install R, visit the Comprehensive R Archive Network (CRAN) and choose the appropriate download link for your operating system.\n\n\n\nRStudio is an interactive development environment (IDE) for R that provides an interface and code editing tools that make it much, much easier to write and edit R code.\nThe DHS Research Hub is organized as an RStudio Project, which requires RStudio. You can download RStudio here.\n\n\n\nWhen you’ve got RStudio set up, install these R packages by running the following in the RStudio console:\n\n# For writing blog posts\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\n# For help setting up Git\ninstall.packages(\"usethis\")\ninstall.packages(\"gitcreds\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf you have trouble installing any of these packages, try to install them in a fresh RStudio session (in the RStudio toolbar, select Session ‣ Restart R)."
  },
  {
    "objectID": "blog-post-workflow.html#configure-git-and-access-blog-materials",
    "href": "blog-post-workflow.html#configure-git-and-access-blog-materials",
    "title": "Blog Post Workflow",
    "section": "",
    "text": "The DHS Research Hub blog materials exist in both a public and private format. When you’re working on the blog, you’ll be working on the private site, which is hosted on the UMN Enterprise GitHub server at https://github.umn.edu/mpc/dhs-research-hub.\nThe UMN GitHub server is accessible only to people affiliated with UMN. By working in this environment, we can develop and edit new posts without having to worry about them becoming visible on the public site prematurely. Also, it allows us to store files necessary for certain posts (e.g., IPUMS data files) without making those files publicly visible. Members of the blog “admin” team will be responsible for migrating completed posts to the public version of the site.\nSince you’re affiliated with UMN, you automatically have access to an account on UMN GitHub. To initialize an account, visit https://github.umn.edu and log in with your University Internet ID and password.\n\n\n\nTo interact with GitHub, you’ll need to install Git on your local machine. Git is the version control software that allows us to track the blog’s history and manage line-by-line changes to files as we edit new posts.\n\n\nMacOS comes with Git already installed. To confirm, you can check its location by running the following in the Mac Terminal or the Terminal in the R Studio window (typically in the tab next to the Console tab).\n\n\nTerminal\n\nwhich git\n\nYou can also check the version you have installed by running\n\n\nTerminal\n\ngit --version\n\nIf for some reason Git is not installed, use the Install Git using Homebrew instructions to install it.\n\n\n\nUsers of other operating systems should download the appropriate git for their operating system.\n\n\n\n\nNext, we’ll link RStudio and Git. This adds a new tab to your RStudio interface where you can see your files being tracked by Git in a convenient point-and-click format.\nIn the RStudio toolbar, select Tools ‣ Global Options and locate the Git/SVN tab. Ensure that the box shown below is checked, and then enter the location of the executable file. To find the git executable\n\non MacOS: run which git in Terminal\non Windows: look for git.exe in your file explorer (most likely in Program Files)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we’ll provide Git with the username and email associated with our UMN GitHub account. We’ll also need to generate a Personal Access Token (PAT) for our account. The PAT functions like a password that allows you to interact with your UMN GitHub account from R and the command line.\nFirst, set the username and email address associated with your UMN GitHub account. Git commands need to be run in the RStudio Terminal.\n\n\n\n\n\n\nImportant\n\n\n\nNote that the RStudio Terminal is not the same thing as the RStudio Console, which executes R code alone!\nYou can find the RStudio Terminal by clicking on the Terminal tab next to the Console tab.\n\n\nIn the RStudio Terminal, I’d run:\n\n\nTerminal\n\ngit config --global user.name \"Finn Roberts\"\ngit config --global user.email robe2037@umn.edu\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t necessarily need to store your credentials to use Git, but if you don’t, you’ll have to enter them in a popup window each time you interact with GitHub from R or the command line.\n\n\n\n\nNext, create a Personal Access Token (PAT) for your account. This functions like a password.\nRun the following in your RStudio console to launch a webpage where you can configure a new PAT:\n\nusethis::create_github_token(host = \"https://github.umn.edu\")\n\nYou can leave the default boxes checked and click the green Generate Token button. This should display a long string of digits—this is your new PAT. Don’t close this page yet! Return to your RStudio console and run:\n\ngitcreds::gitcreds_set(\"https://github.umn.edu\")\n\nThis will prompt you to enter a new token. Follow the instructions to copy and paste the PAT you just generated in your browser and press Enter. From now on, RStudio and Git will be able to access your UMN GitHub account automatically.\n\n\n\n\n\n\nNote\n\n\n\nIf you have a personal GitHub account at https://github.com you could repeat this process substituting \"https://github.com\" for \"https://github.umn.edu\", and Git will automatically choose the right credentials based on the repository associated with your project.\n\n\n\n\n\n\nNow that we have Git configured, we can download (or clone) a copy of the blog materials from UMN GitHub.\nOpen RStudio and navigate to File ‣ New Project, then select Version Control:\n\n\n\n\n\n\n\n\n\nChoose Git to clone the project from a GitHub repository:\n\n\n\n\n\n\n\n\n\nOn the next menu page, enter the address for the UMN GitHub repository: https://github.umn.edu/mpc/dhs-research-hub/\nHit the tab key and the project directory name should populate automatically. If not, enter dhs-research-hub as the directory name:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to clone the private version of the repository (the one located at github.umn.edu), not the public version (located at github.com).\n\n\nIn the third field, choose the location where you would like to store the blog materials on your computer. This can be anywhere that makes sense with your personal file organization approach.\n\n\nWhen choosing a place to save this project, do not save to a network drive. This seems to cause RStudio to crash!\nFinally, click Create Project. After a short bit, RStudio will relaunch and open the new project. If you adjust the windows to show the Files (left) and Git (right) tabs, you should see something like this:\n\n\n\nYou have now downloaded a copy of the DHS Research Hub blog to your computer!\nMoreover, because you’ve connected these files to a GitHub repository, the RStudio Project will now keep track of changes you make to the files in this folder, and it will prompt you to upload your changes back to GitHub: as you add, edit, or delete files, a list of changes will appear in the Git tab."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this Blog",
    "section": "",
    "text": "IPUMS DHS data provide a critical resource to help illuminate the relationship between weather extremes and population health. To support such research, the IPUMS Global Health team received a 2023 supplemental grant from the Eunice Kennedy Shriver National Institute of Child Health and Human Development, or NICHD (3R01HD069471-12S1).\nThis blog is designed to be a resource for researchers who are familiar with population health survey data but new to research using spatial data sources. The blog will introduce spatial data concepts and demonstrate basic spatial data processing techniques in R.\nAdditionally, the supplemental grant supports:\n\nthe integration and dissemination of DHS malaria monitoring data (MIS surveys) in IPUMS DHS.\nthe dissemination of displaced GPS coordinates for DHS primary sampling units and the addition of new contextual variables to the IPUMS data extract system.\n\nThese three elements will make it easier to add environmental context to research using IPUMS DHS surveys and will provide additional data relevant to disease vector transmission. Combined, they will enhance researchers’ ability to use IPUMS DHS data to understand the impacts of weather extremes on health.\n\n\nThis blog is developed in collaboration with the Climate Hazards Center at the University of California, Santa Barbara and is supported by USAID Cooperative Agreement 72DFFP19CA00001.\nThe IPUMS research team also receives support as members of the Minnesota Population Center through a grant from the NICHD Population Research Infrastructure Program (P2C HD041023)."
  },
  {
    "objectID": "about.html#supporting-research-on-extreme-weather-and-health",
    "href": "about.html#supporting-research-on-extreme-weather-and-health",
    "title": "About this Blog",
    "section": "",
    "text": "IPUMS DHS data provide a critical resource to help illuminate the relationship between weather extremes and population health. To support such research, the IPUMS Global Health team received a 2023 supplemental grant from the Eunice Kennedy Shriver National Institute of Child Health and Human Development, or NICHD (3R01HD069471-12S1).\nThis blog is designed to be a resource for researchers who are familiar with population health survey data but new to research using spatial data sources. The blog will introduce spatial data concepts and demonstrate basic spatial data processing techniques in R.\nAdditionally, the supplemental grant supports:\n\nthe integration and dissemination of DHS malaria monitoring data (MIS surveys) in IPUMS DHS.\nthe dissemination of displaced GPS coordinates for DHS primary sampling units and the addition of new contextual variables to the IPUMS data extract system.\n\nThese three elements will make it easier to add environmental context to research using IPUMS DHS surveys and will provide additional data relevant to disease vector transmission. Combined, they will enhance researchers’ ability to use IPUMS DHS data to understand the impacts of weather extremes on health.\n\n\nThis blog is developed in collaboration with the Climate Hazards Center at the University of California, Santa Barbara and is supported by USAID Cooperative Agreement 72DFFP19CA00001.\nThe IPUMS research team also receives support as members of the Minnesota Population Center through a grant from the NICHD Population Research Infrastructure Program (P2C HD041023)."
  },
  {
    "objectID": "about.html#what-is-ipums-dhs",
    "href": "about.html#what-is-ipums-dhs",
    "title": "About this Blog",
    "section": "What is IPUMS DHS?",
    "text": "What is IPUMS DHS?\nIPUMS provides census and survey data from around the world integrated across time and space. IPUMS integration and documentation makes it easy to study change, conduct comparative research, merge information across data types, and analyze individuals within family and community context.\nIPUMS is comprised of several individual products, each with different data sources and content areas. IPUMS DHS is one of several IPUMS Global Health products—representative household surveys, primarily from low- and middle-income countries, that gather extensive information on health, family planning, living conditions, and more.\nIPUMS DHS specifically facilitates analysis of data provided by The Demographic and Health Surveys Program (DHS), which has been administered in low- and middle-income countries since the 1980s.\nIPUMS DHS harmonizes DHS variables over time and provides comprehensive cross-survey documentation to make it easier to find and understand DHS data.\nIPUMS DHS data are available free of charge, but users must register with The DHS Program to gain access. Registered DHS users can enter their DHS username and password to access data from IPUMS DHS. For more information about downloading IPUMS DHS data, see the associated post."
  },
  {
    "objectID": "about.html#how-to-cite",
    "href": "about.html#how-to-cite",
    "title": "About this Blog",
    "section": "How to cite",
    "text": "How to cite\nFor a comprehensive guide to citing IPUMS DHS, see the citation page on the IPUMS DHS website.\nTo cite an individual post on this blog, we suggest the following format:\n\nRoberts, F., & Gunther, M. (2024, Februrary 4). Attaching CHIRPS precipitation data to DHS surveys. IPUMS DHS Spatial Analysis and Health Research Hub. https://tech.popdata.org/dhs-research-hub/posts/2024-02-04-dhs-chirps/index.html"
  },
  {
    "objectID": "about.html#getting-help",
    "href": "about.html#getting-help",
    "title": "About this Blog",
    "section": "Getting help",
    "text": "Getting help\nIPUMS users can find help on the IPUMS User Forum or by contacting IPUMS user support at ipums@umn.edu."
  },
  {
    "objectID": "about.html#related-work",
    "href": "about.html#related-work",
    "title": "About this Blog",
    "section": "Related work",
    "text": "Related work\nIPUMS Global Health also houses the IPUMS PMA Data Analysis Hub, a similar blog focused on working with IPUMS PMA data in R."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a welcoming and respectful experience for everyone."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement Guidelines",
    "text": "Enforcement Guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "License",
    "section": "",
    "text": "1.1. “Contributor” means each individual or legal entity that creates, contributes to the creation of, or owns Covered Software.\n1.2. “Contributor Version” means the combination of the Contributions of others (if any) used by a Contributor and that particular Contributor’s Contribution.\n1.3. “Contribution” means Covered Software of a particular Contributor.\n1.4. “Covered Software” means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof.\n1.5. “Incompatible With Secondary Licenses” means\n(a) that the initial Contributor has attached the notice described\n    in Exhibit B to the Covered Software; or\n\n(b) that the Covered Software was made available under the terms of\n    version 1.1 or earlier of the License, but not also under the\n    terms of a Secondary License.\n1.6. “Executable Form” means any form of the work other than Source Code Form.\n1.7. “Larger Work” means a work that combines Covered Software with other material, in a separate file or files, that is not Covered Software.\n1.8. “License” means this document.\n1.9. “Licensable” means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently, any and all of the rights conveyed by this License.\n1.10. “Modifications” means any of the following:\n(a) any file in Source Code Form that results from an addition to,\n    deletion from, or modification of the contents of Covered\n    Software; or\n\n(b) any new file in Source Code Form that contains any Covered\n    Software.\n1.11. “Patent Claims” of a Contributor means any patent claim(s), including without limitation, method, process, and apparatus claims, in any patent Licensable by such Contributor that would be infringed, but for the grant of the License, by the making, using, selling, offering for sale, having made, import, or transfer of either its Contributions or its Contributor Version.\n1.12. “Secondary License” means either the GNU General Public License, Version 2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General Public License, Version 3.0, or any later versions of those licenses.\n1.13. “Source Code Form” means the form of the work preferred for making modifications.\n1.14. “You” (or “Your”) means an individual or a legal entity exercising rights under this License. For legal entities, “You” includes any entity that controls, is controlled by, or is under common control with You. For purposes of this definition, “control” means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity.\n\n\n\n2.1. Grants\nEach Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:\n\nunder intellectual property rights (other than patent or trademark) Licensable by such Contributor to use, reproduce, make available, modify, display, perform, distribute, and otherwise exploit its Contributions, either on an unmodified basis, with Modifications, or as part of a Larger Work; and\nunder Patent Claims of such Contributor to make, use, sell, offer for sale, have made, import, and otherwise transfer either its Contributions or its Contributor Version.\n\n2.2. Effective Date\nThe licenses granted in Section 2.1 with respect to any Contribution become effective for each Contribution on the date the Contributor first distributes such Contribution.\n2.3. Limitations on Grant Scope\nThe licenses granted in this Section 2 are the only rights granted under this License. No additional rights or licenses will be implied from the distribution or licensing of Covered Software under this License. Notwithstanding Section 2.1(b) above, no patent license is granted by a Contributor:\n\nfor any code that a Contributor has removed from Covered Software; or\nfor infringements caused by: (i) Your and any other third party’s modifications of Covered Software, or (ii) the combination of its Contributions with other software (except as part of its Contributor Version); or\nunder Patent Claims infringed by Covered Software in the absence of its Contributions.\n\nThis License does not grant any rights in the trademarks, service marks, or logos of any Contributor (except as may be necessary to comply with the notice requirements in Section 3.4).\n2.4. Subsequent Licenses\nNo Contributor makes additional grants as a result of Your choice to distribute the Covered Software under a subsequent version of this License (see Section 10.2) or under the terms of a Secondary License (if permitted under the terms of Section 3.3).\n2.5. Representation\nEach Contributor represents that the Contributor believes its Contributions are its original creation(s) or it has sufficient rights to grant the rights to its Contributions conveyed by this License.\n2.6. Fair Use\nThis License is not intended to limit any rights You have under applicable copyright doctrines of fair use, fair dealing, or other equivalents.\n2.7. Conditions\nSections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in Section 2.1.\n\n\n\n3.1. Distribution of Source Form\nAll distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients’ rights in the Source Code Form.\n3.2. Distribution of Executable Form\nIf You distribute Covered Software in Executable Form then:\n\nsuch Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and\nYou may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients’ rights in the Source Code Form under this License.\n\n3.3. Distribution of a Larger Work\nYou may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. If the Larger Work is a combination of Covered Software with a work governed by one or more Secondary Licenses, and the Covered Software is not Incompatible With Secondary Licenses, this License permits You to additionally distribute such Covered Software under the terms of such Secondary License(s), so that the recipient of the Larger Work may, at their option, further distribute the Covered Software under the terms of either this License or such Secondary License(s).\n3.4. Notices\nYou may not remove or alter the substance of any license notices (including copyright notices, patent notices, disclaimers of warranty, or limitations of liability) contained within the Source Code Form of the Covered Software, except that You may alter any license notices to the extent required to remedy known factual inaccuracies.\n3.5. Application of Additional Terms\nYou may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, You may do so only on Your own behalf, and not on behalf of any Contributor. You must make it absolutely clear that any such warranty, support, indemnity, or liability obligation is offered by You alone, and You hereby agree to indemnify every Contributor for any liability incurred by such Contributor as a result of warranty, support, indemnity or liability terms You offer. You may include additional disclaimers of warranty and limitations of liability specific to any jurisdiction.\n\n\n\nIf it is impossible for You to comply with any of the terms of this License with respect to some or all of the Covered Software due to statute, judicial order, or regulation then You must: (a) comply with the terms of this License to the maximum extent possible; and (b) describe the limitations and the code they affect. Such description must be placed in a text file included with all distributions of the Covered Software under this License. Except to the extent prohibited by statute or regulation, such description must be sufficiently detailed for a recipient of ordinary skill to be able to understand it.\n\n\n\n5.1. The rights granted under this License will terminate automatically if You fail to comply with any of its terms. However, if You become compliant, then the rights granted under this License from a particular Contributor are reinstated (a) provisionally, unless and until such Contributor explicitly and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor fails to notify You of the non-compliance by some reasonable means prior to 60 days after You have come back into compliance. Moreover, Your grants from a particular Contributor are reinstated on an ongoing basis if such Contributor notifies You of the non-compliance by some reasonable means, this is the first time You have received notice of non-compliance with this License from such Contributor, and You become compliant prior to 30 days after Your receipt of the notice.\n5.2. If You initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a Contributor Version directly or indirectly infringes any patent, then the rights granted to You by any and all Contributors for the Covered Software under Section 2.1 of this License shall terminate.\n5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user license agreements (excluding distributors and resellers) which have been validly granted by You or Your distributors under this License prior to termination shall survive termination.\n\n\n\nCovered Software is provided under this License on an “as is”\nbasis, without warranty of any kind, either expressed, implied, or\nstatutory, including, without limitation, warranties that the\nCovered Software is free of defects, merchantable, fit for a\nparticular purpose or non-infringing. The entire risk as to the\nquality and performance of the Covered Software is with You.\nShould any Covered Software prove defective in any respect, You\n(not any Contributor) assume the cost of any necessary servicing,\nrepair, or correction. This disclaimer of warranty constitutes an\nessential part of this License. No use of any Covered Software is\nauthorized under this License except under this disclaimer.\n\n\n\nUnder no circumstances and under no legal theory, whether tort\n(including negligence), contract, or otherwise, shall any\nContributor, or anyone who distributes Covered Software as\npermitted above, be liable to You for any direct, indirect,\nspecial, incidental, or consequential damages of any character\nincluding, without limitation, damages for lost profits, loss of\ngoodwill, work stoppage, computer failure or malfunction, or any\nand all other commercial damages or losses, even if such party\nshall have been informed of the possibility of such damages. This\nlimitation of liability shall not apply to liability for death or\npersonal injury resulting from such party’s negligence to the\nextent applicable law prohibits such limitation. Some\njurisdictions do not allow the exclusion or limitation of\nincidental or consequential damages, so this exclusion and\nlimitation may not apply to You.\n\n\n\nAny litigation relating to this License may be brought only in the courts of a jurisdiction where the defendant maintains its principal place of business and such litigation shall be governed by laws of that jurisdiction, without reference to its conflict-of-law provisions. Nothing in this Section shall prevent a party’s ability to bring cross-claims or counter-claims.\n\n\n\nThis License represents the complete agreement concerning the subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not be used to construe this License against a Contributor.\n\n\n\n10.1. New Versions\nMozilla Foundation is the license steward. Except as provided in Section 10.3, no one other than the license steward has the right to modify or publish new versions of this License. Each version will be given a distinguishing version number.\n10.2. Effect of New Versions\nYou may distribute the Covered Software under the terms of the version of the License under which You originally received the Covered Software, or under the terms of any subsequent version published by the license steward.\n10.3. Modified Versions\nIf you create software not governed by this License, and you want to create a new license for such software, you may create and use a modified version of this License if you rename the license and remove any references to the name of the license steward (except to note that such modified license differs from this License).\n10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses\nIf You choose to distribute Source Code Form that is Incompatible With Secondary Licenses under the terms of this version of the License, the notice described in Exhibit B of this License must be attached.\n\n\n\nThis Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\nIf it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice.\nYou may add additional accurate notices of copyright ownership.\n\n\n\nThis Source Code Form is “Incompatible With Secondary Licenses”, as defined by the Mozilla Public License, v. 2.0."
  },
  {
    "objectID": "LICENSE.html#definitions",
    "href": "LICENSE.html#definitions",
    "title": "License",
    "section": "",
    "text": "1.1. “Contributor” means each individual or legal entity that creates, contributes to the creation of, or owns Covered Software.\n1.2. “Contributor Version” means the combination of the Contributions of others (if any) used by a Contributor and that particular Contributor’s Contribution.\n1.3. “Contribution” means Covered Software of a particular Contributor.\n1.4. “Covered Software” means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof.\n1.5. “Incompatible With Secondary Licenses” means\n(a) that the initial Contributor has attached the notice described\n    in Exhibit B to the Covered Software; or\n\n(b) that the Covered Software was made available under the terms of\n    version 1.1 or earlier of the License, but not also under the\n    terms of a Secondary License.\n1.6. “Executable Form” means any form of the work other than Source Code Form.\n1.7. “Larger Work” means a work that combines Covered Software with other material, in a separate file or files, that is not Covered Software.\n1.8. “License” means this document.\n1.9. “Licensable” means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently, any and all of the rights conveyed by this License.\n1.10. “Modifications” means any of the following:\n(a) any file in Source Code Form that results from an addition to,\n    deletion from, or modification of the contents of Covered\n    Software; or\n\n(b) any new file in Source Code Form that contains any Covered\n    Software.\n1.11. “Patent Claims” of a Contributor means any patent claim(s), including without limitation, method, process, and apparatus claims, in any patent Licensable by such Contributor that would be infringed, but for the grant of the License, by the making, using, selling, offering for sale, having made, import, or transfer of either its Contributions or its Contributor Version.\n1.12. “Secondary License” means either the GNU General Public License, Version 2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General Public License, Version 3.0, or any later versions of those licenses.\n1.13. “Source Code Form” means the form of the work preferred for making modifications.\n1.14. “You” (or “Your”) means an individual or a legal entity exercising rights under this License. For legal entities, “You” includes any entity that controls, is controlled by, or is under common control with You. For purposes of this definition, “control” means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity."
  },
  {
    "objectID": "LICENSE.html#license-grants-and-conditions",
    "href": "LICENSE.html#license-grants-and-conditions",
    "title": "License",
    "section": "",
    "text": "2.1. Grants\nEach Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:\n\nunder intellectual property rights (other than patent or trademark) Licensable by such Contributor to use, reproduce, make available, modify, display, perform, distribute, and otherwise exploit its Contributions, either on an unmodified basis, with Modifications, or as part of a Larger Work; and\nunder Patent Claims of such Contributor to make, use, sell, offer for sale, have made, import, and otherwise transfer either its Contributions or its Contributor Version.\n\n2.2. Effective Date\nThe licenses granted in Section 2.1 with respect to any Contribution become effective for each Contribution on the date the Contributor first distributes such Contribution.\n2.3. Limitations on Grant Scope\nThe licenses granted in this Section 2 are the only rights granted under this License. No additional rights or licenses will be implied from the distribution or licensing of Covered Software under this License. Notwithstanding Section 2.1(b) above, no patent license is granted by a Contributor:\n\nfor any code that a Contributor has removed from Covered Software; or\nfor infringements caused by: (i) Your and any other third party’s modifications of Covered Software, or (ii) the combination of its Contributions with other software (except as part of its Contributor Version); or\nunder Patent Claims infringed by Covered Software in the absence of its Contributions.\n\nThis License does not grant any rights in the trademarks, service marks, or logos of any Contributor (except as may be necessary to comply with the notice requirements in Section 3.4).\n2.4. Subsequent Licenses\nNo Contributor makes additional grants as a result of Your choice to distribute the Covered Software under a subsequent version of this License (see Section 10.2) or under the terms of a Secondary License (if permitted under the terms of Section 3.3).\n2.5. Representation\nEach Contributor represents that the Contributor believes its Contributions are its original creation(s) or it has sufficient rights to grant the rights to its Contributions conveyed by this License.\n2.6. Fair Use\nThis License is not intended to limit any rights You have under applicable copyright doctrines of fair use, fair dealing, or other equivalents.\n2.7. Conditions\nSections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in Section 2.1."
  },
  {
    "objectID": "LICENSE.html#responsibilities",
    "href": "LICENSE.html#responsibilities",
    "title": "License",
    "section": "",
    "text": "3.1. Distribution of Source Form\nAll distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients’ rights in the Source Code Form.\n3.2. Distribution of Executable Form\nIf You distribute Covered Software in Executable Form then:\n\nsuch Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and\nYou may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients’ rights in the Source Code Form under this License.\n\n3.3. Distribution of a Larger Work\nYou may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. If the Larger Work is a combination of Covered Software with a work governed by one or more Secondary Licenses, and the Covered Software is not Incompatible With Secondary Licenses, this License permits You to additionally distribute such Covered Software under the terms of such Secondary License(s), so that the recipient of the Larger Work may, at their option, further distribute the Covered Software under the terms of either this License or such Secondary License(s).\n3.4. Notices\nYou may not remove or alter the substance of any license notices (including copyright notices, patent notices, disclaimers of warranty, or limitations of liability) contained within the Source Code Form of the Covered Software, except that You may alter any license notices to the extent required to remedy known factual inaccuracies.\n3.5. Application of Additional Terms\nYou may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, You may do so only on Your own behalf, and not on behalf of any Contributor. You must make it absolutely clear that any such warranty, support, indemnity, or liability obligation is offered by You alone, and You hereby agree to indemnify every Contributor for any liability incurred by such Contributor as a result of warranty, support, indemnity or liability terms You offer. You may include additional disclaimers of warranty and limitations of liability specific to any jurisdiction."
  },
  {
    "objectID": "LICENSE.html#inability-to-comply-due-to-statute-or-regulation",
    "href": "LICENSE.html#inability-to-comply-due-to-statute-or-regulation",
    "title": "License",
    "section": "",
    "text": "If it is impossible for You to comply with any of the terms of this License with respect to some or all of the Covered Software due to statute, judicial order, or regulation then You must: (a) comply with the terms of this License to the maximum extent possible; and (b) describe the limitations and the code they affect. Such description must be placed in a text file included with all distributions of the Covered Software under this License. Except to the extent prohibited by statute or regulation, such description must be sufficiently detailed for a recipient of ordinary skill to be able to understand it."
  },
  {
    "objectID": "LICENSE.html#termination",
    "href": "LICENSE.html#termination",
    "title": "License",
    "section": "",
    "text": "5.1. The rights granted under this License will terminate automatically if You fail to comply with any of its terms. However, if You become compliant, then the rights granted under this License from a particular Contributor are reinstated (a) provisionally, unless and until such Contributor explicitly and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor fails to notify You of the non-compliance by some reasonable means prior to 60 days after You have come back into compliance. Moreover, Your grants from a particular Contributor are reinstated on an ongoing basis if such Contributor notifies You of the non-compliance by some reasonable means, this is the first time You have received notice of non-compliance with this License from such Contributor, and You become compliant prior to 30 days after Your receipt of the notice.\n5.2. If You initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a Contributor Version directly or indirectly infringes any patent, then the rights granted to You by any and all Contributors for the Covered Software under Section 2.1 of this License shall terminate.\n5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user license agreements (excluding distributors and resellers) which have been validly granted by You or Your distributors under this License prior to termination shall survive termination."
  },
  {
    "objectID": "LICENSE.html#disclaimer-of-warranty",
    "href": "LICENSE.html#disclaimer-of-warranty",
    "title": "License",
    "section": "",
    "text": "Covered Software is provided under this License on an “as is”\nbasis, without warranty of any kind, either expressed, implied, or\nstatutory, including, without limitation, warranties that the\nCovered Software is free of defects, merchantable, fit for a\nparticular purpose or non-infringing. The entire risk as to the\nquality and performance of the Covered Software is with You.\nShould any Covered Software prove defective in any respect, You\n(not any Contributor) assume the cost of any necessary servicing,\nrepair, or correction. This disclaimer of warranty constitutes an\nessential part of this License. No use of any Covered Software is\nauthorized under this License except under this disclaimer."
  },
  {
    "objectID": "LICENSE.html#limitation-of-liability",
    "href": "LICENSE.html#limitation-of-liability",
    "title": "License",
    "section": "",
    "text": "Under no circumstances and under no legal theory, whether tort\n(including negligence), contract, or otherwise, shall any\nContributor, or anyone who distributes Covered Software as\npermitted above, be liable to You for any direct, indirect,\nspecial, incidental, or consequential damages of any character\nincluding, without limitation, damages for lost profits, loss of\ngoodwill, work stoppage, computer failure or malfunction, or any\nand all other commercial damages or losses, even if such party\nshall have been informed of the possibility of such damages. This\nlimitation of liability shall not apply to liability for death or\npersonal injury resulting from such party’s negligence to the\nextent applicable law prohibits such limitation. Some\njurisdictions do not allow the exclusion or limitation of\nincidental or consequential damages, so this exclusion and\nlimitation may not apply to You."
  },
  {
    "objectID": "LICENSE.html#litigation",
    "href": "LICENSE.html#litigation",
    "title": "License",
    "section": "",
    "text": "Any litigation relating to this License may be brought only in the courts of a jurisdiction where the defendant maintains its principal place of business and such litigation shall be governed by laws of that jurisdiction, without reference to its conflict-of-law provisions. Nothing in this Section shall prevent a party’s ability to bring cross-claims or counter-claims."
  },
  {
    "objectID": "LICENSE.html#miscellaneous",
    "href": "LICENSE.html#miscellaneous",
    "title": "License",
    "section": "",
    "text": "This License represents the complete agreement concerning the subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not be used to construe this License against a Contributor."
  },
  {
    "objectID": "LICENSE.html#versions-of-the-license",
    "href": "LICENSE.html#versions-of-the-license",
    "title": "License",
    "section": "",
    "text": "10.1. New Versions\nMozilla Foundation is the license steward. Except as provided in Section 10.3, no one other than the license steward has the right to modify or publish new versions of this License. Each version will be given a distinguishing version number.\n10.2. Effect of New Versions\nYou may distribute the Covered Software under the terms of the version of the License under which You originally received the Covered Software, or under the terms of any subsequent version published by the license steward.\n10.3. Modified Versions\nIf you create software not governed by this License, and you want to create a new license for such software, you may create and use a modified version of this License if you rename the license and remove any references to the name of the license steward (except to note that such modified license differs from this License).\n10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses\nIf You choose to distribute Source Code Form that is Incompatible With Secondary Licenses under the terms of this version of the License, the notice described in Exhibit B of this License must be attached."
  },
  {
    "objectID": "LICENSE.html#exhibit-a---source-code-form-license-notice",
    "href": "LICENSE.html#exhibit-a---source-code-form-license-notice",
    "title": "License",
    "section": "",
    "text": "This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\nIf it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice.\nYou may add additional accurate notices of copyright ownership."
  },
  {
    "objectID": "LICENSE.html#exhibit-b---incompatible-with-secondary-licenses-notice",
    "href": "LICENSE.html#exhibit-b---incompatible-with-secondary-licenses-notice",
    "title": "License",
    "section": "",
    "text": "This Source Code Form is “Incompatible With Secondary Licenses”, as defined by the Mozilla Public License, v. 2.0."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html",
    "href": "posts/2024-02-02-download-dhs-data/index.html",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "",
    "text": "The Demographic and Health Surveys Program (DHS) is the leading source of population health data for low- and middle-income countries around the world. IPUMS DHS disseminates a harmonized version of the DHS survey results in which variables are integrated across time and space, facilitating comparative and longitudinal analysis. Furthermore, IPUMS DHS provides a web interface and streamlined documentation to make the data discovery and download process easier."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#browse-data",
    "href": "posts/2024-02-02-download-dhs-data/index.html#browse-data",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Browse data",
    "text": "Browse data\nUsers can browse the available data using the IPUMS DHS data selection interface, which includes sample and variable availability, descriptions, codes, and more.\nFor more information about how to use the interface, see the IPUMS DHS user guide."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#select-data",
    "href": "posts/2024-02-02-download-dhs-data/index.html#select-data",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Select data",
    "text": "Select data\nOnce you’ve selected the samples and variables you want to include in your data extract, click View Cart to review your selections. If you’re satisfied with the contents of your extract, click Create Data Extract.\n\n\n\n\n\n\n\n\n\nIPUMS DHS allows you to select one of several output file formats. On this blog, we will use the default fixed-width (.dat) file option, which is the format expected by the data-reading functions provided in ipumsr.\n\n\n\n\n\n\n\n\n\nClick Submit Extract to submit your extract for processing on the IPUMS servers. You’ll receive an email when your extract is complete and ready to download."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#download-data",
    "href": "posts/2024-02-02-download-dhs-data/index.html#download-data",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Download data",
    "text": "Download data\nClick the green download button to download the compressed data file for your extract.\nYou will also need to download an associated metadata (DDI) file. This is an XML file that contains parsing instructions for the fixed-width data file as well as descriptive information about the variables contained in an extract.\n\n\nDDI stands for Data Documentation Initiative, an international standard for documenting data obtained in survey research.\nYou can do so by right clicking the DDI link and selecting Save link as….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe specific text included in the dropdown may differ based on the browser that you are using. For instance, Safari displays the option Download Linked File As….\nThe important thing is that you download the DDI file in .xml format, not .html format."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#getting-help",
    "href": "posts/2024-02-02-download-dhs-data/index.html#getting-help",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html",
    "href": "posts/2024-02-04-dhs-chirps/index.html",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "",
    "text": "We know that health outcomes are significantly impacted by individuals’ environmental context: excessive rainfall can flood local infrastructure1; warmer temperatures can expand the range of common disease vectors2; and drought can decimate crop-growing regions.3 However, survey data like those from The DHS Program often provide only a limited view of the environmental trends in which individuals are embedded.\nDeveloping a more holistic understanding of the role of environmental conditions in health outcomes therefore requires integrating external data sources with the survey data from DHS.\nIn this post we’ll demonstrate how to obtain raw precipitation data from the Climate Hazards Center InfraRed Precipitation with Station dataset (CHIRPS) and attach that data to survey responses from IPUMS DHS."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#ipums-dhs-survey-data",
    "href": "posts/2024-02-04-dhs-chirps/index.html#ipums-dhs-survey-data",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "IPUMS DHS survey data",
    "text": "IPUMS DHS survey data\nTo get started, we’ll download a data file (or extract, in IPUMS terms) from IPUMS DHS and load it into R. The extract in this post contains the HWHAZWHO variable (which contains the height-for-age Z-score) along with several pre-selected variables for the 2010 Burkina Faso sample. If you need a refresher on how to download IPUMS DHS data, see the Downloading IPUMS DHS Data post.\n\n\n    © IPUMS (MPL-2.0) \nWe’ve downloaded and stored our XML codebook and compressed data file in the data/dhs directory. Be sure to update this path based on your local file setup, so you can follow along.\nTo simplify our output, we’ll select only a subset of the variables included in the extract:\n\nlibrary(ipumsr)\nlibrary(dplyr)\n\n# Load IPUMS DHS extract\ndhs &lt;- read_ipums_micro(\n  ddi = \"data/dhs/idhs_00018.xml\",\n  data_file = \"data/dhs/idhs_00018.dat.gz\",\n  verbose = FALSE\n)\n\n# Select a subset of variables\ndhs &lt;- dhs |&gt; \n  select(SAMPLE, YEAR, IDHSPID, IDHSHID, DHSID, URBAN, HWHAZWHO)\n\ndhs\n#&gt; # A tibble: 15,044 × 7\n#&gt;    SAMPLE                     YEAR IDHSPID      IDHSHID DHSID URBAN   HWHAZWHO  \n#&gt;    &lt;int+lbl&gt;                 &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt; &lt;int+l&gt; &lt;int+lbl&gt; \n#&gt;  1 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -264      \n#&gt;  2 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -113      \n#&gt;  3 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur…  -13      \n#&gt;  4 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -291      \n#&gt;  5 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -211      \n#&gt;  6 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… 9999 [NIU…\n#&gt;  7 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… 9999 [NIU…\n#&gt;  8 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -185      \n#&gt;  9 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… 9999 [NIU…\n#&gt; 10 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur…  -88      \n#&gt; # ℹ 15,034 more rows\n\nThis gives us a tabular data source containing 15,044 individual DHS survey responses for 7 variables. Of particular note is the DHSID variable (which stores the identifier for the location of the survey response)."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#dhs-cluster-coordinates",
    "href": "posts/2024-02-04-dhs-chirps/index.html#dhs-cluster-coordinates",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "DHS cluster coordinates",
    "text": "DHS cluster coordinates\nTo meaningfully attach environmental data to our DHS survey responses, we’ll need to know the location where each survey response was collected. Fortunately, the DHS Program provides GPS coordinates for each surveyed household grouping, or cluster.\n\n\n\n\n\n\nDHS Cluster Displacement\n\n\n\nIt’s important to note that the GPS coordinates provided by The DHS Program do not actually reflect the exact location of the clusters. In fact, the coordinates provided are randomly displaced from their true locations, such that:\n\nurban clusters are displaced up to 2 kilometers in any direction.\n99% of rural clusters are displaced up to 5 kilometers in any direction.\n1% of rural clusters are displaced up to 10 kilometers in any direction.\ndisplaced coordinates do not cross the country boundary and stay within the DHS survey region.\n\nThe current demonstration doesn’t require a precise location of each cluster, so we will ignore this detail. However, for more fine-grained analyses (e.g. road network analyses), you may need to take displacement into account.\nSee the DHS GPS data collection documentation for more details about the DHS cluster point displacement methodology.\n\n\nIPUMS DHS does not yet disseminate DHS cluster coordinates directly. For now, to obtain the GPS coordinates for a specific sample, you’ll have to log into your account from The DHS Program. Specify your country of interest, and, on the line for the appropriate sample year, click the link to download the GPS coordinate data under the heading GPS Datasets. (Again, for this example, we’re using Burkina Faso’s 2010 sample.)\n\n\nIPUMS DHS has recently received permission and funding to distribute DHS GPS data. Stay tuned, as these data will soon be available via IPUMS!\nYou’ll be presented with a new page containing a list of download links. Scroll down to the Geographic Datasets section. You have the option of downloading the file as either a shapefile (.shp) or a comma delimited file (.csv). For our purposes, we will download the shapefile, which contains spatial information in a format that can be easily interpreted by R (as well as by external GIS software).\nFor the Burkina Faso 2010 sample, the file should be named BFGE61FL. If you see a different file name, make sure you’re working with the correct survey year.\n\nVector data\nOur DHS cluster coordinates are what’s known as vector data. Vector data refer to spatial data that represent geographic features using geometric shapes like points, lines, and polygons.\n\n\n\nBasic features of the vector data model4\n\n\nIn R, the {sf} package provides an intuitive tabular framework for working with vector data. sf objects look much like a familiar {tibble} or data.frame, but also include an additional geometry column, which stores the spatial features that correspond to each set of observations.\n\n\n\n\n\n\n\n    © Edzer Pebesma (GPL-2 | MIT) \n\nInstalling sf\nsf requires three operating system dependencies:\n\nGEOS for geometrical operations on projected coordinates\nPRØJ for coordinate reference system conversion and transformation\nGDAL for driver options\n\nMake sure to follow these instructions to ensure that you set up GEOS, PRØJ, and GDAL when installing sf. The installation instructions may vary slightly, depending on your operating system. You may also need to update R, and then run install.packages(\"sf\").\n\n\n\nLoading cluster coordinates\nOnce sf is installed, we can use st_read() to load the file of GPS coordinate data we downloaded above. We’ve stored our shapefile in the data/gps directory. Again, make sure to adjust this path based on the location where you saved the file on your own system.\n\n\nA shapefile is actually a collection of several files; the primary file will have the .shp extension. Other files contain relevant metadata about the geometries contained in the .shp file (for instance, projection or index information).\n\nlibrary(sf)\n\n# Load the 2010 BF cluster coordinate shapefile\nbf_gps &lt;- st_read(\"data/gps/BFGE61FL/BFGE61FL.shp\")\n#&gt; Reading layer `BFGE61FL' from data source \n#&gt;   `/Users/robe2037/Documents/projects/dhs-research-hub/posts/2024-02-04-dhs-chirps/data/gps/BFGE61FL/BFGE61FL.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 573 features and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -5.426079 ymin: 5.684342e-14 xmax: 1.957316 ymax: 14.86258\n#&gt; Geodetic CRS:  WGS 84\n\nOur resulting dataset looks something like a tibble, except that it contains a header describing a simple feature collection with 573 features and 20 fields:\n\nbf_gps\n#&gt; Simple feature collection with 573 features and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -5.426079 ymin: 5.684342e-14 xmax: 1.957316 ymax: 14.86258\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;             DHSID DHSCC DHSYEAR DHSCLUST CCFIPS ADM1FIPS ADM1FIPSNA ADM1SALBNA\n#&gt; 1  BF201000000001    BF    2010        1     UV     NULL       NULL       NULL\n#&gt; 2  BF201000000002    BF    2010        2     UV     NULL       NULL       NULL\n#&gt; 3  BF201000000003    BF    2010        3     UV     NULL       NULL       NULL\n#&gt; 4  BF201000000004    BF    2010        4     UV     NULL       NULL       NULL\n#&gt; 5  BF201000000005    BF    2010        5     UV     NULL       NULL       NULL\n#&gt; 6  BF201000000006    BF    2010        6     UV     NULL       NULL       NULL\n#&gt; 7  BF201000000007    BF    2010        7     UV     NULL       NULL       NULL\n#&gt; 8  BF201000000008    BF    2010        8     UV     NULL       NULL       NULL\n#&gt; 9  BF201000000009    BF    2010        9     UV     NULL       NULL       NULL\n#&gt; 10 BF201000000010    BF    2010       10     UV     NULL       NULL       NULL\n#&gt;    ADM1SALBCO ADM1DHS ADM1NAME DHSREGCO          DHSREGNA SOURCE URBAN_RURA\n#&gt; 1        NULL    9999     NULL       13         Sud-Ouest    GPS          R\n#&gt; 2        NULL    9999     NULL        2          Cascades    GPS          R\n#&gt; 3        NULL    9999     NULL       13         Sud-Ouest    GAZ          U\n#&gt; 4        NULL    9999     NULL       10              Nord    GPS          R\n#&gt; 5        NULL    9999     NULL        1 Boucle de Mouhoun    GPS          R\n#&gt; 6        NULL    9999     NULL        6      Centre-Ouest    GPS          R\n#&gt; 7        NULL    9999     NULL       12             Sahel    GPS          R\n#&gt; 8        NULL    9999     NULL        3            Centre    GPS          U\n#&gt; 9        NULL    9999     NULL        4        Centre-Est    GPS          U\n#&gt; 10       NULL    9999     NULL        5       Centre-Nord    GAZ          R\n#&gt;       LATNUM   LONGNUM ALT_GPS ALT_DEM DATUM                   geometry\n#&gt; 1  10.109415 -2.807555     269     269 WGS84 POINT (-2.807555 10.10942)\n#&gt; 2  10.388513 -3.907798     367     362 WGS84 POINT (-3.907798 10.38851)\n#&gt; 3   9.882864 -2.925703    9999     308 WGS84 POINT (-2.925703 9.882864)\n#&gt; 4  13.573418 -2.163120     323     323 WGS84  POINT (-2.16312 13.57342)\n#&gt; 5  12.453299 -3.461899     301     298 WGS84  POINT (-3.461899 12.4533)\n#&gt; 6  12.045308 -2.083828     338     325 WGS84 POINT (-2.083828 12.04531)\n#&gt; 7  14.354198 -0.672096     328     328 WGS84  POINT (-0.672096 14.3542)\n#&gt; 8  12.311034 -1.562071     322     324 WGS84 POINT (-1.562071 12.31103)\n#&gt; 9  11.780763 -0.363904     317     304 WGS84 POINT (-0.363904 11.78076)\n#&gt; 10 13.225114 -1.337994    9999     343 WGS84 POINT (-1.337994 13.22511)\n\nEach feature corresponds to one cluster location for which we have GPS coordinates, and each field represents a variable measured for each cluster. At the end of the output, you’ll also notice the aforementioned geometry column that contains the latitude and longitude for each displaced cluster location.\nSome clusters contain unverified coordinates and have been assigned a location of (0, 0). Since we have no way of linking these clusters to environmental data, we will remove them for this analysis using dplyr’s filter() function:\n\n# Remove empty geographies\nbf_gps &lt;- bf_gps |&gt; \n  filter(LATNUM != 0, LONGNUM != 0)\n\n\n\nVisualizing spatial data\nIt’s always worth double-checking that spatial data are being processed as expected, and this is often most easily done visually. We’ll use the {ggspatial} package to visualize our spatial data. This package is a convenient extension of the popular {ggplot2} package.\n\n\n    © RStudio, Inc. (MIT) \nAs with ggplot2, ggspatial allows us to think of our plot in layers. The primary difference is that ggspatial’s layer_spatial() automatically maps the plot’s x and y dimensions to the geographic coordinates in the data.\n\n\n\n\n\n\nggplot2 and Themes\n\n\n\nThis post isn’t intended to be an introduction to ggplot2, so don’t worry if some of the plot code is unfamiliar to you—you should still be able to follow along with the other content in the post.\nAlso, we’ve modified some of the default theme elements for use in our plots throughout this post. We won’t cover all the details here, but note that your plots will likely look a bit different if you’re following along. For more about themes, see the associated documentation.\n\n\nTo add some context to our map, we’ll load a shapefile containing the Burkina Faso administrative borders. We’ll use the integrated geographies provided by IPUMS, which can be downloaded by clicking the shapefile link under the Burkina Faso section of this table. We’ve placed this shapefile alongside our DHS coordinate data in our data/gps directory, and can load it with st_read(), as we did before:\n\n# Load administrative boundary shapefile\nbf_borders &lt;- st_read(\n  \"data/gps/geo_bf2003_2010/geo_bf2003_2010.shp\",\n  quiet = TRUE\n)\n\nNow, we can plot the cluster locations (stored in bf_gps) along with the Burkina Faso administrative boundaries (stored in bf_borders):\n\nlibrary(ggplot2)\nlibrary(ggspatial)\n\n# Create map with cluster locations and BF boundaries \nggplot() +\n  layer_spatial(bf_borders, fill = NA) +\n  layer_spatial(bf_gps, size = 2, alpha = 0.4) +\n  labs(\n    title = \"Approximate DHS Cluster Locations\", \n    subtitle = \"Burkina Faso: 2010\",\n    caption = \"Source: DHS Program\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  )\n\n\n\n\n\n\n\n\nSo far, everything seems to be working as expected—all the cluster points fall within the national borders, and there are noticeably more points in known urban areas, like the capital region of Ouagadougou. We should be safe to continue and load our precipitation data."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#chirps-precipitation-data",
    "href": "posts/2024-02-04-dhs-chirps/index.html#chirps-precipitation-data",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "CHIRPS precipitation data",
    "text": "CHIRPS precipitation data\nFor this post, we will use precipitation data from CHIRPS, a quasi-global gridded rainfall time eries with data from 1981 to the near-present.5 CHIRPS is frequently used in health, agricultural, and hydrological research because of its relatively high spatial and temporal resolution, and validation studies have shown it to perform better than other leading precipitation models across several metrics.6,7\nImportantly, the CHIRPS data are provided as raster data, not vector data. Instead of representing precipitation with a set of polygons or points (as with vector data), raster data represent geographic features in a grid, where each cell in the grid is associated with a particular value. In our case, each cell is associated with a single daily precipitation value (in millimeters).\n\n\n\nComparison of vector and raster data models4\n\n\nsf doesn’t provide support for raster data—instead, we’ll turn to the {terra} package.\n\nRaster data\nterra has rapidly become the go-to R package for working with raster data, and it is set to supersede the raster package, which was formerly used for raster operations.\n\n\n    © Robert J. Hijmans et al. (GPL &gt;=3) \nIn most cases, terra provides much faster processing than does the raster package. The magic behind terra is that it avoids reading many raster images into R at once. Instead, it reads metadata about each image—information about its spatial extent, coordinate reference system, pixel count, and so on.\n\n\n\n\n\n\nraster Package Retirement\n\n\n\nAt the time of writing, the raster package is still available on CRAN, but it will be retired soon. You may see other resources referencing the raster package, but we suggest relying only on terra to ensure that your code is robust to the upcoming retirement.\n\n\n\nInstalling terra\nYou’ll need the same operating system dependencies required for sf to use terra to its full potential. If you’ve already got those set up for sf, it should be easy to install terra with install.packages(\"terra\"). If not, follow these instructions to set up the package.\n\n\n\nLoading CHIRPS data\nThe complete CHIRPS precipitation data series can be downloaded directly from the UCSB Climate Hazards Center. However, because storing global data can require significant space, most users will likely want to download data for a specific area of interest.\nThere are two primary ways to go about downloading CHIRPS data for a particular area:\n\nSelect data interactively through the CHIRPS API provider: ClimateSERV. ClimateSERV hosts a graphical user interface (GUI) that allows you to select an area of interest on a map.\nAccess data through the CHIRPS API using the API client tools provided by the {chirps} package.\n\nThe first option has the advantage of being visual and intuitive. However, if you need to update your data for a new area or time range, you’ll have to go through the manual download process again.\nIn contrast, the second option prioritizes reproducibility and adaptability: updating the data used in your analysis is a matter of adjusting just a few lines of code. However, your R session will be occupied while downloading data, which can take a fair amount of time for long time series.\n\nOption 1: ClimateSERV\nFirst, navigate to the ClimateSERV Map. From there, open the Statistical Query toolbar on the top of the left sidebar and click Select. Click Country under the Select features by dropdown. Then, click on the country of interest—in this case, Burkina Faso (in West Africa).\n\n\n\n\n\n\nWarning\n\n\n\nThe ClimateSERV interface may not work properly on all web browsers (e.g. Firefox). If you have difficulty, try opening the interface in a different browser.\n\n\n\n\n\n\n\n\n\n\n\nOnce you’ve indicated the country of interest, change the type of request to Download Raw Data and the download format to TIF. Finally, adjust the date range to start on 01/01/2001 and end on 12/31/2010. This will give us a 10-year range running up to the survey year, allowing us to calculate a long-run average.\nFinally, click Submit Query to download the daily rainfall totals. Note that downloading this much data may take a fair amount of time!\nOnce your request has been processed, you’ll receive a compressed (.zip) folder containing one TIF image for each day in the time span: that’s 3652 files containing many megabytes of raster data. We’ve placed the .zip file in the data directory and renamed it chirps.zip. You can unzip the file in R with unzip():\n\n# Unzip and place individual .tif files in a new `data/chirps` directory\nchirps_files &lt;- unzip(\"data/chirps.zip\", exdir = \"data/chirps\")\n\nOr, if you prefer to unzip the file manually, you can do so and then list the individual raster files with list.files():\n\n# Create list of paths to all individual raster files\nchirps_files &lt;- list.files(\"data/chirps\", full.names = TRUE)\n\nWe can load raster files with the rast() function from terra. When providing multiple files, terra will stack them so each input file becomes a single layer in a larger raster stack:\n\nlibrary(terra)\n\n# Load set of .tif files into into a single raster stack\nbf_precip &lt;- rast(chirps_files)\n\n\n\nOption 2: The chirps package\nWe can use the get_chirps() function from the {chirps} package to access CHIRPS data via the CHIRPS API. Since we don’t have a map where we can select our region of interest, we’ll have to provide a pre-loaded region representing the area for which we’d like data.\nWe can use the bf_borders object we created earlier to get data for the Burkina Faso region. We’ll use vect() to convert it to a SpatVector from terra (get_chirps() currently has more robust support for these objects). We’ll also input our desired date range for which to obtain data:\n\n\nThe SpatVector is the main object terra uses to represent vector data. We’ve already shown how to use sf to handle vector data, but terra also provides support for this data model. In general, we find that sf’s vector objects are a little more intuitive, but there are some cases where using terra may make certain operations easier or faster.\n\n# Load the chirps package\nlibrary(chirps)\n\n# Get CHIRPS data from 2001-2010 for Burkina Faso \nbf_precip &lt;- get_chirps(\n  vect(bf_borders),\n  dates = c(\"2001-01-01\",\"2010-12-31\"),\n  server = \"CHC\" # Recommended when obtaining data for multiple dates and/or locations\n)\n\nget_chirps() obtains data for a rectangular region around the country. For consistency with the results you would get from ClimateSERV (which only downloads data within the country borders), we’ll use terra’s mask() function to remove data from outside the Burkina Faso borders:\n\nbf_precip &lt;- mask(bf_precip, bf_borders, touches = FALSE)\n\n\n\n\n\n\n\nClimateSERV vs. chirps\n\n\n\nThere may be some trivial discrepancies between the data obtained via ClimateSERV and those obtained via the chirps package. When creating this post, we used data obtained via ClimateSERV.\nIf you instead obtained data from chirps, your values may differ slightly from those we present below. These differences shouldn’t meaningfully affect any of the operations we demonstrate.\n\n\n\n\n\nSpatRaster objects\nRegardless of the method you chose to access the CHIRPS data, you should now have a SpatRaster object from terra containing the raster stack of precipitation data covering the entirety of Burkina Faso from 2001 to 2010:\n\nbf_precip\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 115, 159, 3652  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -5.549997, 2.400003, 9.349999, 15.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; sources     : 20010101.tif  \n#&gt;               20010102.tif  \n#&gt;               20010103.tif  \n#&gt;               ... and 3649 more sources\n#&gt; names       : 20010101, 20010102, 20010103, 20010104, 20010105, 20010106, ...\n\nAs you can see, this raster layer contains 115 rows and 159 columns of pixels. The value in each pixel represents the rainfall (in millimeters) for an area 0.05 degrees longitude by 0.05 degrees latitude (shown in the resolution field).\nEach of the 3652 layers in the SpatRaster represents the CHIRPS precipitation values for a single day of the 10-year period between 2001 and 2010.\nTo better understand this structure, we can visualize the rainfall patterns for one of these days. We’ll first need to select an individual layer from the raster stack. We can index layers by name or layer position using [[ notation:\n\n# Select a single layer\n# (Layer names will vary depending on how you obtained your CHIRPS data)\nprecip_day &lt;- bf_precip[[\"20010719\"]]\n\nOnce we’ve selected a layer, we can use layer_spatial() to plot it, just as we did for vector objects.\n\n\nShow plot code\n# Create map of rainfall for single day\nggplot() +\n  layer_spatial(\n    precip_day, \n    alpha = if_else(values(precip_day) == 0, 0, 0.8), \n    na.rm = TRUE\n  ) +\n  layer_spatial(bf_borders, fill = NA, color = \"#888888\") +\n  layer_spatial(bf_gps,  fill = NA, size = 2, alpha = 0.4) +\n  labs(\n    title = \"Burkina Faso Rainfall: July 19, 2001\",\n    subtitle = \"CHIRPS precipitation data with DHS cluster locations\",\n    fill = \"Rainfall total (mm)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  ) +\n  scale_fill_gradient(low = \"white\", high = \"#00263A\", na.value = NA)\n\n\n\n\n\n\n\n\n\nIt can be unwieldy to manage many layers of raster data. In the next section, we’ll introduce methods for simplifying these data across both time and space so they can be more easily attached to our DHS survey."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#cluster-buffers",
    "href": "posts/2024-02-04-dhs-chirps/index.html#cluster-buffers",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "Cluster buffers",
    "text": "Cluster buffers\nWe might think it logical to extract the mean precipitation values at each cluster’s point location. However, recall that our motivating question centered around the idea that precipitation influences health by way of impacting local crop yields. The farms that serve the population of a given cluster won’t necessarily be situated right at the recorded cluster location. Furthermore, each farm is probably impacted by the precipitation patterns in its surrounding area, not only those that occur at its exact location.\nWith this in mind, it makes sense to generate a summary metric of the precipitation in the general area of each cluster, rather than at the cluster point locations. We can accomplish this by creating buffer zones around each cluster coordinate.\nWe can use st_buffer() to create an appropriately sized buffer zone around the displaced GPS coordinates for each cluster. However, sf’s geometrical operations (powered by GEOS) assume that the input data are in meters, and our data are currently in degrees.\nTherefore, we first need to project our GPS coordinates into a new coordinate reference system, or CRS. We will use the UTM Coordinate System Zone 30N for our projection, which provides limited distortion for Burkina Faso. The EPSG code for this UTM zone is 32630, which we can provide to st_transform() to project our data:\n\n\nMap projections constitute an entire topic on their own. To learn more about projected coordinate systems, there are countless resources online, like this treatment from ESRI\n\n# Project cluster locations to the UTM 30N reference system\nbf_gps &lt;- st_transform(bf_gps, crs = 32630)\n\nWe can use st_geometry() to see a summary of our sf object’s geometry. Note that the Projected CRS now shows UTM zone 30N, and that our data are now stored in meters, rather than degrees:\n\nst_geometry(bf_gps)\n#&gt; Geometry set for 541 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 234659.3 ymin: 1092462 xmax: 1039190 ymax: 1645174\n#&gt; Projected CRS: WGS 84 / UTM zone 30N\n#&gt; First 5 geometries:\n#&gt; POINT (521084.1 1117516)\n#&gt; POINT (400626 1148510)\n#&gt; POINT (508145.5 1092462)\n#&gt; POINT (590542.4 1500704)\n#&gt; POINT (449803.2 1376723)\n\n\n\nIf you need to check your work, you can always access the current CRS of your data with st_crs().\nNow that we’re working in meters, we can buffer our points using st_buffer(). The appropriate buffer distance will depend on your specific research question of interest. For this demonstration, we’ll create a 10-kilometer (10,000 meter) buffer by setting dist = 10000:\n\nbf_gps &lt;- st_buffer(bf_gps, dist = 10000)\n\nWe see that the geometry column now describes POLYGON geometries rather than POINT geometries. Each polygon is defined by a series of points that form the circumference of the buffer zone.\n\nst_geometry(bf_gps)\n#&gt; Geometry set for 541 features \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 224659.3 ymin: 1082462 xmax: 1049190 ymax: 1655174\n#&gt; Projected CRS: WGS 84 / UTM zone 30N\n#&gt; First 5 geometries:\n#&gt; POLYGON ((531084.1 1117516, 531070.3 1116993, 5...\n#&gt; POLYGON ((410626 1148510, 410612.3 1147987, 410...\n#&gt; POLYGON ((518145.5 1092462, 518131.8 1091939, 5...\n#&gt; POLYGON ((600542.4 1500704, 600528.7 1500181, 6...\n#&gt; POLYGON ((459803.2 1376723, 459789.5 1376200, 4...\n\nWe’re finished measuring distance in meters, so we’ll again use st_transform() to revert back to degrees of latitude and longitude (EPSG 4326) for consistency with our other maps.\n\nbf_gps &lt;- st_transform(bf_gps, crs = 4326)\n\n\n\nShow plot code\nggplot() + \n  layer_spatial(bf_borders, fill = NA) +\n  layer_spatial(bf_gps, fill = \"black\", alpha = 0.2) +\n  labs(\n    title = \"Buffered DHS Cluster Locations\",\n    subtitle = \"Burkina Faso: 2010\",\n    caption = \"Source: DHS Program\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling cluster displacement\n\n\n\nPreviously, we discussed the fact that the cluster coordinates provided by The DHS Program are displaced from their true locations. While we don’t need to address this for our precipitation data, there may be other cases where it is useful to obtain a more precise location for each cluster (for instance, if working with road networks or other fine-grained data).\nIn these cases, you can certainly buffer points conditionally on other values in the data by providing a conditional statement to the buffer dist argument. For instance, to buffer urban clusters (URBAN_RURA == \"U\") by 2000 meters and other clusters by 5000 meters, use an if_else() statement:\n\nst_buffer(\n  bf_gps, \n  dist = if_else(bf_gps$URBAN_RURA == \"U\", 2000, 5000)\n)"
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#aggregating-rainfall-within-cluster-regions",
    "href": "posts/2024-02-04-dhs-chirps/index.html#aggregating-rainfall-within-cluster-regions",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "Aggregating rainfall within cluster regions",
    "text": "Aggregating rainfall within cluster regions\nNow that we have buffered cluster regions, we can focus on combining our two data sources. Let’s zoom in on a single cluster as an example. We’ll use the cluster with DHSCLUST number 160:\n\nclust &lt;- bf_gps |&gt; \n  filter(DHSCLUST == 160)\n\nTo simplify this example, we’ll use terra’s crop() to restrict our raster to the cells surrounding this cluster:\n\nprecip_clust &lt;- crop(bf_precip_mean, clust, snap = \"out\")\n\n\n\nShow plot code\nggplot() + \n  layer_spatial(precip_clust, alpha = 0.8) + \n  layer_spatial(clust, alpha = 0, linewidth = 0.5, color = \"black\") +\n  labs(\n    title = \"Average Rainfall: 2001-2010\",\n    subtitle = \"Single DHS cluster\",\n    fill = \"10-year average rainfall (mm/day)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  scale_fill_gradient(\n    low = \"#FAEFD1\", \n    high = \"#00263A\", \n    na.value = NA, \n    limits = c(2.3, 2.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nFor demonstration purposes, we’ve used different limits for the color scale in this plot and the next. In general, we’d want to ensure that the colors in this plot map to the same values used in other plots with the same color scheme.\nWe see a range of rainfall totals across the 20 pixels in this area. How can we summarize the information contained in these individual pixels into a single estimate of the rainfall for this buffered cluster region?\nThe most straightforward approach might be to take the mean of all the pixel values that overlap our buffer region. First, we need to obtain the values of the overlapping raster cells. We can use terra’s extract() to obtain the raster cell values from the region defined by an overlapping vector data source (in this case, our example cluster, clust):\n\nextract(bf_precip_mean, clust)\n#&gt;    ID     mean\n#&gt; 1   1 2.337887\n#&gt; 2   1 2.329431\n#&gt; 3   1 2.344792\n#&gt; 4   1 2.317271\n#&gt; 5   1 2.343546\n#&gt; 6   1 2.329051\n#&gt; 7   1 2.381871\n#&gt; 8   1 2.367312\n#&gt; 9   1 2.395538\n#&gt; 10  1 2.395361\n#&gt; 11  1 2.402505\n#&gt; 12  1 2.415130\n\nNote that this provides mean daily precipitation values for 12 different cells. By default, extract() will only extract values from the cells whose center point lies within the overlaid polygon:\n\n\nShow plot code\n# New SpatRaster with same extent as bf_precip_mean, but with empty values\next_cells &lt;- rast(bf_precip_mean, vals = NA)\n\n# ID cell locations for each cell that is extracted\ncell_idx &lt;- extract(ext_cells, clust, cells = TRUE)$cell\n\next_cells[cell_idx] &lt;- bf_precip_mean[cell_idx]\n\nggplot() + \n  layer_spatial(\n    crop(ext_cells, clust, snap = \"out\"), \n    alpha = 0.8, \n    na.rm = TRUE\n  ) +\n  layer_spatial(clust, alpha = 0, linewidth = 0.5, color = \"black\") +\n  labs(\n    title = \"Average Rainfall: 2001-2010\",\n    subtitle = \"Single DHS cluster\",\n    fill = \"10-year average rainfall (mm/day)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  scale_fill_gradient(\n    low = \"#FAEFD1\", \n    high = \"#00263A\", \n    na.value = NA, \n    limits = c(2.3, 2.5)\n  )\n\n\n\n\n\n\n\n\n\nTo instead include all cells that intersect the polygon, set touches = TRUE:\n\nclust_precip &lt;- extract(bf_precip_mean, clust, touches = TRUE)\n\nclust_precip\n#&gt;    ID     mean\n#&gt; 1   1 2.350189\n#&gt; 2   1 2.319925\n#&gt; 3   1 2.385132\n#&gt; 4   1 2.337887\n#&gt; 5   1 2.329431\n#&gt; 6   1 2.328763\n#&gt; 7   1 2.344792\n#&gt; 8   1 2.317271\n#&gt; 9   1 2.343546\n#&gt; 10  1 2.329051\n#&gt; 11  1 2.381871\n#&gt; 12  1 2.367312\n#&gt; 13  1 2.395538\n#&gt; 14  1 2.395361\n#&gt; 15  1 2.393321\n#&gt; 16  1 2.402505\n#&gt; 17  1 2.415130\n#&gt; 18  1 2.395817\n\nThis operation provides the mean daily precipitation values for each of the 18 cells that intersect our cluster region. However, some cells cover a much larger portion of the cluster region than others. To account for the heterogeneity in size, we can set weights = TRUE to include a weight column, which records the proportion of each cell that is covered by the polygon:\n\n\nweights = TRUE automatically includes all intersected cells, but those with negligible weights will be removed.\n\nclust_precip &lt;- extract(bf_precip_mean, clust, weights = TRUE)\n\nclust_precip\n#&gt;    ID     mean weight\n#&gt; 1   1 2.385132   0.20\n#&gt; 2   1 2.337887   0.90\n#&gt; 3   1 2.329431   0.94\n#&gt; 4   1 2.328763   0.33\n#&gt; 5   1 2.344792   0.70\n#&gt; 6   1 2.317271   1.00\n#&gt; 7   1 2.343546   1.00\n#&gt; 8   1 2.329051   0.86\n#&gt; 9   1 2.381871   0.57\n#&gt; 10  1 2.367312   1.00\n#&gt; 11  1 2.395538   1.00\n#&gt; 12  1 2.395361   0.74\n#&gt; 13  1 2.393321   0.03\n#&gt; 14  1 2.402505   0.47\n#&gt; 15  1 2.415130   0.53\n#&gt; 16  1 2.395817   0.09\n\nWe can then provide the weights to the weighted.mean() function to calculate a weighted mean that is shifted toward the cell values that form a larger proportion of the polygon area.\n\nweighted.mean(clust_precip$mean, clust_precip$weight)\n#&gt; [1] 2.358508\n\nBecause this kind of workflow is so common, terra provides a quicker shorthand in extract(). You can use the fun argument to specify an aggregation function at the same time that you extract the raster cell values:\n\n# Extract area-weighted average for all cells covered by `clust`\nextract(\n  bf_precip_mean,\n  clust,\n  fun = \"mean\", # Average all extracted values\n  weights = TRUE # Use area weights\n)\n#&gt;   ID     mean\n#&gt; 1  1 2.358508\n\nThe 2.358508 value represents the area-weighted mean of the 10-year mean precipitation values (in millimeters per day) within the given cluster region.\n\nScaling up for all clusters\nTo aggregate the rainfall data for each DHS cluster in the sample, we simply scale up. We again use extract(), but we provide our raster of precipitation values for the entire country and the polygons for all of our buffered clusters.\n\n# Average the mean CHIRPS values within each DHS cluster\nclust_means &lt;- extract(\n  bf_precip_mean,\n  bf_gps, # All clusters included\n  fun = \"mean\",\n  weights = TRUE,\n  na.rm = TRUE, # Ignore missing CHIRPS values\n  bind = TRUE # Recombine with original `bf_gps` data\n)\n\nBy setting bind = TRUE, we can automatically recombine the extracted means with the other cluster data in bf_gps.\nWe also set na.rm = TRUE to ensure we don’t get missing values for clusters whose regions expand beyond the country borders (where we haven’t downloaded any CHIRPS data). This decision may be reasonable if our interest is in rainfall over local crop-producing regions, and we don’t believe that clusters in border regions obtain substantial inputs from farms outside of the country. (If we wanted to include the rainfall outside of the country borders, we would simply need to expand our bounding box when downloading our CHIRPS data.)\nFinally, we’ll convert our cluster means back to an sf object. Notice that there now exists a mean column that stores the weighted mean values for each cluster:\n\n# Convert to sf object\nclust_means &lt;- st_as_sf(clust_means)\n\nclust_means |&gt; \n  select(DHSID, mean, geometry)\n#&gt; Simple feature collection with 541 features and 2 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -5.517451 ymin: 9.792417 xmax: 2.049008 ymax: 14.95288\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;             DHSID     mean                       geometry\n#&gt; 1  BF201000000001 2.758146 POLYGON ((-2.71628 10.10935...\n#&gt; 2  BF201000000002 2.782180 POLYGON ((-3.816453 10.3887...\n#&gt; 3  BF201000000003 3.004422 POLYGON ((-2.834491 9.88283...\n#&gt; 4  BF201000000004 1.678181 POLYGON ((-2.070699 13.5730...\n#&gt; 5  BF201000000005 2.129466 POLYGON ((-3.369883 12.4534...\n#&gt; 6  BF201000000006 2.203529 POLYGON ((-1.991964 12.0449...\n#&gt; 7  BF201000000007 1.113443 POLYGON ((-0.579432 14.3532...\n#&gt; 8  BF201000000008 2.110246 POLYGON ((-1.470133 12.3105...\n#&gt; 9  BF201000000009 2.281224 POLYGON ((-0.2722169 11.779...\n#&gt; 10 BF201000000010 1.722371 POLYGON ((-1.245736 13.2245...\n\nNow that we have aggregated rainfall across all clusters, we can plot the average rainfall for each of the 10-kilometer buffered cluster regions. Each cluster region is associated with a single 10-year average rainfall value.\n\n\nShow plot code\nggplot() + \n  layer_spatial(bf_borders, fill = NA) +\n  layer_spatial(clust_means, aes(fill = mean), alpha = 0.8) +\n  labs(\n    title = \"Average Rainfall within Clusters: 2001-2010\",\n    subtitle = \"Burkina Faso: 2010 Clusters\",\n    fill = \"10-year average rainfall (mm/day)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  ) +\n  scale_fill_steps(low = \"#FAEFD1\", high = \"#00263A\", n.breaks = 4) +\n  theme(legend.ticks = element_blank())"
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#getting-help",
    "href": "posts/2024-02-04-dhs-chirps/index.html#getting-help",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html",
    "href": "posts/2024-04-09-spatial-harmonization/index.html",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "",
    "text": "The Demographic and Health Surveys (DHS) are the leading source of population and health data for low and middle-income countries. The data from the nationally representative DHS samples can be disaggregated by regions and, in some cases, by sub-regions, such as districts. However, changes in the boundaries of reported regions are common, due to either shifts in the survey sampling strategy or due to political changes within countries. Researchers using DHS data to study change over time need to hold space constant by using regions with a consistent geographic footprint to get meaningful results.\nIPUMS DHS has addressed the issue of changing boundaries by providing users with both spatially harmonized (or integrated) and sample-specific geographic variables and GIS boundaries. Harmonized variables provide consistent geographic units for a country across the sample years, facilitating analytical comparisons across time. Sample-specific variables retain all geographic detail from each sample and are not consistent over time.\n\n\nDHS survey boundaries are obtained from the DHS Program’s Spatial Data Repository and overlaid on a GIS software system. When geographic boundaries of a later sample do not align with units of the previous DHS survey for that country because of boundary changes, we create larger aggregated units that are stable over time. We refer to this process as the spatial harmonization of geographic boundaries.\nIf administrative units split or merge, the harmonized unit will have the boundaries of the largest version of the unit. If a territory is redistributed between two or more units, the units are combined. Some detail is always lost in the process of spatial harmonization because administrative units are merged to find the geographic least common denominator.\nIn a few cases, boundaries have been reorganized to such an extent that harmonization is nearly impossible. In such cases (e.g., for Cameroon, Guinea, Jordan, and Mali), we have typically created sets of consistent units spanning different year ranges and placed them in different integrated variables. Sometimes early DHS surveys labeled regions through vague descriptive terms (e.g., “Coast” or “Mountains”) rather than naming recognized political units. In such cases, we have provided only sample-specific geographic variables (e.g. for Congo Brazzaville, Cote D’Ivoire, and Yemen)."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#the-spatial-harmonization-process",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#the-spatial-harmonization-process",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "",
    "text": "DHS survey boundaries are obtained from the DHS Program’s Spatial Data Repository and overlaid on a GIS software system. When geographic boundaries of a later sample do not align with units of the previous DHS survey for that country because of boundary changes, we create larger aggregated units that are stable over time. We refer to this process as the spatial harmonization of geographic boundaries.\nIf administrative units split or merge, the harmonized unit will have the boundaries of the largest version of the unit. If a territory is redistributed between two or more units, the units are combined. Some detail is always lost in the process of spatial harmonization because administrative units are merged to find the geographic least common denominator.\nIn a few cases, boundaries have been reorganized to such an extent that harmonization is nearly impossible. In such cases (e.g., for Cameroon, Guinea, Jordan, and Mali), we have typically created sets of consistent units spanning different year ranges and placed them in different integrated variables. Sometimes early DHS surveys labeled regions through vague descriptive terms (e.g., “Coast” or “Mountains”) rather than naming recognized political units. In such cases, we have provided only sample-specific geographic variables (e.g. for Congo Brazzaville, Cote D’Ivoire, and Yemen)."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#countries-and-samples",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#countries-and-samples",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "Countries and samples",
    "text": "Countries and samples\nIn this demonstration, we will highlight the sub-Saharan countries of Malawi, Tanzania, Mozambique, Zambia, and Zimbabwe. The table below lists these countries and their sample years and notes the changes in boundaries for each country. For Malawi, we use sub-regions or the second level administrative units; here, 28 sub-regions in 2010 increased to 32 sub-regions in 2016. For Tanzania and Zambia, the regional boundaries changed from one sample to the next. For Zimbabwe, there were no changes in regional boundaries from one sample year to another. We also display a single sample for Mozambique in our year 1 maps. Note that, for the purposes of this demonstration, we do not show all the sample years available in IPUMS DHS, but rather only selected sample years.\n\nSummary of samples used in this demonstration\n\n\n\n\n\n\n\n\nCountry\nSurvey Year 1\nSurvey Year 2\nBoundary changes?\n\n\n\n\nMalawi\n2010\n2016\nYes—28 sub-regions to 32 sub-regions\n\n\nTanzania\n2010\n2015\nYes—26 regions to 30 regions\n\n\nMozambique\n2011\n–\nNo—single year with 11 regions\n\n\nZambia\n2007\n2013\nYes—9 regions to 10 regions\n\n\nZimbabwe\n2010\n2015\nNo—10 regions in both years"
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#visualizing-the-percent-of-households-using-improved-drinking-water",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#visualizing-the-percent-of-households-using-improved-drinking-water",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "Visualizing the percent of households using improved drinking water",
    "text": "Visualizing the percent of households using improved drinking water\nAll maps shown below represent the percentage of households that use drinking water from improved sources. For the countries listed in the table above, paired maps display results from two time periods. Map A shows results from the earlier survey (i.e., 2010 Malawi, 2010 Tanzania, 2011 Mozambique, 2007 Zambia, and 2010 Zimbabwe) and Map B displays results from the later survey (i.e., 2016 Malawi, 2015 Tanzania, 2013 Zambia, and 2015 Zimbabwe). Shades of the color blue represent spatially harmonized maps; shades of the color green appear on sample-specific maps. In general, the darker the color, the higher the percentage of households using improved sources of drinking water.\nFigure 1 displays two sample-specific GIS maps, meaning the spatial boundaries for the two time periods for a given country are not the same. These maps show that, overall, the percentage of households using improved drinking water increased. The rate of increase differs, however, both between countries and within countries.\nWhile very broad conclusions about change based on sample-specific geography may be sound, a closer look at the maps shows problems with using sample-specific geography. We have circled three areas affected by changes in geography. Specifically, the changes consist of splitting of boundaries in eastern Zambia (the largest circle), reorganization of boundaries in northern Tanzania (the top circle), and separation of rural and urban areas in central Malawi (the right-hand circle).\n\n\n\n\nFigure 1: Percent of households that use improved sources of drinking water. Note that the maps do not show consistent spatial boundaries. The red circles indicate areas with changes in geographic boundaries.Map A shows 2010 data from Malawi, Tanzania, and Zimbabwe; 2011 data from Mozambique; and 2007 data from Zambia.Map B shows 2015 data from Tanzania and Zimbabwe; 2016 data from Malawi, and 2013 data from Zambia.\n\n\n\n\nWhen spatial harmonization is necessary\nFigure 2 displays the part of Zambia where two units split up to form three units. A new political region, Muchinga, was formed from parts of the Northern and Eastern regions. Although the names of the Northern and Eastern regions remained the same, their spatial footprint changed from 2007 to 2013.\nIf we focus on our metric of households with access to improved drinking water, it initially appears that the percentage in the Eastern region rose dramatically from 16% to 72%, while the change in the neighboring Northern region was from 19% to 30%.\nGiven changes in boundaries, the apparent increase in clean water access in the Eastern region is misleading. It is quite possible that rural areas from the former Eastern region in 2007 formed much of Muchinga in 2013, with the latter having only 35% of households with access to improved drinking water. The actual change for the Eastern region may be a shift toward more urban composition as it shed rural territory, rather than a sanitation revolution.\nThe solution to the problem of changing boundaries is imposing integrated geography for this section of Zambia. In this case, the three regions of Northern, Muchinga, and Eastern are combined into one region, with results mapped in Figure 3. Here, we see more modest improvement in this aggregated region, with the percentage of households with access to an improved water source changing from 18% to 52% between 2007 and 2013.\n\n\n\n\nFigure 2: Percent of households that use improved sources of drinking water. The maps highlight changes in sample-specific boundaries. Map A shows Zambia in 2007 and Map B shows Zambia in 2013. Muchinga was formed from parts of Northern and Eastern.\n\n\n\n\n\nFigure 3: Percent of households that use improved sources of drinking water. The maps highlight spatially harmonized boundaries. Map A shows Zambia in 2007 and Map B shows Zambia in 2013. In this map, Eastern, Northern, and Muchinga were combined to form a spatially consistent boundary.\n\n\n\nAs Figure 4 shows, between 2010 and 2015, boundaries in Tanzania were redrawn such that three units (Kagera, Mwanza, and Shinyanga) were reorganized to form five units (Kagera, Mwanza, Shinyanga, Simiyu, and Geita). While it is mechanically possible to map our metric for these various regions in the two years, boundary changes preclude our drawing meaningful conclusions about progress within sub-national units. The Tanzanian case illustrates a general principle: holding space constant is critical in analyzing change at sub-national levels, because units that have changed boundaries cannot be compared across time in a meaningful way.\nCreating integrated geography for this part of Tanzania required aggregating smaller units into a single larger unit, as show in Figure 5. Focusing on the single combined region, we see that the share of households with an improved source of drinking water increased from only 40% to 56% between 2010 and 2015.\n\n\n\n\nFigure 4: Percent of households that use improved sources of drinking water. The maps highlight changes in sample-specific boundaries. Map A shows Tanzania in 2010 with three regions (Kagera, Mwanza, Shinyanga). Map B shows Tanzania in 2015 with 5 regions (Kagera, Mwanza, Shinyanga, Simiyu, and Geita) in the same spatial area.\n\n\n\n\n\nFigure 5: Percent of households that use improved sources of drinking water. The maps highlight spatially harmonized boundaries. Map A shows Tanzania in 2010, and Map B shows Tanzania in 2015. The 5 units Kagera, Mwanza, Shinyanga, Simiyu, and Geita were combined to form a single spatially consistent unit.\n\n\n\n\n\nGains and losses from using integrated geography\nFor spatial harmonization, we hold boundaries constant over time. In our prior examples, sub-national boundaries were merged to override any boundary changes that occurred between DHS samples. While that approach enables an apples‐to‐apples temporal comparison of places, detail that might be useful for the analysis gets lost.\nBoundary changes in Malawi illustrate this point. Consider Figure 6 and Figure 7. Map A from 2010 shows 74% households using an improved source of drinking water in Nkhata Bay, with the number inching up to 76% in 2016. The blue spatially harmonized maps in Figure 7 also show a lack of change. However, Figure 7 hides the advantaged sanitary position of Mzuzu City, which split from Nkhata Bay, with 99% of households in that city having improved drinking water in 2016. Similar results mark other urban areas, such as Lilongwe city, Zomba city, and Blantyre city (Zomba and Blantyre not displayed on the map). Taking full advantage of sample-specific geographic detail demonstrates that, in this instance, apparent regional progress in safe water access was largely localized in urban areas. As this case illustrates, sample-specific geography often provides greater detail and should be used in conjunction with spatially harmonized geography.\n\n\n\n\nFigure 6: Percent of households that use improved sources of drinking water. The maps highlight changes in sample-specific boundaries from 2010 to 2016. Map A shows Malawi in 2010 and Map B shows Malawi in 2016.\n\n\n\n\n\nFigure 7: Percent of households that use improved sources of drinking water. The maps highlight spatially harmonized boundaries. Map A shows Malawi in 2010 and Map B shows Malawi in 2016. In this map the sub-regions Nkhatabay and Likoma, Lilongwe and Lilongwe city, and Mzimba and Mzuzu city are combined.\n\n\n\n\n\nWhen geographic boundaries don’t change\nWhile Zambia, Tanzania, and Malawi all had regional boundary changes between DHS samples, sometimes regional boundaries stay the same across two or more DHS surveys. This was the case for Zimbabwe between 2010 and 2015, as shown in Figure 8, which displays all the countries used in this demonstration. For instances like Zimbabwe, IPUMS DHS still offers multi-year “integrated geography” variables (without modifying boundaries), for the convenience of researchers and to ensure that geographic footprints as well as regional labels stay the same across time. In other cases, only a single sample is available for a country in the DHS, for all years or within a limited time span. This is exemplified by Mozambique, with a single sample from 2011 (shown in Map A but not included in Map B).\n\n\n\n\nFigure 8: Percent of households that use improved sources of drinking water. Note that both the maps show spatially harmonized boundaries.Map A shows 2010 data from Malawi, Tanzania, and Zimbabwe, 2011 data from Mozambique, and 2007 data from Zambia.Map B shows 2015 data from Tanzania and Zimbabwe, 2016 data from Malawi, and 2013 data from Zambia."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#getting-help",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#getting-help",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html",
    "href": "posts/2024-05-31-ts-join/index.html",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "",
    "text": "In our last technical post, we demonstrated how to calculate several different monthly temperature metrics with the ultimate goal of linking these values to individual child records in the 2012 Mali DHS survey.\nHowever, in contrast to the technique we used in our CHIRPS post, we can’t simply join our temperature data onto our DHS sample by enumeration cluster alone because each child in our sample has a different birth date, and therefore different temperature exposure prior to birth.\nTo account for this, we need to join our temperature data to each child survey record both by enumeration cluster (which contains the spatial information) as well as the child’s birth date (which contains the temporal information).\nFirst, we’ll identify the monthly CHIRTS data corresponding to each of our enumeration cluster areas, producing a time series of temperature values for each cluster. Next, we’ll identify the time series of temperature values for the 9 months prior to each child’s birth and use it to calculate trimester-specific temperature exposure values for each child. Finally, we’ll use the joined data to build a simple model predicting birth weight outcomes from temperature exposure.\nThis post will build on our previous post where we built several monthly temperature metrics. If you haven’t had the chance to read through that post yet, we encourage you to start there.\nBefore getting started, we’ll load the necessary packages for this post:\nlibrary(ipumsr)\nlibrary(sf)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(terra)"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#dhs-survey-data",
    "href": "posts/2024-05-31-ts-join/index.html#dhs-survey-data",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "DHS survey data",
    "text": "DHS survey data\nWe will obtain 2012 Mali data from IPUMS DHS. In this extract, we’ve selected the following variables:\n\nHEIGHTFEM: Height of woman in centimeters\nKIDSEX: Sex of child\nKIDDOBCMC: Child’s date of birth (CMC)\nBIRTHWT: Birth weight in kilos\nBIRTHWTREF: Source of weight at birth (health card or recall)\n\n\n\nCheck out our walkthrough for downloading IPUMS DHS data if you need help producing a similar extract.\nWe’ll load our data using {ipumsr}:\n\nml_dhs &lt;- read_ipums_micro(\"data/idhs_00021.xml\")\n#&gt; Use of data from IPUMS DHS is subject to conditions including that users should cite the data appropriately. Use command `ipums_conditions()` for more details.\n\nml_dhs\n#&gt; # A tibble: 10,326 × 48\n#&gt;    SAMPLE            SAMPLESTR     COUNTRY    YEAR IDHSPID IDHSHID DHSID IDHSPSU\n#&gt;    &lt;int+lbl&gt;         &lt;chr+lbl&gt;     &lt;int+lbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt;  1 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  2 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  3 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  4 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  5 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  6 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  7 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  8 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  9 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt; 10 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt; # ℹ 10,316 more rows\n#&gt; # ℹ 40 more variables: IDHSSTRATA &lt;dbl&gt;, CASEID &lt;chr&gt;, HHID &lt;chr&gt;, PSU &lt;dbl&gt;,\n#&gt; #   STRATA &lt;dbl&gt;, DOMAIN &lt;dbl&gt;, HHNUM &lt;dbl&gt;, CLUSTERNO &lt;dbl&gt;, LINENO &lt;int&gt;,\n#&gt; #   BIDX &lt;int&gt;, PERWEIGHT &lt;dbl&gt;, KIDWT &lt;dbl&gt;, AWFACTT &lt;dbl&gt;, AWFACTU &lt;dbl&gt;,\n#&gt; #   AWFACTR &lt;dbl&gt;, AWFACTE &lt;dbl&gt;, AWFACTW &lt;dbl&gt;, DVWEIGHT &lt;dbl&gt;,\n#&gt; #   URBAN &lt;int+lbl&gt;, GEO_ML1987_2018 &lt;int+lbl&gt;, GEO_ML1995_2018 &lt;int+lbl&gt;,\n#&gt; #   GEO_ML2012 &lt;int+lbl&gt;, AGE &lt;int&gt;, AGE5YEAR &lt;int+lbl&gt;, RESIDENT &lt;int+lbl&gt;, …"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#dhs-boundaries",
    "href": "posts/2024-05-31-ts-join/index.html#dhs-boundaries",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "DHS boundaries",
    "text": "DHS boundaries\nWe’ll use the Mali borders that we prepared in our previous CHIRTS post. We describe the process there, but we’ve reproduced the code in the following collapsed block if you need to refresh your memory. As a reminder, you can obtain IPUMS integrated geography files from this table.\n\n\nBorder preparation\nml_borders &lt;- read_ipums_sf(\"data/gps/geo_ml1995_2018.zip\")\n\n# Validate internal borders\nml_borders_neat &lt;- st_make_valid(ml_borders)\n\n# Collapse internal borders to get single country border\nml_borders_out &lt;- ml_borders_neat |&gt; \n  st_union() |&gt; \n  st_simplify(dTolerance = 1000) |&gt; \n  st_as_sf()\n\n# Transform to UTM 29N coordinates, buffer, and convert back to WGS84\nml_borders_buffer &lt;- ml_borders_out |&gt; \n  st_transform(crs = 32629) |&gt; \n  st_buffer(dist = 10000) |&gt;\n  st_transform(crs = 4326)"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#dhs-enumeration-clusters",
    "href": "posts/2024-05-31-ts-join/index.html#dhs-enumeration-clusters",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "DHS enumeration clusters",
    "text": "DHS enumeration clusters\nWe can download the enumeration cluster coordinates from the DHS Program. We first described this process in our CHIRPS post. Simply follow the same instructions, substituting Mali where we previously used Burkina Faso.\nYou should obtain a file called MLGE6BFL.zip, which can be loaded with read_ipums_sf():\n\nml_clust &lt;- read_ipums_sf(\"data/MLGE6BFL.zip\")\n\nAs we’ve done in the past, we’ll buffer our cluster coordinates so we capture the environmental effects of the general region around each survey location\n\nml_clust_buffer &lt;- ml_clust |&gt; \n  st_transform(crs = 32629) |&gt; \n  st_buffer(dist = 10000) |&gt;\n  st_transform(crs = 4326)"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#chirts",
    "href": "posts/2024-05-31-ts-join/index.html#chirts",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "CHIRTS",
    "text": "CHIRTS\nFor this post, we’ll use the monthly heatwave proportions that we calculated at the end of our previous CHIRTS post.\nHowever for the purposes of demonstration, in that post we used a single year of CHIRTS data. Now that we’re planning to attach temperature data to our DHS survey data, we need to consider the full range of birth dates represented in our sample.\n\nExtending CHIRTS time series\nThe DHS contains records for children born within 5 years of the survey date to women between 15 and 49 years of age. Thus, for the 2012 Mali sample, we will have records for children ranging from 2008-2012. Further, since we want to identify monthly weather records for the time preceding each birth, we will also need to have data for the year prior to the earliest birth in the sample.\nThat means that for our 2012 Mali sample, we’ll actually need CHIRTS data for 2007-2012.\nFortunately, the pipeline we set up in our previous post can be easily scaled to accommodate additional years of data. Recall that we built a dedicated function to calculate monthly heatwave proportions:\n\nprop_heatwave &lt;- function(temps, thresh, n_seq) {\n  # Convert to RLE of days above threshold\n  bin_rle &lt;- rle(temps &gt;= thresh)\n  \n  # Identify heatwave events based on sequence length\n  is_heatwave &lt;- bin_rle$values & (bin_rle$lengths &gt;= n_seq)\n  \n  # Count heatwave days and divide by total number of days\n  sum(bin_rle$lengths[is_heatwave]) / length(temps)\n}\n\nAll we need to do, then, is apply this function to input CHIRTS data for 2007 to 2012, rather than just for 2012. To do so, we’ve downloaded all 6 years of data manually as described previously and placed each file in a data/chirts directory.\nWe can load them by first obtaining the file path to each file with list.files(). Then, we use the familiar rast() to load all 6 files into a single SpatRaster.\n\nchirts_files &lt;- list.files(\"data/chirts\", full.names = TRUE)\n\nml_chirts &lt;- rast(chirts_files)\n\nAs we did last time, we’ll crop the CHIRTS raster to the region directly around Mali:\n\nml_chirts &lt;- crop(ml_chirts, ml_borders_buffer, snap = \"out\")\n\nNow we can use the tapp() function introduced in our previous post to calculate monthly heatwave proportions for each month/year combination. The only difference here is that we use index = \"yearmonths\" instead of index = \"months\" since we have multiple years of data. This ensures that we don’t aggregate months together across years.\n\n# Calculate monthly proportion of heatwave days\nml_chirts_heatwave &lt;- tapp(\n  ml_chirts,\n  function(x) prop_heatwave(x, thresh = 35, n_seq = 3),\n  index = \"yearmonths\" # New index to account for multiple years\n)\n\nAs expected, we now have a SpatRaster with 72 layers: one for each month/year combination from 2007-2012. Each layer contains the proportion of days that met our heatwave threshold of 3+ consecutive days exceeding 35°C.\n\nml_chirts_heatwave\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 72  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 \n#&gt; source      : ml_chirts_heatwave.nc \n#&gt; names       : ml_ch~ave_1, ml_ch~ave_2, ml_ch~ave_3, ml_ch~ave_4, ml_ch~ave_5, ml_ch~ave_6, ... \n#&gt; time (ymnts): 2007-Jan to 2012-Dec\n\n\n\n\n\n\n\nCaution\n\n\n\nThis post was built using terra 1.7-78. Older versions of terra contain a bug that may produce incorrect time units when index = \"yearmonths\".\nIf you’re running an older version of terra, we suggest updating the package with install.packages(\"terra\"). Otherwise, you may be able to temporarily avoid the issue by manually setting the correct time values:\n\n# Manual update of offset date values\nyear_months &lt;- seq(\n  lubridate::ym(\"2007-01\"), \n  lubridate::ym(\"2012-12\"), \n  by = \"months\"\n)\n\ntime(ml_chirts_heatwave) &lt;- year_months"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#reconcile-date-representations",
    "href": "posts/2024-05-31-ts-join/index.html#reconcile-date-representations",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Reconcile date representations",
    "text": "Reconcile date representations\nTo deal with the temporal component of the join, we’ll need to identify the relevant months of data for each child in our DHS sample. Each child’s birth date recorded in the KIDDOBCMC variable:\n\nml_dhs$KIDDOBCMC\n#&gt;     [1] 1311 1301 1316 1356 1305 1337 1301 1318 1337 1313 1354 1313 1326 1312\n#&gt;    [15] 1330 1316 1354 1311 1335 1338 1305 1321 1305 1347 1315 1354 1309 1323\n#&gt;    [29] 1324 1349 1315 1297 1334 1317 1336 1334 1338 1304 1323 1307 1354 1311\n....\n\nYou might have expected that these would be dates, but we actually have a series of 4-digit numbers. This is because dates are encoded as century month codes in the DHS. A century month code (CMC) encodes time as the number of months that have elapsed since 1900. CMCs are useful for calculating intervals of time, because they can be easily added and subtracted.\nHowever, the time encoded in our CHIRTS heatwave data aren’t encoded in CMC format, so we’ll need to reconcile these two time representations.\n\nWorking with CMCs\nSince converting between CMCs and traditional dates is a common and well-defined task, we’ll build some helper functions to handle the conversion. That way, we can easily convert back and forth without having to remember the CMC conversion formula each time.\nFortunately, the description of the KIDDOBCMC variable describes the arithmetic required to convert. We’ve translated that text into the following function, which takes an input CMC and converts it to year-month format:\n\n\nYou can view the description for any variable in your extract using the ipums_var_desc() function from ipumsr.\n\ncmc_to_ym &lt;- function(cmc) {\n  year &lt;- floor((cmc - 1) / 12) + 1900\n  month &lt;- cmc - ((year - 1900) * 12)\n  \n  year_month &lt;- paste(year, month, sep = \"-\")\n  \n  lubridate::ym(year_month)\n}\n\nSimilarly, we’ll create a function that goes in the reverse direction:\n\nym_to_cmc &lt;- function(date) {\n  year &lt;- lubridate::year(date)\n  month &lt;- lubridate::month(date)\n  \n  (year - 1900) * 12 + month\n}\n\nFor instance, a CMC of 1307 turns out to be the same as November, 2008:\n\ncmc_to_ym(1307)\n#&gt; [1] \"2008-11-01\"\n\nAnd vice versa:\n\nym_to_cmc(\"2008-11-01\")\n#&gt; [1] 1307\n\nNow we have an easy way to reconcile the temporal information in our two data sources."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#attaching-heatwaves-to-dhs-clusters",
    "href": "posts/2024-05-31-ts-join/index.html#attaching-heatwaves-to-dhs-clusters",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Attaching heatwaves to DHS clusters",
    "text": "Attaching heatwaves to DHS clusters\nTo extract the average heatwave proportions for each DHS enumeration cluster, we’ll use terra’s extract(), which we’ve introduced previously.\n\n# Extract mean heatwave proportions for each DHS cluster region\nchirts_clust &lt;- extract(\n  ml_chirts_heatwave, \n  ml_clust_buffer, \n  fun = mean,\n  weights = TRUE\n) |&gt; \n  as_tibble()\n\nchirts_clust\n#&gt; # A tibble: 413 × 73\n#&gt;       ID ml_chirts_heatwave_1 ml_chirts_heatwave_2 ml_chirts_heatwave_3\n#&gt;    &lt;int&gt;                &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n#&gt;  1     1                0.210                0.498                0.806\n#&gt;  2     2                0.226                0.714                0.903\n#&gt;  3     3                0.270                0.910                1    \n#&gt;  4     4                0.226                0.857                0.968\n#&gt;  5     5                0.258                0.898                0.996\n#&gt;  6     6                0.226                0.841                0.935\n#&gt;  7     7                0.169                0.679                0.903\n#&gt;  8     8                0.161                0.5                  0.892\n#&gt;  9     9                0.226                0.858                0.973\n#&gt; 10    10                0.194                0.627                0.924\n#&gt; # ℹ 403 more rows\n#&gt; # ℹ 69 more variables: ml_chirts_heatwave_4 &lt;dbl&gt;, ml_chirts_heatwave_5 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_6 &lt;dbl&gt;, ml_chirts_heatwave_7 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_8 &lt;dbl&gt;, ml_chirts_heatwave_9 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_10 &lt;dbl&gt;, ml_chirts_heatwave_11 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_12 &lt;dbl&gt;, ml_chirts_heatwave_13 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_14 &lt;dbl&gt;, ml_chirts_heatwave_15 &lt;dbl&gt;, …\n\n\nWide vs. long format\nextract() provides data in wide format, where each column represents our temperature values (in this case, mean proportion of heatwave days in a given month) and each row represents a DHS enumeration cluster.\nTo join on our DHS survey data, we’ll want our data to be in long format, where each row represents a single month of temperature data for a single DHS cluster. We can accomplish this conversion with the pivot_longer() function from the {tidyr} package.\n\n\n    © RStudio, Inc. (MIT) \nFirst, we’ll rename the columns in our extracted data. Currently, they’re listed in incremental order, which isn’t very intuitive. Instead, we’ll rename them using the CMC code of the month that each column represents. We can use our helper function to convert these dates to CMC format.\nWe’ll also update the \"ID\" column to be named \"DHSID\" for consistency with the name used in the DHS survey to represent the enumeration cluster ID:\n\n# Sequence of months for the time range in our data\nyear_months &lt;- seq(\n  lubridate::ym(\"2007-01\"), \n  lubridate::ym(\"2012-12\"), \n  by = \"months\"\n)\n\n# Change layer names\nnames(chirts_clust) &lt;- c(\"DHSID\", ym_to_cmc(year_months))\n\nNext, we need to convert the incremental ID numbers for each cluster to their corresponding DHSID code. The ID values in chirts_clust represent the clusters extracted from ml_clust in index order. Thus, we simply need to reassign the incremental ID codes in chirts_clust with the IDs in ml_clust.\n\n# Convert index numbers to corresponding DHSID codes\nchirts_clust$DHSID &lt;- ml_clust$DHSID\n\nchirts_clust\n#&gt; # A tibble: 413 × 73\n#&gt;    DHSID  `1285` `1286` `1287` `1288` `1289` `1290`  `1291` `1292` `1293` `1294`\n#&gt;    &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 ML201…  0.210  0.498  0.806  1      1      0.935 0.358        0 0.318   1    \n#&gt;  2 ML201…  0.226  0.714  0.903  1      0.999  0.825 0.346        0 0.0355  0.857\n#&gt;  3 ML201…  0.270  0.910  1      0.999  1      0.604 0            0 0       0.150\n#&gt;  4 ML201…  0.226  0.857  0.968  1      1      0.718 0.186        0 0       0.159\n#&gt;  5 ML201…  0.258  0.898  0.996  0.988  1      0.599 0            0 0       0.141\n#&gt;  6 ML201…  0.226  0.841  0.935  1      1      0.572 0            0 0       0.373\n#&gt;  7 ML201…  0.169  0.679  0.903  1      1      0.933 0.305        0 0.427   0.778\n#&gt;  8 ML201…  0.161  0.5    0.892  1      1      0.967 0.581        0 0.5     0.901\n#&gt;  9 ML201…  0.226  0.858  0.973  0.991  1      0.569 0.00296      0 0       0.144\n#&gt; 10 ML201…  0.194  0.627  0.924  1      1      1.00  0.572        0 0.385   0.957\n#&gt; # ℹ 403 more rows\n#&gt; # ℹ 62 more variables: `1295` &lt;dbl&gt;, `1296` &lt;dbl&gt;, `1297` &lt;dbl&gt;, `1298` &lt;dbl&gt;,\n#&gt; #   `1299` &lt;dbl&gt;, `1300` &lt;dbl&gt;, `1301` &lt;dbl&gt;, `1302` &lt;dbl&gt;, `1303` &lt;dbl&gt;,\n#&gt; #   `1304` &lt;dbl&gt;, `1305` &lt;dbl&gt;, `1306` &lt;dbl&gt;, `1307` &lt;dbl&gt;, `1308` &lt;dbl&gt;,\n#&gt; #   `1309` &lt;dbl&gt;, `1310` &lt;dbl&gt;, `1311` &lt;dbl&gt;, `1312` &lt;dbl&gt;, `1313` &lt;dbl&gt;,\n#&gt; #   `1314` &lt;dbl&gt;, `1315` &lt;dbl&gt;, `1316` &lt;dbl&gt;, `1317` &lt;dbl&gt;, `1318` &lt;dbl&gt;,\n#&gt; #   `1319` &lt;dbl&gt;, `1320` &lt;dbl&gt;, `1321` &lt;dbl&gt;, `1322` &lt;dbl&gt;, `1323` &lt;dbl&gt;, …\n\nNow we’re ready to convert to long format data. pivot_longer() will convert a set of columns in our wide data to two columns in our long data. The names of these columns will be stored in one of the output columns, and their associated values will be stored in the other.\nBelow, we indicate that we want to pivot all columns except the DHSID column (the data are already long on DHSID). We also indicate that we want the new column of names to be called \"CHIRTSCMC\" and the new column of values to be called \"PROPHEATWAVE\". Finally, we use the names_transform argument to convert all the names to numeric format, so they will be interpretable as CMCs.\n\n# Convert to long format for each cluster/month combination\nchirts_clust &lt;- pivot_longer(\n  chirts_clust,\n  cols = -DHSID,\n  names_to = \"CHIRTSCMC\",\n  values_to = \"PROPHEATWAVE\",\n  names_transform = as.numeric\n)\n\nchirts_clust\n#&gt; # A tibble: 29,736 × 3\n#&gt;    DHSID          CHIRTSCMC PROPHEATWAVE\n#&gt;    &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 ML201200000001      1285        0.210\n#&gt;  2 ML201200000001      1286        0.498\n#&gt;  3 ML201200000001      1287        0.806\n#&gt;  4 ML201200000001      1288        1    \n#&gt;  5 ML201200000001      1289        1    \n#&gt;  6 ML201200000001      1290        0.935\n#&gt;  7 ML201200000001      1291        0.358\n#&gt;  8 ML201200000001      1292        0    \n#&gt;  9 ML201200000001      1293        0.318\n#&gt; 10 ML201200000001      1294        1    \n#&gt; # ℹ 29,726 more rows\n\nAs you can see, each row now corresponds to a cluster/month combination, and the corresponding heatwave value is stored in the PROPHEATWAVE column."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#joining-on-dhs-survey-data",
    "href": "posts/2024-05-31-ts-join/index.html#joining-on-dhs-survey-data",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Joining on DHS survey data",
    "text": "Joining on DHS survey data\nBefore we join our heatwave data on our DHS survey data, we need to do some housekeeping. Some of the variables we want to include in our model include missing values or implied decimals, which we’ll want to clean up before we try to join the DHS survey with our CHIRTS data.\n\nDHS survey data preparation\nFirst, we’ll make a new column that stores a unique ID for each child in the sample by combining the ID for each woman and the index for each of her births. This will make it easier to ensure that we join data independently for each child in the sample:\n\n# str_squish() removes excess whitespace in the ID strings\nml_dhs &lt;- ml_dhs |&gt; \n  mutate(KIDID = stringr::str_squish(paste(IDHSPID, BIDX)))\n\nWe’ll also want to make sure to recode several of our variables that include missing values. We can check the missing value codes for a variable using ipums_val_labels(). For instance, for our key outcome variable, BIRTHWT, we see that all values over 9996 are missing in some way:\n\nipums_val_labels(ml_dhs$BIRTHWT)\n#&gt; # A tibble: 5 × 2\n#&gt;     val lbl                  \n#&gt;   &lt;int&gt; &lt;chr&gt;                \n#&gt; 1  9995 9995+                \n#&gt; 2  9996 Not weighed at birth \n#&gt; 3  9997 Don't know           \n#&gt; 4  9998 Missing              \n#&gt; 5  9999 NIU (not in universe)\n\nLooking at these values, you may be surprised to see 4-digit weights. To investigate further, we can display detailed variable information with ipums_var_desc():\n\nipums_var_desc(ml_dhs$BIRTHWT)\n#&gt; [1] \"For children born in the three to five years before the survey, BIRTHWT (M19) reports the child's birthweight in kilos with three implied decimal places (or, alternatively stated, in grams with no decimal places). Children who were not weighed are coded 9996.\"\n\nNote that the description mentions that there are 3 implied decimal places. So, we’ll recode BIRTHWT to be NA in the cases where the value is more than 9996, and we’ll divide by 1000 otherwise. We’ll do a similar process with a few other variables as well:\n\nml_dhs &lt;- ml_dhs |&gt; \n  mutate(\n    BIRTHWT = if_else(BIRTHWT &gt;= 9996, NA, BIRTHWT / 1000),\n    HEIGHTFEM = if_else(HEIGHTFEM &gt;= 9994, NA, HEIGHTFEM / 10),\n    EDUCLVL = if_else(EDUCLVL == 8, NA, EDUCLVL),\n    BIRTHWTREF = if_else(BIRTHWTREF &gt;= 7, NA, BIRTHWTREF)\n  )\n\n\n\n\n\n\n\nCensoring\n\n\n\nIn practice, we would likely want to deal with BIRTHWT values of 9995+, which represent all weights above the 9.995 kg threshold. It’s not fully correct to treat these values as equal to 9.995, since their values are actually unknown. This problem is known as censoring, but we won’t address it in this post to keep this demonstration focused on our core goal of integrating environmental data with DHS surveys.\n\n\n\nResidency\nWhen working with environmental data, we want to be attentive to the members of the sample who may have recently moved to a location, as recent arrivals likely didn’t experience the previous environmental effects recorded for the area they currently live!\nIPUMS DHS includes the RESIDEINTYR variable to indicate how long survey respondents have lived at their current location.\nUnfortunately, this variable wasn’t collected for the Mali 2012 sample. However, we do have a record of whether a respondent is a visitor to the location or not. At a minimum, we’ll remove records for those who are listed as visitors, since we can’t be confident that they experienced the environmental data for the cluster their response was recorded in.\n\nml_dhs &lt;- ml_dhs |&gt; \n  filter(RESIDENT == 1)\n\n\n\nData types\nFinally, we want to recode our variables to the appropriate data type. In our case, we just need to convert all labeled variables to factors. BIDX is unlabeled, but represents a factor, so we’ll explicitly convert that variable as well:\n\nml_dhs &lt;- ml_dhs |&gt; \n  mutate(BIDX = as_factor(BIDX)) |&gt; \n  as_factor() # Convert all remaining labeled columns to factors\n\n\n\n\nAttaching data sources\nAt its simplest, we could join our prepared DHS survey data with our extracted heatwave data by matching records based on their cluster and birth month. We can use left_join() from {dplyr} to do so. This will retain all records in our DHS survey sample and attach temperature values where the birth date (KIDDOBCMC) is equal to the month of temperature data in chirts_clust:\n\n# Join temperature exposure for each child during the month of their birth\nleft_join(\n  ml_dhs,\n  chirts_clust,\n  by = c(\"DHSID\", \"KIDDOBCMC\" = \"CHIRTSCMC\")\n)\n\nHowever, this only attaches a single month of temperature data for each child. What we’d rather have is a time series of values covering the time between conception and birth.\nTo accomplish this, we can define a new variable, KIDCONCEPTCMC, which contains the CMC of the month 9 months before the birth date.\n\n# Make unique ID for each kid using woman's ID and Birth index\nml_dhs &lt;- ml_dhs |&gt;  \n  mutate(KIDCONCEPTCMC = KIDDOBCMC - 9)\n\nNow, we want to join all 9 of the relevant monthly temperature records to each child’s DHS survey record. That is, we want to match records that have the same DHSID, a birthdate that is after the CHIRTS data that will be joined, and a conception date that is before the CHIRTS data that will be joined. We can use join_by() from {dplyr} to specify these more complex criteria:\n\n# Join 9 months of CHIRTS data to each child record\nml_dhs_chirts &lt;- left_join(\n  ml_dhs,\n  chirts_clust,\n  by = join_by(\n    DHSID == DHSID, # Cluster ID needs to match\n    KIDDOBCMC &gt; CHIRTSCMC, # DOB needs to be after all joined temp. data\n    KIDCONCEPTCMC &lt;= CHIRTSCMC # Conception date needs to be before all joined temp. data\n  )\n)\n\nWe should end up with a dataset that contains 9 records for each child: one for each month between their conception and birth. We can confirm by counting the records for each KIDID:\n\n# Each kid should have 9 rows now\nml_dhs_chirts |&gt; \n  count(KIDID)\n#&gt; # A tibble: 10,300 × 2\n#&gt;    KIDID               n\n#&gt;    &lt;chr&gt;           &lt;int&gt;\n#&gt;  1 46605 1 14 2 1      9\n#&gt;  2 46605 1 16 2 1      9\n#&gt;  3 46605 1 16 5 1      9\n#&gt;  4 46605 1 16 5 2      9\n#&gt;  5 46605 1 4 2 1       9\n#&gt;  6 46605 1 42 2 1      9\n#&gt;  7 46605 1 42 2 2      9\n#&gt;  8 46605 1 50 2 1      9\n#&gt;  9 46605 1 75 12 1     9\n#&gt; 10 46605 1 75 12 2     9\n#&gt; # ℹ 10,290 more rows\n\nWe can pull out a few columns for an individual child to make it more clear how things are being joined:\n\n# Example of what our data look like for one kid:\nml_dhs_chirts |&gt; \n  filter(KIDID == \"46605 1 4 2 1\") |&gt; \n  select(KIDID, DHSID, KIDDOBCMC, CHIRTSCMC, PROPHEATWAVE)\n#&gt; # A tibble: 9 × 5\n#&gt;   KIDID         DHSID          KIDDOBCMC CHIRTSCMC PROPHEATWAVE\n#&gt;   &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 46605 1 4 2 1 ML201200000001      1311      1302       0.822 \n#&gt; 2 46605 1 4 2 1 ML201200000001      1311      1303       0.103 \n#&gt; 3 46605 1 4 2 1 ML201200000001      1311      1304       0     \n#&gt; 4 46605 1 4 2 1 ML201200000001      1311      1305       0.244 \n#&gt; 5 46605 1 4 2 1 ML201200000001      1311      1306       0.787 \n#&gt; 6 46605 1 4 2 1 ML201200000001      1311      1307       0.624 \n#&gt; 7 46605 1 4 2 1 ML201200000001      1311      1308       0     \n#&gt; 8 46605 1 4 2 1 ML201200000001      1311      1309       0.0968\n#&gt; 9 46605 1 4 2 1 ML201200000001      1311      1310       0.741\n\nAs we can see, an individual child (KIDID) is associated with heatwave data (PROPHEATWAVE) for each of the 9 CHIRTS months (CHIRTSCMC) prior to their birth date (KIDDOBCMC).\nFor a different child we’ll get a similar output, but because this child has a different birth date, the specific PROPHEATWAVE values will be different, even if the cluster ID is the same:\n\nml_dhs_chirts |&gt; \n  filter(KIDID == \"46605 1255 2 2\") |&gt; \n  select(KIDID, DHSID, KIDDOBCMC, CHIRTSCMC, PROPHEATWAVE)\n#&gt; # A tibble: 9 × 5\n#&gt;   KIDID          DHSID          KIDDOBCMC CHIRTSCMC PROPHEATWAVE\n#&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 46605 1255 2 2 ML201200000001      1305      1296        0    \n#&gt; 2 46605 1255 2 2 ML201200000001      1305      1297        0    \n#&gt; 3 46605 1255 2 2 ML201200000001      1305      1298        0.345\n#&gt; 4 46605 1255 2 2 ML201200000001      1305      1299        1    \n#&gt; 5 46605 1255 2 2 ML201200000001      1305      1300        0.999\n#&gt; 6 46605 1255 2 2 ML201200000001      1305      1301        1    \n#&gt; 7 46605 1255 2 2 ML201200000001      1305      1302        0.822\n#&gt; 8 46605 1255 2 2 ML201200000001      1305      1303        0.103\n#&gt; 9 46605 1255 2 2 ML201200000001      1305      1304        0\n\nSimilarly, for a child from a different cluster, we’ll have different PROPHEATWAVE values regardless of which month is under consideration:\n\nml_dhs_chirts |&gt; \n  filter(KIDID == \"46605 518171 3 1\") |&gt; \n  select(KIDID, DHSID, KIDDOBCMC, CHIRTSCMC, PROPHEATWAVE)\n#&gt; # A tibble: 9 × 5\n#&gt;   KIDID            DHSID          KIDDOBCMC CHIRTSCMC PROPHEATWAVE\n#&gt;   &lt;chr&gt;            &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 46605 518171 3 1 ML201200000518      1314      1305       0.379 \n#&gt; 2 46605 518171 3 1 ML201200000518      1314      1306       0.877 \n#&gt; 3 46605 518171 3 1 ML201200000518      1314      1307       0.456 \n#&gt; 4 46605 518171 3 1 ML201200000518      1314      1308       0     \n#&gt; 5 46605 518171 3 1 ML201200000518      1314      1309       0.0213\n#&gt; 6 46605 518171 3 1 ML201200000518      1314      1310       0.505 \n#&gt; 7 46605 518171 3 1 ML201200000518      1314      1311       0.923 \n#&gt; 8 46605 518171 3 1 ML201200000518      1314      1312       1     \n#&gt; 9 46605 518171 3 1 ML201200000518      1314      1313       1\n\nNow that we have the correct 9-month time series for each child, we can further aggregate to get a trimester-specific exposure metric for each child.\nFirst, we’ll create a TRIMESTER variable, which encodes the trimester that each month belongs to for each child:\n\nml_dhs_chirts &lt;- ml_dhs_chirts |&gt; \n  arrange(DHSID, KIDID, CHIRTSCMC) |&gt; # order by CMC to ensure trimesters are in correct order\n  mutate(TRIMESTER = as_factor(rep(c(1, 2, 3), each = 3)), .by = KIDID)\n\nThen, we’ll average the heatwave proportions across trimesters for each child:\n\n# Average proportion of heatwave days within each child and trimester\nml_dhs_tri &lt;- ml_dhs_chirts |&gt; \n  summarize(\n    MEANPROPHEATWAVE = mean(PROPHEATWAVE), \n    .by = c(KIDID, TRIMESTER)\n  )\n\nml_dhs_tri\n#&gt; # A tibble: 30,900 × 3\n#&gt;    KIDID          TRIMESTER MEANPROPHEATWAVE\n#&gt;    &lt;chr&gt;          &lt;fct&gt;                &lt;dbl&gt;\n#&gt;  1 46605 1 14 2 1 1                    0.439\n#&gt;  2 46605 1 14 2 1 2                    0.218\n#&gt;  3 46605 1 14 2 1 3                    0.781\n#&gt;  4 46605 1 16 2 1 1                    0.240\n#&gt;  5 46605 1 16 2 1 2                    0.903\n#&gt;  6 46605 1 16 2 1 3                    0.731\n#&gt;  7 46605 1 16 5 1 1                    0.944\n#&gt;  8 46605 1 16 5 1 2                    0.322\n#&gt;  9 46605 1 16 5 1 3                    0.578\n#&gt; 10 46605 1 16 5 2 1                    0.115\n#&gt; # ℹ 30,890 more rows\n\nWe now have 3 records for each child—one for each trimester. Our temperature metric now represents the average monthly proportion of days spent in a heatwave across the three months of each trimester as well as across the spatial region of each buffered cluster region.\nRecall that in this case we define a heatwave day as any day belonging to a sequence of at least 3 days over 35°C."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#getting-help",
    "href": "posts/2024-05-31-ts-join/index.html#getting-help",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-07-02-ndvi-concepts/index.html",
    "href": "posts/2024-07-02-ndvi-concepts/index.html",
    "title": "Introducing NDVI as a Tool for Population Health Research",
    "section": "",
    "text": "Long term shifts in weather patterns have created both chronic and acute changes in growing conditions in many parts of the world. By altering food production and access, these shifts can produce long-term impacts on the lives, biology, and behavior of the subsistence farmers, agriculturalists, and pastoralist residents who rely on specific environmental conditions to support their livelihoods.\nThese effects are especially pronounced in low-income countries where extreme weather may have an outsize impact. Beyond merely the choices of when to plant, water, and harvest, research consistently demonstrates that growing conditions—and by extension, food access and availability—play a key role in migration decisions, livelihood selection and diversification, health outcomes, resource related conflict, and the choice of whether, when and how often to have children. Because of this, data on immediate and longitudinal growing conditions are at the heart of scientists’ understanding of the link between weather extremes and health, particularly among the most vulnerable and poorest regions and populations."
  },
  {
    "objectID": "posts/2024-07-02-ndvi-concepts/index.html#getting-help",
    "href": "posts/2024-07-02-ndvi-concepts/index.html#getting-help",
    "title": "Introducing NDVI as a Tool for Population Health Research",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html",
    "href": "posts/2024-08-16-ndvi-data-2/index.html",
    "title": "Aggregation Methods for NDVI Data",
    "section": "",
    "text": "Update\n\n\n\nSince we originally released this post, we now recommend using VIIRS when doing contemporary NDVI research. See our VIIRS post for more information about how to adapt the contents discussed in this post for use with VIIRS files.\nPreviously, we showed how to load NDVI data for two time points in Kenya using NASA’s Earthdata Search interface. If you’re new to NDVI, we suggest you go ahead and take a look at our introductory post as well as our NDVI data loading post before continuing.\nIn this post, we’ll use two NDVI time series to demonstrate some of the considerations that emerge when aggregating and processing NDVI raster data in the context of population health research. We need to conduct many of the same aggregation steps for our NDVI data as we did for previous data sources, like CHIRPS and CHIRTS. However, as with any new data source, the decisions we make about how to do so will vary when working with NDVI.\nTo highlight some of the features of NDVI data, we’ll load NDVI time series from two capitals with different climates: Nairobi, Kenya and Ouagadougou, Burkina Faso. We’ll use a familiar workflow to build buffers around the DHS cluster points in these regions and will extract NDVI time series for each. Once we have these time series, we’ll use them to explore how extreme weather and seasonality may play a role in the way we aggregate NDVI data.\nAs always, we’ll start by loading some of the packages used in the post:\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggspatial)\nlibrary(patchwork)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#obtaining-data-from-nasa",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#obtaining-data-from-nasa",
    "title": "Aggregation Methods for NDVI Data",
    "section": "Obtaining data from NASA",
    "text": "Obtaining data from NASA\nTo simplify things, we’ve gone ahead and prepared these NDVI time series as we described in our previous post and saved them as stand-alone .tif files. We won’t show the step-by-step process for producing these files, but they track closely with the methods shown in our previous post, so take a look there if you need a refresher on how to ingest MODIS NDVI data into R.\nSince our data are stored in .tif files, we can load them directly with {terra}:\n\n# Load NDVI time series\n# Nairobi region\nndvi_nairobi &lt;- rast(\"data_local/ndvi_nairobi.tif\")\n\n# Ouagadougou region\nndvi_ouaga &lt;- rast(\"data_local/ndvi_ouaga.tif\")"
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#dhs-cluster-coordinates",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#dhs-cluster-coordinates",
    "title": "Aggregation Methods for NDVI Data",
    "section": "DHS cluster coordinates",
    "text": "DHS cluster coordinates\nIn this post, we’ll be aggregating our NDVI raster data to the DHS cluster level as we’ve demonstrated before. To do so, we’ll need the cluster coordinate data provided by the DHS. We can use {sf} to load the DHS cluster coordinates once we’ve downloaded the shapefiles provided by the DHS Program.\n\n\nIf you need a refresher on where you can access cluster coordinate data, see our CHIRPS post.\n\n# Load DHS cluster coordinates\n# KE 2014 coordinates\nke_gps &lt;- st_read(\"data/KEGE71FL/KEGE71FL.shp\", quiet = TRUE)\n\n# BF 2010 coordinates\nbf_gps &lt;- st_read(\"data/BFGE61FL/BFGE61FL.shp\", quiet = TRUE)\n\nSince we’re only working with the two capital regions for this demonstration, we’ll filter our cluster coordinate data to remove records for clusters that are outside each region of interest.\n\n# Filter Kenya cluster locations to those in the Nairobi region\nnairobi_gps &lt;- ke_gps |&gt;\n  filter(DHSREGNA == \"Nairobi\")\n\nFor Burkina Faso, we’ll also need to remove some clusters that do not have valid coordinate locations, so we’ll filter out cases with (0, 0) coordinates:\n\n# Filter Burkina Faso cluster locations to those in the Ouagadougou region\nouaga_gps &lt;- bf_gps |&gt;\n  filter(\n    !(LATNUM == 0 & LONGNUM == 0), # Remove missing coordinate locations\n    DHSREGNA == \"Centre\"\n  )\n\nEven though we’re working with urban areas, our DHS clusters are still located across a range of NDVI values. Here we see our cluster point locations overlaid on the NDVI raster for the region:\n\n\nShow plot code\n# We have converted our NDVI palette from the previous post to a scale function\n# for easier use in this post\nndvi_pal &lt;- function() {\n  list(\n    pal = c(\n      \"#fdfbdc\",\n      \"#f1f4b7\",\n      \"#d3ef9f\",\n      \"#a5da8d\",\n      \"#6cc275\",\n      \"#51a55b\",\n      \"#397e43\",\n      \"#2d673a\",\n      \"#1d472e\"\n    ),\n    values = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1)\n  )\n}\n\nscale_fill_ndvi &lt;- function(pal = ndvi_pal(), ...) {\n  scale_fill_gradientn(colors = pal$pal, values = pal$values, ...)\n}\n\nscale_color_ndvi &lt;- function(pal = ndvi_pal(), ...) {\n  scale_color_gradientn(colors = pal$pal, values = pal$values, ...)\n}\n\nggplot() +\n  layer_spatial(ndvi_nairobi[[3]]) +\n  # layer_spatial(nairobi_gps, color = \"white\", alpha = 1) +\n  layer_spatial(nairobi_gps, color = \"black\", fill = \"white\", alpha = 0.9, shape = 21, size = 1.5, stroke = 1) +\n  scale_fill_ndvi(limits = c(0, 1), na.value = \"transparent\") +\n  labs(\n    title = \"2014 DHS Cluster Coordinates: Nairobi\",\n    subtitle = \"With NDVI from February 2-18, 2003\",\n    fill = \"NDVI\"\n  ) +\n  theme_dhs_map()"
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#buffer-cluster-coordinates",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#buffer-cluster-coordinates",
    "title": "Aggregation Methods for NDVI Data",
    "section": "Buffer cluster coordinates",
    "text": "Buffer cluster coordinates\nAs we’ve shown before, the next step is to create a buffer around each cluster point location to get a polygon containing the general region around each cluster.\nWe’ll first project our cluster points and then create a buffer with st_buffer(). Then, we’ll project our buffered points to the same coordinate reference system as our NDVI data to prepare to aggregate our NDVI data to our newly-created buffer regions.\n\n# Project and buffer Nairobi clusters\nnairobi_gps_buff &lt;- nairobi_gps |&gt;\n  st_transform(crs = 32637) |&gt; # UTM Zone 37N\n  st_buffer(dist = 5000) |&gt;\n  st_transform(crs = crs(ndvi_nairobi))\n\n# Project and buffer Ouagadougou clusters\nouaga_gps_buff &lt;- ouaga_gps |&gt;\n  st_transform(crs = 32630) |&gt; # UTM Zone 30N\n  st_buffer(dist = 5000) |&gt;\n  st_transform(crs = crs(ndvi_ouaga))"
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#basic-approach",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#basic-approach",
    "title": "Aggregation Methods for NDVI Data",
    "section": "Basic approach",
    "text": "Basic approach\nPreviously, we introduced terra’s extract(), which will allow us to use the NDVI raster along with our buffered cluster polygons to spatially aggregate the NDVI pixels within each cluster region. For instance, to extract the average NDVI value within each buffer, we could use fun = mean as shown below (note that we’ve set na.rm = TRUE to exclude missing values from the calculation):\n\n# Extract mean NDVI in each cluster buffer region:\nke_mean_ndvi &lt;- extract(\n  ndvi_nairobi,\n  nairobi_gps_buff,\n  weights = TRUE,\n  fun = mean,\n  na.rm = TRUE # Exclude missing raster values in average\n)\n\nke_mean_ndvi\n#&gt;    ID \"250m 16 days NDVI\" \"250m 16 days NDVI\" \"250m 16 days NDVI\"\n#&gt; 1   1           0.6577829           0.6587233           0.5955906\n#&gt; 2   2           0.6517902           0.6471524           0.5866545\n#&gt; 3   3           0.6726900           0.6747525           0.6094023\n#&gt; 4   4           0.7095437           0.7192108           0.6485346\n....\n\nThis gives us a tabular record where each row corresponds to a cluster and each column to the mean NDVI value in that cluster buffer region for a particular time point.\nHowever, our interest in NDVI is primarily as a measure for agricultural production, and some areas aren’t intended to be used for agriculture (like water bodies or dense urban environments). Including these low-NDVI pixels in our aggregation serves to reduce the mean NDVI value in a cluster, even if the available agricultural land in that cluster is highly productive.\nIf we want an estimate of the agricultural productivity of the vegetated land in an area, we likely want to remove very low NDVI values (which typically represent impervious surfaces and water) from our calculation. Below, we’ll detail a few approaches that could accomplish this goal."
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#idea-1-mask-out-sub-zero-ndvi-values",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#idea-1-mask-out-sub-zero-ndvi-values",
    "title": "Aggregation Methods for NDVI Data",
    "section": "Idea 1: Mask out sub-zero NDVI values",
    "text": "Idea 1: Mask out sub-zero NDVI values\nRecall that NDVI can range from -1 to 1, where values near and below 0 typically represent water or bare soil. Some of these low NDVI values in our raster are already treated as NA. Using our approach from above (with na.rm = TRUE), these are removed from consideration during aggregation. However, pixels that border these regions may still have low NDVI values. We can set these to NA manually using terra’s classify().\nclassify() takes an input matrix that defines the range of values in our raster that should be reclassified. In this case, we create a matrix called subzero_to_na that defines the range of values from -Inf to 0. The third value in this matrix indicates the output value for all raster cells that fall within this range.\n\nsubzero_to_na &lt;- matrix(c(-Inf, 0, NA), nrow = 1)\n\nsubzero_to_na\n#&gt;      [,1] [,2] [,3]\n#&gt; [1,] -Inf    0   NA\n\nWe can use this matrix with our NDVI raster to reclassify sub-zero NDVI values to NA.\n\nndvi_nairobi &lt;- classify(ndvi_nairobi, subzero_to_na)\n\nNow, as long as we continue to use na.rm = TRUE in our call to extract(), we’ll automatically ignore these low NDVI values.\n\nke_mean_ndvi &lt;- extract(\n  ndvi_nairobi,\n  nairobi_gps_buff,\n  weights = TRUE,\n  fun = mean,\n  na.rm = TRUE # Exclude missing raster values in average\n)"
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#idea-2-use-maximum-ndvi",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#idea-2-use-maximum-ndvi",
    "title": "Aggregation Methods for NDVI Data",
    "section": "Idea 2: Use maximum NDVI",
    "text": "Idea 2: Use maximum NDVI\nCrops are often grown in localized areas, so even if a large portion of a cluster region contains low vegetation values, the crop-producing regions may still be thriving.\nFor instance, let’s take an example cluster from the Ouagadougou area. If we plot the NDVI values in its vicinity over the course of the year, we notice a few obvious patterns.\n\n\nShow plot code\nlibrary(patchwork)\n\nx &lt;- ouaga_gps_buff[8, ]\nndvi_clust &lt;- crop(ndvi_ouaga, x)\n\npanels &lt;- purrr::map(\n  1:23,\n  function(i) {\n    ggplot() +\n      layer_spatial(mask(ndvi_clust[[i]], x, inverse = TRUE), alpha = 0.2) +\n      layer_spatial(mask(ndvi_clust[[i]], x)) +\n      layer_spatial(x, fill = NA, color = \"black\") +\n      scale_fill_ndvi(\n        limits = c(0, 1),\n        na.value = \"transparent\"\n      ) +\n      labs(fill = \"NDVI\")\n  }\n)\n\npurrr::reduce(panels, `+`) +\n  plot_annotation(title = \"2003 NDVI\", subtitle = \"Single Cluster: 2010 Burkina Faso Sample\") +\n  plot_layout(guides = \"collect\")  &\n  theme_void() +\n  theme_dhs_map() +\n  theme(\n    legend.position = \"bottom\",\n    legend.title.position = \"top\",\n    legend.title = element_text(size = 10, hjust = 0.5),\n    legend.key.height = unit(7, \"points\"),\n    legend.key.width = unit(45, \"points\"),\n    legend.ticks = element_line(color = \"white\", linewidth = 0.2),\n    legend.ticks.length = unit(1, \"points\"),\n    legend.frame = element_rect(\n      fill = NA,\n      color = \"#999999\",\n      linewidth = 0.2\n    ),\n    plot.title = element_text(\n      hjust = 0,\n      size = 18, \n      color = \"#00263A\", # IPUMS navy\n      margin = margin(b = 7)\n    ), \n    plot.subtitle = element_text(\n      size = 12, \n      hjust = 0,\n      color = \"#00000099\",\n      margin = margin(b = 10)\n    ),\n    plot.caption = element_text(\n      size = 10,\n      hjust = 1,\n      color = \"#00000099\",\n      margin = margin(t = 5)\n    ),\n    text = element_text(family = \"cabrito\", size = 10), \n  )\n\n\n\n\n\n\n\n\n\nFirst, we notice that the overall region has a period of higher vegetation in the middle of the year. We also notice that there is a small area in the north of the cluster region that has consistently higher vegetation than its surrounding area throughout the year, even in times with less vegetation overall.\nThis could be a result of water proximity (we notice that some missing values appear near the green patch, which could have been removed because they represent a water source), but it could also be because of human interaction, like irrigation.\nThis human element is an important feature of NDVI that distinguishes it from other environmental metrics, like precipitation and temperature. Vegetation is more directly influenced by human behavior than these other sources.\nWhether this particular pattern is prompted primarily by physical geography (a nearby water source) or human behavior (irrigation), it does serve as an example of why calculating a mean NDVI value may not reflect the lived experience on the ground. If most of an area’s crops are grown in a particular area, the most critical portion of a cluster region may have very high vegetation values, even if the rest of the cluster does not.\nOne option to incorporate this into our data processing would be to use the maximum NDVI value as this cluster’s value rather than the mean. This is easily done in R: we just need to change our aggregation function from fun = mean to fun = max:\n\n# Reclassify low NDVI values\nndvi_ouaga &lt;- classify(ndvi_ouaga, subzero_to_na)\n\nbf_max_ndvi &lt;- extract(\n  ndvi_ouaga,\n  ouaga_gps_buff,\n  weights = TRUE,\n  fun = max, # Extract max value within cluster, not mean\n  na.rm = TRUE\n)\n\nThis gives us a similarly formatted output, but each value now represents the maximum NDVI pixel value in a given cluster (row) for a given time point (column):\n\nbf_max_ndvi\n#&gt;    ID \"250m 16 days NDVI\" \"250m 16 days NDVI\" \"250m 16 days NDVI\"\n#&gt; 1   1              0.4393              0.3828              0.3759\n#&gt; 2   2              0.4320              0.4590              0.3903\n#&gt; 3   3              0.6664              0.4498              0.4392\n#&gt; 4   4              0.4393              0.3828              0.3759\n....\n\n\nComparing maximum and mean aggregation\nUsing the maximum cluster region NDVI value (rather than the mean) will obviously provide a different NDVI estimate for each cluster, but how much of a difference does this decision make overall?\nFirst, let’s extract the mean NDVI values for our Ouagadougou clusters as we did earlier for Nairobi:\n\nbf_mean_ndvi &lt;- extract(\n  ndvi_ouaga,\n  ouaga_gps_buff,\n  weights = TRUE,\n  fun = mean,\n  na.rm = TRUE\n)\n\nNow we can compare our maximum and mean NDVI values for each cluster. In the plot below, each line corresponds to a DHS cluster, with the lower points showing the mean NDVI value for that cluster and the higher points representing the max NDVI value for that cluster (for a single date):\n\n\nShow plot code\ncolnames(bf_max_ndvi) &lt;- c(\"ID\", as.character(time(ndvi_ouaga)))\ncolnames(bf_mean_ndvi) &lt;- c(\"ID\", as.character(time(ndvi_ouaga)))\n\nbf_max_ndvi &lt;- bf_max_ndvi |&gt;\n  tidyr::pivot_longer(-ID) |&gt;\n  mutate(name = as.Date(name)) |&gt;\n  rename(time = name)\n\nbf_mean_ndvi &lt;- bf_mean_ndvi |&gt;\n  tidyr::pivot_longer(-ID) |&gt;\n  mutate(name = as.Date(name)) |&gt;\n  rename(time = name)\n\nfull_join(bf_max_ndvi, bf_mean_ndvi, by = c(\"ID\", \"time\")) |&gt;\n  mutate(d = value.x - value.y) |&gt;\n  filter(time == \"2003-01-01\") |&gt;\n  ggplot() +\n  geom_segment(aes(x = reorder(ID, d), y = value.x, yend = value.y), linewidth = 0.5, color = \"gray20\", alpha = 0.8) +\n  geom_point(aes(x = reorder(ID, d), y = value.x, fill = value.x), size = 2.5, shape = 21) +\n  geom_point(aes(x = reorder(ID, d), y = value.y, fill = value.y), size = 2.5, shape = 21) +\n  annotate(\"segment\", x = 33.6, xend = 27.8, y = 0.68, yend = 0.88, color = \"gray80\") +\n  annotate(\"segment\", x = 19.2, xend = 24, y = 0.45, yend = 0.88, color = \"gray80\") +\n  geom_label(\n    x = 26,\n    y = 0.89,\n    label = \"Cluster Max NDVI\",\n    color = \"gray60\",\n    label.size = 0,\n    label.padding = unit(0.35, \"lines\"),\n    label.r = unit(0.5, \"lines\"),\n    alpha = 0.3,\n    size = 4,\n    family = \"cabrito\"\n  ) +\n  annotate(\"segment\", x = 9, xend = 7.05, y = 0.06, yend = 0.18, color = \"gray80\") +\n  annotate(\"segment\", x = 14.7, xend = 20.85, y = 0.06, yend = 0.135, color = \"gray80\") +\n  geom_label(\n    x = 12,\n    y = 0.04,\n    label = \"Cluster Mean NDVI\",\n    color = \"gray60\",\n    label.size = 0,\n    label.padding = unit(0.35, \"lines\"),\n    label.r = unit(0.5, \"lines\"),\n    alpha = 0.3,\n    size = 4,\n    family = \"cabrito\"\n  ) +\n  scale_fill_ndvi(limits = c(0, 1), guide = \"none\") +\n  lims(y = c(0, 1)) +\n  labs(\n    title = \"Difference in Maximum and Mean NDVI\",\n    subtitle = \"2010 DHS Clusters: Ouagadougou Region\",\n    x = \"DHS cluster (ordered by difference)\",\n    y = \"NDVI\"\n  ) +\n  theme(\n    axis.text.x = element_blank(),\n    panel.grid.major.x = element_blank(),\n    panel.grid.minor.y = element_blank()\n  )\n\n\n\n\n\n\n\n\n\nAcross clusters, we see some variability in how much of a difference the aggregation function makes. That is, some clusters have a mean that closely resembles the cluster maximum, while other clusters show more of a difference. This could be an indication of\nWe can also see that there is much more variability in maximum NDVI values than mean NDVI values, which may help draw out relationships between NDVI and other health outcomes. When measuring with mean NDVI, all the clusters had nearly the same NDVI value!\n\n\n\n\n\n\nNote\n\n\n\nRemember that this example uses clusters exclusively from the capital region. Because of the overlap in their buffer areas, we would expect their NDVI values (especially their mean values) to be highly correlated.\nIf we included clusters from across the country, we’d likely see far more variability in these results, even for mean values.\n\n\nOf course, this approach also has its pitfalls. For instance, a single pixel with an outlying NDVI value may inflate the overall aggregated NDVI value for an entire cluster region.\n\n\nIdea 2.5: Quantile aggregation\nAs an alternative, it’s also possible to get the NDVI value at a certain percentile of a given cluster’s pixel values. That is, we could get the NDVI value that represents the 95th percentile of all NDVI values in a given cluster. In R, we can do this with the quantile() function.\nRecall that extract() allows us to provide an anonymous function to its fun argument. We can use quantile() with the 0.95 probability level (for instance) to get the 95th percentile NDVI value for each cluster:\n\nextract(\n  ndvi_ouaga,\n  ouaga_gps_buff,\n  fun = function(x) quantile(x, 0.95, na.rm = TRUE)\n)\n\nCalculating the quantile for a cluster may mitigate the effect of outliers, but because NDVI pixels are correlated with one another (that is, pixels with high NDVI values will disproportionately be located next to other pixels with high values), it’s not unusual to observe several pixels near the maximum value in a cluster. In these cases, a quantile approach may not produce a significant difference from using the maximum."
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#idea-3-a-relative-measure",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#idea-3-a-relative-measure",
    "title": "Aggregation Methods for NDVI Data",
    "section": "Idea 3: A relative measure",
    "text": "Idea 3: A relative measure\nBecause NDVI is an index, its values don’t have any intrinsic units (what does it really mean when we see an NDVI value of 0.6, for example?). This means that NDVI is often easier to interpret when considered relative to past NDVI values in a given location. Instead of aggregating NDVI directly, we can compare each pixel to its prior values over the course of many years. These long-run comparisons are called normals.\nTo demonstrate, imagine we wanted to calculate the deviation from normal NDVI we observed in 2021. First, we’ll split our final year of data (2021) from the rest of the time series:\n\n# Split 2021 data from the rest of our data for demonstration\nis_2021 &lt;- year(time(ndvi_nairobi)) == 2021\n\nndvi_nairobi_2021 &lt;- ndvi_nairobi[[is_2021]]\nndvi_nairobi_comp &lt;- ndvi_nairobi[[!is_2021]]\n\nRemember that our NDVI data are recorded on 16-day intervals; we can simplify by calculating the mean monthly NDVI for 2021 and for the rest of the series:\n\nndvi_nairobi_2021 &lt;- tapp(ndvi_nairobi_2021, fun = mean, index = \"months\")\nndvi_nairobi_comp &lt;- tapp(ndvi_nairobi_comp, fun = mean, index = \"months\")\n\n\n\nWe first introduced terra’s tapp() in our CHIRTS post.\nNow our 2021 data and our comparison data are both measured at the monthly level. We can simply subtract them to get the monthly deviation of 2021’s NDVI values from the long-run normal NDVI for each pixel:\n\nndvi_dev &lt;- ndvi_nairobi_2021 - ndvi_nairobi_comp\n\nRecall that subtracting two rasters in terra will operate layer-by-layer. This means that we will correctly subtract each month of 2021 from the corresponding average for that month in the 18-year monthly average raster.\n\n\nShow plot code\n# Helper to split raster layers into a list for small-multiple panel mapping\nsplit_raster &lt;- function(r) {\n  purrr::map(seq_len(nlyr(r)), function(i) r[[i]])\n}\n\n# Function to build individual panels for a small-multiple map using \n# continuous color scheme\nndvi_panel_continuous &lt;- function(x, \n                                  panel_title = \"\",\n                                  show_scale = TRUE,\n                                  ...) {\n  ggplot() + \n    layer_spatial(x, alpha = 1, na.rm = TRUE) +\n    labs(subtitle = panel_title, fill = \"NDVI Deviation\") +\n    scale_fill_gradient2(\n      low = \"#724b00\",\n      mid = \"#f1f1f1\",\n      high = \"#00673f\",\n      na.value = \"transparent\",\n      ...\n    ) +\n    theme_dhs_map(show_scale = show_scale) +\n    theme(\n      axis.text.x = element_blank(), \n      axis.text.y = element_blank(),\n      plot.subtitle = element_text(hjust = 0.5, size = 12),\n      panel.grid = element_blank()\n    )\n}\n\n# Split raster by layer\nr &lt;- split_raster(ndvi_dev)\n\n# Panel labels\nmonths &lt;- c(\"January\", \"February\", \"March\", \"April\", \n            \"May\", \"June\", \"July\", \"August\",\n            \"September\", \"October\", \"November\", \"December\")\n\n# Create map panels\npanels &lt;- purrr::map2(\n  r, \n  months,\n  function(x, y) ndvi_panel_continuous(\n    x, \n    y, \n    show_scale = FALSE,\n    n.breaks = 8, \n    limits = c(-0.6, 0.6)\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Deviation from 18-Year Normal NDVI\",\n    subtitle = \"Nairobi region, 2021\"\n  )\n\n\n\n\n\n\n\n\n\nThis approach gives us a sense of whether certain parts of the year were more or less vegetated than normal. Depending on when periods of abnormally low vegetation occur, they could be an indicator of poor food production.\nAs before, we could proceed to aggregate these values to our DHS clusters with extract():\n\nextract(\n  ndvi_dev,\n  nairobi_gps_buff,\n  weights = TRUE,\n  fun = mean,\n  na.rm = TRUE\n)\n\n\nNote that this is by no means an exhaustive list of aggregation techniques you might use with NDVI data. By demonstrating several possible options, our goal is to emphasize that selecting an appropriate method is a process of evaluating its relative advantages and disadvantages in the context of your overall research."
  },
  {
    "objectID": "posts/2024-08-16-ndvi-data-2/index.html#getting-help",
    "href": "posts/2024-08-16-ndvi-data-2/index.html#getting-help",
    "title": "Aggregation Methods for NDVI Data",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html",
    "href": "posts/2024-09-13-leaflet-pt1/index.html",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "",
    "text": "Up to this point, this blog has used static maps to visualize new datasets and raster processing techniques. Single-image maps like these are often the best choice when presenting specific findings or patterns because you can focus attention on the most important details of your work.\nHowever, there are cases when it can be useful to view your data interactively. For instance, interactive maps can allow you to:\nIn this post and the next, we’ll demonstrate how to build an interactive map to display the population density distribution in Ethiopia. Using data from the Global Human Settlement Layer (GHSL) and the {leaflet} package in R, we’ll build an interactive map that we can use to explore and compare population density at two different time points. Finally, we’ll add DHS survey data to consider how the IPUMS DHS URBAN variable compares to the density estimates we get from the GHSL.\nThis post will focus on how to access GHSL data and build the foundations of a leaflet map. Then, in our next post, we’ll dive into more detailed map options, including color palettes, layer toggling, and more.\nFirst, we’ll load some of the packages we’ll be using. The {leaflet} package is the going to be our main focus for this post:\nlibrary(leaflet)\nlibrary(terra)\nlibrary(dplyr)\nlibrary(sf)"
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#global-human-settlement-layer",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#global-human-settlement-layer",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Global Human Settlement Layer",
    "text": "Global Human Settlement Layer\nWe’ll use data from the Global Human Settlement Layer (GHSL), which provides several data sources for measuring human presence on the planet.\nIn particular, we’ll use GHS-SMOD, which combines two other GHSL products: GHS-BUILT-S, which estimates the presence of built-up surfaces, and GHS-POP, which estimates population distribution.\nYou can find more detailed information about the methodologies used to produce the GHSL in our previous post and on the GHSL website."
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#obtaining-ghs-smod-data",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#obtaining-ghs-smod-data",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Obtaining GHS-SMOD data",
    "text": "Obtaining GHS-SMOD data\nYou can download GHS-SMOD data for free from the GHSL website.\nGHS-SMOD data is distributed for specific epochs (in 5-year increments) and tiles, similar to the NDVI data we used in previous posts. The GHSL interface provides you with an interactive map where you can point and click on the tiles you want to download:\n\n\n\nIn this post, we’re focusing on Ethiopia. To obtain data for the entirety of the country, we’ll need to download 4 separate tiles with the following IDs:\n\nR8_C22\nR9_C22\nR8_C23\nR9_C23\n\nYou can see the tile IDs in the bottom left corner of the tile map as you hover over individual tiles on the world map (see the red oval in the image above).\nWe’ll download data for 2010 and 2020. You can do so by selecting these years in the Epoch selection grid on the left side of the screen (see the red square in the image above) and clicking on the same tiles again.\nClicking the tiles will download a .zip archive containing the GHS-SMOD data for the selected tile and epoch. We’ve extracted the files within each of the 4 .zip archives and placed them in the data directory of our project. This produces a set of directories (with names like GHS_SMOD_E2020_GLOBE_R2023A_54009_1000_V2_0_R8_C22) containing .tif files that hold the actual raster data for each tile and epoch."
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#loading-ghs-smod-data",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#loading-ghs-smod-data",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Loading GHS-SMOD data",
    "text": "Loading GHS-SMOD data\nWe could load each .tif file individually with rast(), but because we have several files with similar naming patterns, we can identify the relevant files in our data directory more efficiently by listing them with list.files().\nHere, we use the pattern argument to specify that we only want to list files that match the text \"GHS_SMOD_E2010\" (for the 2010 epoch) and have the tif extension.\nWe’ve also set recursive = TRUE to list all files (even within subdirectories of the data directory) and full.names = TRUE to produce the full file path for each file:\n\nghs_smod_2010 &lt;- list.files(\n  \"data\", \n  pattern = \"GHS_SMOD_E2010.+tif$\", \n  recursive = TRUE, \n  full.names = TRUE\n)\n\nghs_smod_2010\n#&gt; [1] \"data/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R8_C22/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R8_C22.tif\"\n#&gt; [2] \"data/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R8_C23/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R8_C23.tif\"\n#&gt; [3] \"data/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R9_C22/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R9_C22.tif\"\n#&gt; [4] \"data/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R9_C23/GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R9_C23.tif\"\n\n\n\n\n\n\n\nRegular Expressions\n\n\n\nWe’re use a regular expression to create the text-matching pattern for these files. We’ve introduced regular expressions previously, so take a look there if this is unfamiliar.\nIn this case, the .+ pattern matches any sequence of characters between the GHS_SMOD_E2010 text and the tif text. The $ pattern indicates that we want the tif text to appear at the very end of the file name.\nIf you’re not comfortable with regular expressions, you can always manually write out the file paths for the files you want to load.\n\n\nWe can do the same for our 2020 files. All we need to do is change the match pattern to include E2020 instead of E2010:\n\nghs_smod_2020 &lt;- list.files(\n  \"data\", \n  pattern = \"GHS_SMOD_E2020.+tif$\", \n  recursive = TRUE, \n  full.names = TRUE\n)\n\nThe next steps follow closely with the workflow we introduced for tiled NDVI data: first, we’ll use the {purrr} package to iterate over each file path input and load it into its own raster with rast():\n\nghs_smod_2010 &lt;- purrr::map(ghs_smod_2010, rast)\nghs_smod_2020 &lt;- purrr::map(ghs_smod_2020, rast)\n\nThis produces a list of rasters for each of 2010 and 2020. For instance, the ghs_smod_2010 variable stores 4 rasters, each of which contains a different spatial area (tile) for the 2010 epoch.\nTo combine the tiles for each epoch, we need to mosaic the tiles together. We can do so with mosaic() from terra, as we’ve shown before. Since we have multiple tiles to combine, we can iteratively mosaic them with reduce() from the purrr package. This takes each raster in the list and combines it with the others one-by-one using the supplied function (in this case, mosaic()).\n\nghs_smod_2010 &lt;- purrr::reduce(ghs_smod_2010, mosaic)\nghs_smod_2020 &lt;- purrr::reduce(ghs_smod_2020, mosaic)\n\nAt this point, we have two rasters for the full spatial extent of Ethiopia, one for each epoch. We can combine them with the familiar c() function so each epoch is represented in a single raster layer in the same raster stack.\n\nghs_smod &lt;- c(ghs_smod_2010, ghs_smod_2020)\n\nNow we finally have a single raster data source for the entirety of Ethiopia with 2 layers: one for 2010 and one for 2020.\n\nghs_smod\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 2000, 2000, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 1000, 1000  (x, y)\n#&gt; extent      : 2959000, 4959000, 0, 2e+06  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : World_Mollweide \n#&gt; source(s)   : memory\n#&gt; varnames    : GHS_SMOD_E2010_GLOBE_R2023A_54009_1000_V2_0_R8_C22 \n#&gt;               GHS_SMOD_E2020_GLOBE_R2023A_54009_1000_V2_0_R8_C22 \n#&gt; names       : GHS_SMOD_E2010_~000_V2_0_R8_C22, GHS_SMOD_E2020_~000_V2_0_R8_C22 \n#&gt; min values  :                              10,                              10 \n#&gt; max values  :                              30,                              30"
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#categorical-rasters",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#categorical-rasters",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Categorical rasters",
    "text": "Categorical rasters\nGHS-SMOD is in raster format, but it’s different than some of the other rasters we’ve considered up to this point. Rather than being a continuous measure of population density, it provides a set of density categories. Each pixel in the raster will have a single numeric value, but these values correspond to particular categories:\n\nGHS-SMOD Density Categories\n\n\nCode\nDensity Category\n\n\n\n\n10\nWater\n\n\n11\nVery low\n\n\n12\nLow\n\n\n13\nRural\n\n\n21\nSuburban\n\n\n22\nSemi-dense urban\n\n\n23\nDense urban\n\n\n30\nUrban center\n\n\n\nIf we examine all the unique values in the raster, we notice they all fall neatly into one of these categories:\n\nas.numeric(unique(values(ghs_smod[[1]])))\n#&gt; [1] 11 12 13 10 21 23 22 30\n\nHowever, because the raster values are still encoded as numbers, our ghs_smod object currently has no indication that its values should be treated as categories rather than continuous numeric values.\nTo indicate the categorical nature of our raster, we can set the levels() of the raster using {terra}. We’ll make a data.frame that contains a mapping of the numeric values and their associated density categories:\n\ndensity_lvls &lt;- data.frame(\n  id = c(10, 11, 12, 13, 21, 22, 23, 30),\n  urban = c(\n    \"Water\", \"Very low\", \"Low\", \"Rural\", \"Suburban\", \n    \"Semi-dense urban\", \"Dense urban\", \"Urban center\"\n  )\n)\n\ndensity_lvls\n#&gt;   id            urban\n#&gt; 1 10            Water\n#&gt; 2 11         Very low\n#&gt; 3 12              Low\n#&gt; 4 13            Rural\n#&gt; 5 21         Suburban\n#&gt; 6 22 Semi-dense urban\n#&gt; 7 23      Dense urban\n#&gt; 8 30     Urban center\n\nThen, we can assign these levels to our raster layers with the levels() function:\n\n\nEach layer (i.e. epoch) needs to be assigned levels individually. For raster stacks containing more epochs, you may want to do this iteratively. Since we are only using two epochs, we’ll simply assign levels manually for each layer.\n\nlevels(ghs_smod[[1]]) &lt;- density_lvls\nlevels(ghs_smod[[2]]) &lt;- density_lvls\n\nNow, we can confirm that our raster layers are indeed categorical with is.factor():\n\nis.factor(ghs_smod)\n#&gt; [1] TRUE TRUE\n\nExplicitly converting our raster to categorical format will come in handy later when we project our rasters to be compatible with our interactive maps."
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#basemaps",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#basemaps",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Basemaps",
    "text": "Basemaps\nLeaflet comes with several built in basemaps, or background layers that show common geographic boundaries to help orient a viewer to the map.\nTo make a new Leaflet map, first use the leaflet() function. On its own, this doesn’t add anything to our map, but we can add a basemap with the addProviderTiles() function. We’re going to use the Dark Matter basemap tiles from CartoDB.\n\n\nTo see all the available basemap providers, use leaflet::providers.\nThe setView() function allows us to specify the centerpoint and initial zoom level of the map:\n\nleaflet() |&gt;\n  addProviderTiles(\"CartoDB.DarkMatterNoLabels\") |&gt; \n  setView(lng = 10, lat = 40, zoom = 2)\n\n\n\n\n\n\n\n\n\n\n\nMap Style\n\n\n\nDark maps should be deployed with care, as they occasionally become difficult to read. In this case, we’ll plot our density data as if it were “lighting up” on top of the background, so a dark basemap should work well. This visual nod to city lights will hopefully help reinforce the subject matter of the map. Note that we’re not actually displaying a map of light sources themselves, though!\n\n\nAs you can see, we already have an interactive map of the world! We’ll go ahead and save this basemap in an R variable so we don’t have to rewrite this code. We’ll also update our view parameters to set the default map view to be centered on Ethiopia, since this will be our area of interest:\n\nbasemap &lt;- leaflet() |&gt; \n  addProviderTiles(\"CartoDB.DarkMatterNoLabels\") |&gt; \n  setView(lng = 40.81, lat = 8.23, zoom = 6)"
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#adding-data-layers",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#adding-data-layers",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Adding data layers",
    "text": "Adding data layers\nOf course, we ultimately want to map our own data, not just a prepackaged basemap.\nAdding data to a map is as simple as adding more layers, but we’ll need to consider what type of data we’re adding, as this will affect what type of geometry layer we use (this is similar to the different geom_ layer options in ggplot2). You can see the most common types of layers by looking at the help for ?addControl.\nIn our case, our ghs_smod data is in raster format, so we’ll want to use addRasterImage(). All we need to do is provide our data to the map layer (we’ll only plot the 2010 data for now):\n\nbasemap |&gt; \n  addRasterImage(ghs_smod[[1]])\n\n\n\n\n\n\nCoordinate reference systems\nYou may notice that the raster above appears a bit warped on the edges, despite the fact that the GHSL download page showed square tiles. Before we forge ahead, it’s worth considering our data’s coordinate reference system (CRS) to make sure we understand why this is happening.\n\n\nWe’ve covered reference systems and projections in a previous post, so check that out if you need a refresher.\n\nWeb Mercator\nLeaflet uses a CRS known as Web Mercator in its maps. Web Mercator (EPSG:3857) is a modified form of the common Mercator projection and was designed to improve processing speeds for web-based maps. Many web-based mapping applications therefore use Web Mercator by default.\nAccordingly, leaflet will automatically project our data to Web Mercator. So, even though our GHS data is in a Mollweide projection, leaflet displays the raster in Web Mercator. This is why the raster boundaries appear distorted from what we may have expected based on the GHSL website.\nIf we turn off this default behavior, we’ll notice the discrepancy in our data and basemap (which is also in Web Mercator). See how the coastlines in the Red Sea appear offset between the raster and the basemap if we don’t project our data:\n\nbasemap |&gt; \n  setView(lat = 15.5, lng = 41.89, zoom = 7) |&gt; \n  addRasterImage(\n    ghs_smod[[1]], \n    project = FALSE, # Suppress automatic projection\n    opacity = 0.6\n  )\n\n\n\n\n\n\n\nRaster reprojection\nWhile leaflet is willing to do this transformation for us, it’s often better to explicitly transform our data to the correct CRS to ensure that any other spatial operations we do to our rasters work as expected. We can do so by using project() with the EPSG code for Web Mercator\n\nghs_smod &lt;- project(ghs_smod, \"epsg:3857\")\n\nAs we’ve mentioned before, it’s typically best to avoid projecting raster data when possible because doing so requires that the raster values be resampled, introducing uncertainty into the data.\nBecause our goal here is visualization, we don’t need to be as concerned about the loss of precision in our data for these maps. If we were using these data for spatial analysis, we would want to be more careful to find a CRS that would minimize distortion in our region of interest.\n\n\n\n\n\n\nProjecting Categorical Rasters\n\n\n\nCategorical raster present an additional challenge when projecting because each cell must belong to a single category.\nFortunately, because we’ve made sure that our ghs_smod raster is represented as a categorical raster, project() will automatically use a raster resampling method that preserves this property.\nHowever, if the input raster were still in numeric format, resampling would likely produce some intermediate cell values that would no longer belong to one of the density categories provided by the GHS!"
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#up-next",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#up-next",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Up next",
    "text": "Up next\nWe’ve now completed the primary data preparation steps! We’ve managed to\n\nDownload GHS-SMOD data for Ethiopia\nLoad and organize our raster data into a single raster stack\nConvert our raster data to categorical format\nProject our raster to the CRS expected by leaflet\n\nIn our next post, we’ll focus more on the aesthetic options available when producing leaflet maps. We’ll discuss color palettes, additional layer types, legends, and some basic interactivity!"
  },
  {
    "objectID": "posts/2024-09-13-leaflet-pt1/index.html#getting-help",
    "href": "posts/2024-09-13-leaflet-pt1/index.html#getting-help",
    "title": "Using Leaflet for Exploratory Data Analysis: Part 1",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html",
    "href": "posts/2024-10-25-qgis/index.html",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "",
    "text": "Up to this point, this blog has used R to demonstrate spatial processing and analysis techniques. We’ve emphasized some of the benefits of using R, but it’s far from the only software available for spatial processing. Many spatial analysts instead rely on dedicated GIS (Geographic Information Systems) software, which provide a point-and-click interface for performing spatial analysis workflows.\nIn this post, we will use the open-source GIS software QGIS (Quantum GIS) to integrate environmental data with health survey data. Using a case study from Kenya, we will combine monthly temperature data from CHIRTS with geolocated sample clusters from the Demographic and Health Surveys Program (DHS). If you’ve been following along with this blog, you’ll recognize some similarities between this post and previous posts. But while the spatial analysis techniques will be familiar, the tools we use to implement them will be new.\nThis step-by-step guide will provide an overview of QGIS and show you how to import data, process raster data, create spatial buffers, and extract aggregated data for analysis."
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#download-and-install-qgis",
    "href": "posts/2024-10-25-qgis/index.html#download-and-install-qgis",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Download and install QGIS",
    "text": "Download and install QGIS\nFollow the instructions on the QGIS download page to download the appropriate QGIS version for your operating system. You have the option to select the Long Term Version for stability or the Latest Version to gain access to the newest features and updates."
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#the-qgis-interface",
    "href": "posts/2024-10-25-qgis/index.html#the-qgis-interface",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "The QGIS interface",
    "text": "The QGIS interface\nOnce QGIS is installed, you’re ready to explore its interface and tools. Figure 1 shows a screenshot of the main QGIS interface with some of the main components highlighted.\n\nMenu Bar: Located at the top of the interface, the Menu Bar provides access to all functions and tools within QGIS, organized into categories like Project, Edit, View, Layer, Raster, Vector, Settings, etc. This is where we can access more advanced features, adjust settings, and manage our workspace.\nToolbar: Directly below the Menu Bar, the Toolbar contains icons for many of the most frequently used tools in QGIS. Here, we’ll find quick access to functions like adding layers, zooming in and out, saving our project, and using geoprocessing tools. Each icon corresponds to a specific function, making it easy to perform actions without needing to navigate through menus.\nBrowser: The Browser panel on the left helps us quickly access files and databases on our computer or connected servers. It’s similar to a file manager, allowing us to drag and drop data directly into the Map Window. If it’s not visible, you can enable it by selecting View ‣ Panels ‣ Browser in the toolbar.\nLayers Panel: The Layers Panel displays all the spatial layers that we’ve added to our current QGIS project. Each layer represents a different data source (e.g., country boundaries, temperature data, or health survey points). We can control the visibility of each layer, reorder them (by directly dragging them up or down), and change their styles directly from this panel. Right-clicking on a layer gives us additional options like opening the attribute table or adjusting the symbology. The visibility of layers on the Map Window depends on their position in this panel—make sure to keep the layer you want to visualize on top.\nMap Window: The Map Window is the central area where our spatial data is visually displayed. As we add, modify, and analyze data, the results are shown here. We can zoom in, zoom out, and pan across different parts of our map using tools from the toolbar or our mouse.\nInformation Bar: At the bottom of the QGIS window, the Information Bar displays details about our current map view, including coordinates, scale, magnification level, and active CRS (Coordinate Reference System). It’s helpful for ensuring accurate spatial alignment and for referencing the precise location of our map elements.\nSearch Bar: The Search Bar (or Locator Bar) allows us to quickly find tools, layers, and functions within QGIS. We can type in keywords like “Buffer” or “Raster Calculator” to access specific tools or commands without having to navigate through multiple menus. This feature speeds up our workflow and is especially useful for beginners still familiarizing themselves with the software.\n\n\n\n\n\n\n\n\nFigure 1: An overview of the QGIS interface"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#spatial-data-sources",
    "href": "posts/2024-10-25-qgis/index.html#spatial-data-sources",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Spatial data sources",
    "text": "Spatial data sources\nFor this analysis, we’ll need three spatial datasets. We’ve created a project folder called Kenya_Temperature_Analysis to store these files. To follow along, we suggest you create a similar directory to store all the spatial data files used in this demo.\n\nKenya boundary data\nThis vector shapefile contains the administrative boundaries of Kenya. This will be useful as a reference layer for clipping and other spatial operations. We’ll need boundaries for the level-0 administrative boundary—that is, the country’s border.\nThere are a variety of resources available to obtain administrative boundary data. In the past, we’ve download data from the DHS Spatial Data Repository. For this post, we’ve decided to download our data from the Database of Global Administrative Areas (GADM) website because it provides level-0 data directly.\nFrom the GADM website, select “Kenya” in the country drop-down and download the border data in shapefile format. This will provide a compressed folder containing multiple files (e.g., .shp, .dbf, .shx, .prj) that are essential for the shapefile to function correctly in GIS software. The file may be compressed, so be sure to extract all files into the same directory, as QGIS requires them to be together for proper rendering. Save this extracted folder under your overall project folder.\n\n\nTemperature raster data\nWe’ve introduced CHIRTS in previous posts, but so far we’ve always used daily data. To simplify things for this post, we’ll use the CHIRTS monthly data product instead.\nWe’ll use monthly maximum temperature (Tmax) data for 2016, which you can download from the CHIRTS website. Each file represents the maximum temperature for a specific month from January 2016 (CHIRTSmax.2016.01.tif) to December 2016 (CHIRTSmax.2016.12.tif) These files will allow us to calculate the 2016 annual average maximum temperature.\nOnce you’ve downloaded these files, you’ll want to put them in their own sub-directory within your project. In our case, we’ve called this directory CHIRTS.\n\n\nDHS cluster data\nAs we’ve done in many posts, we’ll use geolocated DHS cluster coordinates to identify the displaced locations of DHS sample clusters for Kenya.\nAs we’ve described before, to obtain the GPS coordinates for a specific sample, you’ll have to log into your DHS Program account. Specify the country of interest (Kenya, in this case), and, on the line for the appropriate sample year (we’re using the 2022 sample for this demo), click the link to download the GPS coordinate data under the GPS Datasets heading.\n\n\nAs always, you’ll need to have a registered DHS project to download data for these cluster coordinates. If needed, you can always use the workflow from this demonstration with data from another DHS sample and area if you haven’t requested permission for Kenya samples specifically.\nYou’ll be presented with a new page containing a list of download links. Scroll down to the Geographic Datasets section. Download the file listed as a shapefile (.shp).\nOnce downloaded, save the shapefile into a sub-folder under your project folder to keep everything organized. For the 2022 sample, the downloaded file name should be KEGE8AFL.shp."
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#preparing-our-project",
    "href": "posts/2024-10-25-qgis/index.html#preparing-our-project",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Preparing our project",
    "text": "Preparing our project\nBefore we can begin, we need to initiate a QGIS project to store our work and add the data sources identified above to our project.\n\n1. Create a new QGIS project\nOpen QGIS and create a new project by navigating to Project ‣ New.\nSave the project by navigating to Project ‣ Save. Provide a project name (in our case, we’ve used Spatial_Analysis_Kenya.qgz) and select the designated project folder we created previously (Kenya_Temperature_Analysis). Saving the project file ensures we can pause and resume our work without losing progress.\n\n\n2. Add the temperature raster data\nSelect Layer ‣ Add Raster Layer. Navigate to the CHIRTS sub-folder within our project folder and select all the monthly raster files (CHIRTSmax.2016.01.tif to CHIRTSmax.2016.12.tif). Click Add to load the rasters into QGIS.\n\n\n3. Add the Kenya shapefile\nGo to Layer ‣ Add Layer ‣ Add Vector Layer. Browse to the Shapefiles sub-folder and select the Kenya boundary shapefile (If you downloaded from GADM, the file should be named gadm41_KEN_0.shp). Again, click Add to add the shapefile to our QGIS project.\n\n\n4. Load the DHS cluster data\nGo to Layer ‣ Add Layer ‣ Add Vector Layer to import the DHS cluster shapefile (‘KEGE8AFL.shp’). Click Add.\nAt this point, your interface should look something like Figure 2:\n\n\n\n\n\n\n\nFigure 2: QGIS interface with initial data layers loaded"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#organizing-and-visualizing-layers-in-qgis",
    "href": "posts/2024-10-25-qgis/index.html#organizing-and-visualizing-layers-in-qgis",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Organizing and visualizing layers in QGIS",
    "text": "Organizing and visualizing layers in QGIS\nWhen we import data into QGIS, it is represented in its raw form and at its full spatial extent (as in Figure 2). To make sense of this data and facilitate analysis, it’s crucial to understand some basic organizing and visualizing tools within QGIS. These tools help us manage the appearance and arrangement of layers, ensuring that the data is clear and easy to interpret.\n\nLayer visibility and order\nLayers in QGIS are displayed based on their order in the Layers Panel. Make sure that the DHS cluster shapefile is on top, followed by the Kenya boundary shapefile, and then the raster layers. This will help maintain a clean and visible map. You can reorder layers by dragging them up or down within the Layers Panel.\n\n\nZoom to layer\nAfter adding layers, it’s useful to zoom in on specific layers for better visualization:\n\nRight-click on a layer in the Layers Panel and select Zoom to Layer(s).\nThis adjusts the map view to focus on the full extent of the selected layer, making it easier to work with your data.\n\nIf you lose track of your map (which happens often in QGIS!), using Zoom to Layer(s) can quickly help re-center the view on any specific dataset.\n\n\nWorking with coordinate reference systems\nWe’ve mentioned coordinate reference systems in previous posts. As was the case there, it’s worth confirming the CRS of the layers in QGIS to ensure that we’re conducting the expected spatial operations. To check the CRS of a layer:\n\nRight-click on any layer (e.g., the Kenya shapefile or DHS clusters) and select Properties.\nIn the Information tab, check the CRS of the layer, shown as an EPSG code (e.g., EPSG:4326 for WGS 84).\n\nQGIS also sets a project-wide CRS, which is used for on the fly projection. To check or change the CRS of the project, click on the CRS displayed in the bottom-right corner of the QGIS window. Make sure that the project CRS matches the CRS of your layers for consistent spatial alignment.\n\n\n\n\n\n\nNote\n\n\n\nWhile you’re working, QGIS projects layers “on the fly”. That is, if you have layers in different reference systems, QGIS will visualize them in the single project CRS so that the layers align visually. However, this doesn’t actually project any layer’s data to a new CRS. Thus, for spatial analysis, you’ll want to double check to ensure each layer is in an appropriate CRS for the operations you’re conducting.\n\n\n\nChange the CRS of the DHS cluster point shapefile\nAs we’ve discussed before, we need to project our cluster points before we can buffer them, as we want to buffer in units of meters, not degrees. For Kenya, an appropriate projection is UTM zone 37S (EPSG:32737).\n\n\nRefer to EPSG.io to identify the EPSG code for a given projection and for help identifying projection properties.\nThere are two ways to reproject the vector layer in QGIS:\n\nMethod 1: Export the layer\n\nRight-click the DHS cluster layer (KEGE8AFL)\nSelect Export ‣ Save Features As\nIn the CRS section, choose the appropriate UTM projection. In our case, we will use zone 37S. Provide a simple name to the reprojected shapefile (e.g., Clusters.shp) and save.\n\nMethod 2: Reproject via Data Management\n\nNavigate to Vector ‣ Data Management Tools ‣ Reproject Layer\nIn the window that opens, select the DHS cluster layer as the Input Layer\nChoose the appropriate UTM projection (again, UTM 37S in our case) in the Target CRS section.\nClick Run to complete the reprojection. By default this creates a temporary layer, but you also have the option to specify an output file.\n\n\n\n\n\nSymbology and visualization\nSymbology refers to the visual representation of our data on the map. We can adjust symbology to make for a more interpretable and visually appealing map.\n\nRight-click on the Kenya shapefile and select Properties ‣ Symbology.\nChoose Outline: Simple Line in the Symbol layer type field to help distinguish the boundary from the background raster layers. Feel free to explore other symbology options to familiarize yourself with the interface. Click Apply to periodically apply changes to the map, and OK when you’re satisfied.\nFor the DHS clusters, use a Simple Marker layer type to mark each point clearly on the map. Play around with colors, strokes, and line weights until you’re satisfied and click OK.\nFor our CHIRTS data, change the Render Type to singleband pseudocolor. You can select from several pre-made color ramps or design your own by clicking the Color Ramp drop-down arrow. In our case, we’re using the Spectral color ramp.\nYou’ll also want to change the Min option to something much greater than -9999 (we’re using -10 for now). By default, the raster color scheme will use the full range of the data and expand to accommodate extreme values like -9999, which represent “no data” areas. This distorts the color scale. Adjusting the minimum and maximum values allows for better visual representation and more accurate analysis of the data.\n\nFigure 3 presents an example QGIS interface after applying the organization and visualization techniques discussed. It’s okay if your symbology looks a little different from ours. We recommend that you take the time to explore the available symbology options, as they offer interesting ways to enhance your data presentation and analysis.\n\n\n\n\n\n\n\nFigure 3: Data layers after symbology adjustments"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#temporal-aggregation-using-raster-calculator",
    "href": "posts/2024-10-25-qgis/index.html#temporal-aggregation-using-raster-calculator",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Temporal aggregation using raster calculator",
    "text": "Temporal aggregation using raster calculator\nSince we have 12 raster layers representing monthly maximum temperatures (Tmax) for the year 2016, we can perform temporal aggregation by averaging these layers.\nIn this example, we will calculate the annual average Tmax for 2016. Navigate to Raster ‣ Raster Calculator. In the calculator window, we need to enter the formula to calculate the average from each of the 12 raster bands. To do so, you can double click on each raster band in the Raster Bands selection pane to bring it in the expression bar. To add addition and division operators, you can type them directly into the expression bar or add them from the drop-down Operators panel.\nYou should end up with an expression in the expression bar that looks like the one shown in the Raster Calculator Expression box shown in Figure 4. This formula sums the values from all 12 months and divides by 12 to obtain the average temperature for the year. Name the output file as Annual_Average_Tmax_2016.tif and save it in an appropriate folder within your project directory (Figure 4). We can keep the defaults for the other options.\n\n\n\n\n\n\nTip\n\n\n\nAfter selecting a file location, the raster calculator window may disappear behind the main QGIS window. Be sure to find this window and officially run the process, or it will appear as if nothing has happened!\nThis may occur with other geoprocessing tools as well.\n\n\nThe new raster will appear as a new layer in our Layers Panel, and you can apply the same singleband pseudocolor scheme for visualization to better interpret the temperature data. We can remove the single-month files from QGIS to keep things tidy (right click on the single-year layers and select Remove layer).\n\n\n\n\n\n\n\nFigure 4: Raster Calculator window"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#clipping-raster-to-the-area-of-interest",
    "href": "posts/2024-10-25-qgis/index.html#clipping-raster-to-the-area-of-interest",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Clipping raster to the area of interest",
    "text": "Clipping raster to the area of interest\nClipping the raster to the Kenya boundary is important because it ensures that our analysis is focused only on the area of interest, eliminating unnecessary data from outside the country’s borders and reducing file size for better processing.\nTo perform this task, we will follow a simple process in QGIS. First, we will navigate to Raster ‣ Extraction ‣ Clip Raster by Mask Layer.\nIn the window that appears, we will set the Input Layer to the newly generated annual average raster (Annual_Average_Tmax_2016.tif). Then, for the Mask Layer, we will select the Kenya boundary shapefile (gadm41_KEN_0.shp). This ensures that the raster is clipped to the boundaries of Kenya. Finally, we will name the output file as Kenya_Clipped_Tmax_2016.tif and save it in the CHIRTS folder (Figure 5).\n\n\n\n\n\n\n\nFigure 5: Clip Raster window\n\n\n\n\nAfter the process is complete, we will have a new raster that only covers the extent of Kenya, ready for further analysis. We can remove the full extent raster file from the layer panel and use the singleband pseudocolor scheme for better visualization. Notice that the minimum value in this case is already accurate as we do not have missing data within this clipped raster (Figure 6).\n\n\n\n\n\n\n\nFigure 6: Results of raster clip, with updated symbology"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#buffering-dhs-cluster-points",
    "href": "posts/2024-10-25-qgis/index.html#buffering-dhs-cluster-points",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Buffering DHS cluster points",
    "text": "Buffering DHS cluster points\nCreating buffers around point locations is crucial for spatial analysis because it allows us to define areas of influence or impact around each point, enabling more focused environmental or demographic analysis. In this post as in previous ones, we are interested in creating buffers around the point locations of the geolocated DHS clusters.\nTo create buffers, we will go to Vector ‣ Geoprocessing Tools ‣ Buffer. In the window that opens, we will set the Input Layer to be the reprojected DHS cluster file. Next, we will specify a buffer distance of 10,000 meters, which creates a 10 km zone around each cluster point. Finally, we will save the output as DHS_10km_Buffers.shp in the Shapefiles folder (Figure 7). Once this process is complete, we will have buffer zones around the DHS clusters, ready for further spatial analysis and extraction of environmental data.\n\n\n\n\n\n\n\nFigure 7: Spatial buffer window\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you haven’t successfully reprojected the cluster location shapefile, you won’t be able to buffer in meters. Double check the CRS of the layer as described previously and ensure that the layer has been projected to UTM coordinates.\n\n\nAs with other layers, we can adjust the symbology of the buffer regions (e.g., to decrease opacity) by right clicking the layer in the Layers pane and selecting Properties ‣ Symbology.\nUsing the Magnifier tool in the bottom right, we can zoom into specific areas to inspect parts of the map without changing the overall map scale, which helps in closely examining details (Figure 8).\n\n\n\n\n\n\nFigure 8: Use the Magnifier tool to inspect geoprocessing output"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#aggregating-raster-values",
    "href": "posts/2024-10-25-qgis/index.html#aggregating-raster-values",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Aggregating raster values",
    "text": "Aggregating raster values\nExtracting environmental data at aggregated levels, such as within buffer zones, allows us to quantify environmental conditions (in this case, temperature) in specific areas. This process helps in identifying patterns and relationships between environmental factors and outcomes in defined regions and their populations.\nWe are interested in aggregating buffer-level Tmax statistics. To aggregate the data, go to Processing ‣ Toolbox ‣ Raster analysis ‣ Zonal Statistics.\n\n\nIf you can’t find a tool, you can always search for it in the search bar. For instance, a search for “Zonal statistics” would reveal where the tool is located.\nIn the window that opens, set the Input Layer to DHS_10km_Buffers.shp and the Raster Layer to Kenya_Clipped_Tmax_2016.tif. Set the output column prefix to \"Tmax_\". Under Statistics to calculate, choose Mean to compute the average temperature for each buffer based on the raster pixels within that buffer. Save the file in the Shapefile sub-folder with the name Mean_Tmax_buffer.shp (Figure 9).\n\n\n\n\n\n\nFigure 9: Zonal Statistics window\n\n\n\nWith this, we now have a new shapefile that includes a new column (Tmax_mean) in the buffer layer’s attribute table, representing the mean temperature for each buffer zone (Figure 10). You can verify this by opening the attribute table (right-click on the new layer and select Open Attribute Table). To export the attribute table as a CSV file, right-click on the layer, go to Export ‣ Save Feature As, or save it in another tabular format. It is crucial to keep the unique cluster ID (DHSID) linked to the extracted data for easy reference and further analysis.\n\n\n\n\n\n\n\nFigure 10"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#additional-resources",
    "href": "posts/2024-10-25-qgis/index.html#additional-resources",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Additional Resources",
    "text": "Additional Resources\n\n\nQGIS Documentation\nDHS Blog post discussing linking extreme weather data for use in health research.\nDetailed Tutorial Video on IUSSP’s YouTube Channel\nGPS data collection and random displacement in DHS"
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#getting-help",
    "href": "posts/2024-10-25-qgis/index.html#getting-help",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-10-25-qgis/index.html#acknowledgments",
    "href": "posts/2024-10-25-qgis/index.html#acknowledgments",
    "title": "Using QGIS for Spatial Data Analysis",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nSpecial thanks to Finn Roberts, Senior Data Analyst at IPUMS, for assistance in editing and reviewing this post."
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html",
    "href": "posts/2025-03-31-viirs/index.html",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "",
    "text": "The Normalized Vegetation Index (NDVI) is a measure that can be used in research involving climate patterns, agriculture, access to green space and much more. We’ve introduced NDVI in a previous post, where we downloaded and prepared NDVI data from MODIS.\nMODIS, or the Moderate Resolution Imaging Spectroradiometer, is a global imager on two satellites (Terra and Aqua) that has collected images of Earth’s surface for more than 20 years. While originally designed with the expectation of a 5-year lifespan, MODIS is still operating today. However, as we mentioned previously, both Terra and Aqua are set to be decommissioned. As they drift from their original orbits, their overpass times will increasingly lag, making the data they collect more difficult to compare over time. Terra will continue to collect data until December 2025, while Aqua will remain in orbit until August 2026.\nFortunately, a new imaging instrument has already been launched: the Visible Infrared Imaging Radiometer Suite (VIIRS). In this post, we’ll adapt the workflows we introduced before using MODIS data with a new workflow using VIIRS data."
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#why-coarse-resolution-data",
    "href": "posts/2025-03-31-viirs/index.html#why-coarse-resolution-data",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "Why coarse resolution data?",
    "text": "Why coarse resolution data?\nJust like MODIS, VIIRS provides several different spatial and temporal resolution options.\nWhile it may seem obvious that you’d always want to find the highest resolution data available, this isn’t always the case. Particularly when linking environmental data to large-scale surveys as we’ve demonstrated throughout this blog, the limiting factor on our precision is typically the location data available in the survey, not the environmental data. Essentially, because of the inherent uncertainty in our survey locations, the benefit of highly detailed environmental data is lost.\nIn many cases, NASA will provide data products that have already been aggregated both spatially and temporally. These data are often easier to work with and smaller in size, and the aggregation methods that NASA uses often do a better job of handling data quality issues than we could do when aggregating ourselves.\nThat being said, there are certainly cases where it’s worthwhile to obtain higher-resolution data. This may be the case if you have a high degree of spatial resolution in the data you’re linking to the environmental metrics, or if you want to aggregate data in a particular way to calculate environmental metrics that NASA doesn’t provide out of the box (see our CHIRTS heatwave post for an example).\nThe key is that when coarse resolution data are sufficient—as they often are—they’re typically the best option.\n\nVIIRS global data: VNP13C2\nIn the Earthdata Search interface, the product we’ll use has the code VNP13C2.\nVNP is the prefix used for products that use the VIIRS instrument aboard the NPP (as opposed to MOD, which we used for Terra-based MODIS products).\n\n\n\n\n\n\nNote\n\n\n\nNPP refers to the Suomi National Polar-orbiting Partnership, a satellite launched in 2011 with VIIRS (and other instruments) on board. The newer Joint Polar Satellite System (JPSS-1) also houses VIIRS instruments (with the code VJ1).\nWe’re using data from 2014 in this demo for consistency with our previous post on MODIS. However, if you’re working with data more recent than 2017, you likely will want to use JPSS-1 VIIRS data.\n\n\nThe 13 component of the collection name is a code that indicates that the collection is a vegetation index.\nThe spatial resolution is represented by the final 2 digits of the collection code. In this case we’ll use C2, which is the coarsest data available from VIIRS (~5.5 km) and is delivered globally. This and other global data products are provided on the Climate Modeling Grid, or CMG, in which data are in geographic (latitude and longitude) coordinates (this will be important later!).\nTo find the VNP13C2 product, enter the code in the search bar on the Earthdata Search interface.\nYou should see a few different collections that pop up. We’ll want to use the latest version of the data. NASA regularly makes improvements and corrections to the data from its instruments, and importantly these changes are retroactively applied to prior years of data. Thus, it’s always advisable to use the latest version of a data product once it’s released. At the time of writing, the latest version is v002, so we’ll select the “VIIRS/NPP Vegetation Indices Monthly L3 Global 0.05Deg CMG V002” collection.\n\n\n\nFrom here, you can follow the instructions we introduced in our previous post (or use the Earthdata Search walk-through available when you launch the website) to narrow down your temporal range of interest. Here, our data are global, but if you were working with a higher resolution dataset, you could also select data for a specific spatial region through this interface as well.\nWe’ve restricted our data to the first three months of 2014 for demonstration. You can download the data by clicking Download All.\n\n\n\nWe’ve placed these VNP13C2 files in a data/VNP13C2 directory, which we’ll use for the rest of the post."
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#fixing-raster-metadata",
    "href": "posts/2025-03-31-viirs/index.html#fixing-raster-metadata",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "Fixing raster metadata",
    "text": "Fixing raster metadata\nAs stated in the VIIRS Vegetation Index Product Guide, the VIIRS CMG data are provided in WGS84 geographic coordinates (that is, latitude and longitude) for the entire globe.\nWe could use this knowledge to provide the missing extent and CRS information ourselves. We know that the global extent should span from -180° to 180° in longitude and -90° to 90° in latitude, so we can easily set the extent using terra’s ext():\n\n# Note that `ext()` expects coordinates in (xmin, xmax, ymin, ymax) order\next(viirs_cmg) &lt;- c(-180, 180, -90, 90)\n\nSimilarly, we could provide the EPSG code for WGS84 using terra’s crs():\n\ncrs(viirs_cmg) &lt;- \"epsg:4326\"\n\nUnfortunately, if we go ahead and plot our data, we notice something a bit unexpected:\n\nplot(viirs_cmg[[1]])\n\n\n\n\n\n\n\n\n\n\nAs it turns out, because of the lack of geographic metadata, the file was flagged as “flipped” by GDAL (the spatial translation library used by terra), so terra flipped the file vertically. When loading our raster, we can set noflip = TRUE to ensure our data are loaded in the correct orientation.\nYou may also have noticed that the range of values in the plot above doesn’t correspond to what we’d expect from NDVI (which should range from -1 to 1). This is because our input file actually contains several measures (“subdatasets”) in addition to NDVI, and we’ve only shown the first. We can use the subds argument to select a particular subdataset from the HDF5 file. (Later, we’ll show how you can use the metadata to find the correct subdataset name.)\nMaking these adjustments, we get:\n\nviirs_cmg &lt;- rast(\n  files, \n  subds = \"//HDFEOS/GRIDS/VIIRS_Grid_monthly_VI_CMG/Data_Fields/CMG_0.05_Deg_monthly_NDVI\",\n  noflip = TRUE\n)\n\n# Don't forget to attach geographic info as we've re-loaded the file!\next(viirs_cmg) &lt;- c(-180, 180, -90, 90)\ncrs(viirs_cmg) &lt;- \"epsg:4326\"\n\nWe now have a georeferenced NDVI raster dataset for 3 months:\n\nviirs_cmg\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 3600, 7200, 3  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; sources     : VNP13C2.A2014001.002.2024060013808.h5://CMG_0.05_Deg_monthly_NDVI  \n#&gt;               VNP13C2.A2014032.002.2024060030203.h5://CMG_0.05_Deg_monthly_NDVI  \n#&gt;               VNP13C2.A2014060.002.2024060044137.h5://CMG_0.05_Deg_monthly_NDVI  \n#&gt; varnames    : CMG_0 \n#&gt;               CMG_0 \n#&gt;               CMG_0 \n#&gt; names       : CMG_0.05_D~nthly_NDVI, CMG_0.05_D~nthly_NDVI, CMG_0.05_D~nthly_NDVI\n\nA quick map of our data shows that it’s now oriented correctly:\n\nplot(viirs_cmg[[1]])\n\n\n\n\n\n\n\n\n\n\nHowever, our NDVI units still appear to be off—we’d expect them to range from -1 to 1, but here we see they appear to be scaled. The VIIRS documentation indicates that the values have been scaled by 10000 and that the valid range of data is from -10000 to 10000.\nWe’ll rescale our data with simple division:\n\n# Rescale data\nviirs_cmg &lt;- viirs_cmg / 10000\n\nThen, we’ll use classify() to convert all raster values below -1 to missing values. classify() expects a matrix that contains an input range (in this case, -Inf to -1) and an output value that should be used as a replacement for values in that range (in this case, NA).\n\n# Reclassify out-of-range data to NA\nm &lt;- matrix(c(-Inf, -1, NA), nrow = 1)\nviirs_cmg &lt;- classify(viirs_cmg, m)\n\nThis looks much better:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(viirs_cmg[[1]])\n\n\n\n\n\n\n\n\n\n\nAt this point, we could load spatial boundary files for a particular region to crop our data and proceed with our analysis. We’ve covered this kind of process previously, so we won’t demonstrate again here. As is often the case, the bulk of the work when introducing a new data product is figuring out how to get it loaded and prepared correctly."
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#hdf5-metadata",
    "href": "posts/2025-03-31-viirs/index.html#hdf5-metadata",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "HDF5 metadata",
    "text": "HDF5 metadata\nIn general, the process for loading and working with higher-resolution files mirrors the process we demonstrated above. However, instead of downloading global data, we’ll download data for a collection of tiles of data in our area of interest.\n\n\n\n\n\n\nTip\n\n\n\nWe showed you how to use Earthdata Search to download data on a tile-by-tile basis in our MODIS post. The only difference here is that we’ve downloaded data for the VNP13A1 product, not the MOD13Q1 product.\n\n\nThere are two key differences when working with VNP13A1 as compared to the global VNP13C2 data described above.\n\nFirst, NASA uses a sinusoidal projection for its high resolution products, not the geographic coordinates used for the global CMG product.\nSecond, because we will be stitching together multiple tiles of data, each will have a different geographic extent.\n\nTo deal with these additional complexities, we’ll need to access the file metadata directly to attempt to identify the correct CRS and extent information.\n\nBioconductor and the rhdf5 package\nFirst, we’ll load a collection of 8 HDF files that cover 2 timestamps for the same 4 tiles that we used in our MODIS post. We’ve placed them in a data/VNP13A1 directory.\n\nfiles &lt;- list.files(\"data/VNP13A1\", full.names = TRUE)\n\nUnfortunately, terra isn’t designed to provide flexible access to metadata for HDF5 files. However, there is another R package that can help, appropriately named {rhdf5}.\nrhdf5 is a package from Bioconductor, an open-source software project focused on building bioinformatics tools. You’re probably familiar with installing packages from CRAN, which is the default behavior when using install.packages(), but Bioconductor packages are not stored on CRAN.\nFortunately, Bioconductor maintains the {BiocManager} package to help install Bioconductor packages. If you’ve never worked with a Bioconductor package before, you can install the manager with\n\ninstall.packages(\"BiocManager\")\n\nThen, you can use it to install rhdf5:\n\nBiocManager::install(\"rhdf5\")\n\nNote that you’ll only need to do this once, unless you later need to update the BiocManager or rhdf5 packages.\n\n\nAccessing metadata\nFirst, we’ll load the {rhdf5} package:\n\nlibrary(rhdf5)\n\nWe can view the available data and metadata fields in an HDF5 file with h5ls().\n\nds &lt;- h5ls(files[1])\n\nHDF5 files are organized into groups. Groups refer to an abstract collection of objects, like datasets or metadata objects. Each data object has a name. You can think of each group as a directory and the objects inside as different files within that directory.\nFor instance, if we view this file’s groups, we notice that there are several entries in the Data Fields group, among others:\n\nds$group\n#&gt;  [1] \"/\"                                                 \n#&gt;  [2] \"/HDFEOS\"                                           \n#&gt;  [3] \"/HDFEOS/ADDITIONAL\"                                \n#&gt;  [4] \"/HDFEOS\"                                           \n#&gt;  [5] \"/HDFEOS/GRIDS\"                                     \n#&gt;  [6] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m\"            \n#&gt;  [7] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt;  [8] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt;  [9] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [10] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [11] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [12] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [13] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [14] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [15] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [16] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [17] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [18] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [19] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [20] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [21] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [22] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [23] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"\n#&gt; [24] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m\"            \n#&gt; [25] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m\"            \n#&gt; [26] \"/\"                                                 \n#&gt; [27] \"/HDFEOS INFORMATION\"                               \n#&gt; [28] \"/HDFEOS INFORMATION\"                               \n#&gt; [29] \"/HDFEOS INFORMATION\"\n\nIf we look at the specific objects within these groups, we see that each of these data fields is a different metric (e.g.  EVI [Enhanced Vegetation Index], NDVI, etc.):\n\npaste(ds$group, ds$name, sep = \"/\")\n#&gt;  [1] \"//HDFEOS\"                                                                                  \n#&gt;  [2] \"/HDFEOS/ADDITIONAL\"                                                                        \n#&gt;  [3] \"/HDFEOS/ADDITIONAL/FILE_ATTRIBUTES\"                                                        \n#&gt;  [4] \"/HDFEOS/GRIDS\"                                                                             \n#&gt;  [5] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m\"                                                    \n#&gt;  [6] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields\"                                        \n#&gt;  [7] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days EVI\"                      \n#&gt;  [8] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days EVI2\"                     \n#&gt;  [9] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days NDVI\"                     \n#&gt; [10] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days NIR reflectance\"          \n#&gt; [11] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days SWIR1 reflectance\"        \n#&gt; [12] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days SWIR2 reflectance\"        \n#&gt; [13] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days SWIR3 reflectance\"        \n#&gt; [14] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days VI Quality\"               \n#&gt; [15] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days blue reflectance\"         \n#&gt; [16] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days composite day of the year\"\n#&gt; [17] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days green reflectance\"        \n#&gt; [18] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days pixel reliability\"        \n#&gt; [19] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days red reflectance\"          \n#&gt; [20] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days relative azimuth angle\"   \n#&gt; [21] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days sun zenith angle\"         \n#&gt; [22] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/500 m 16 days view zenith angle\"        \n#&gt; [23] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data Fields/Projection\"                             \n#&gt; [24] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/XDim\"                                               \n#&gt; [25] \"/HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/YDim\"                                               \n#&gt; [26] \"//HDFEOS INFORMATION\"                                                                      \n#&gt; [27] \"/HDFEOS INFORMATION/ArchiveMetadata.0\"                                                     \n#&gt; [28] \"/HDFEOS INFORMATION/CoreMetadata.0\"                                                        \n#&gt; [29] \"/HDFEOS INFORMATION/StructMetadata.0\"\n\nOf particular interest is the \"500 m 16 days NDVI\" data field, which contains the NDVI data we’ll eventually want to load.\nAlso of note is the \"/HDFEOS INFORMATION/StructMetadata.0\" object. By convention, this is where NASA stores some of the key geographic metadata for the file.\nWe can read the HDF5 metadata using h5read(). The name argument should be set to one of the group and name values from the h5ls() output above.\n\nmetadata &lt;- h5read(files[1], name = \"//HDFEOS INFORMATION/StructMetadata.0\")\n\nYou can use cat() to display the metadata text in the R console, making it a bit easier to read.\n\ncat(metadata)\n#&gt; GROUP=SwathStructure\n#&gt; END_GROUP=SwathStructure\n#&gt; GROUP=GridStructure\n#&gt;  GROUP=GRID_1\n#&gt;      GridName=\"VIIRS_Grid_16Day_VI_500m\"\n#&gt;      XDim=2400\n#&gt;      YDim=2400\n#&gt;      UpperLeftPointMtrs=(3335851.559000,1111950.519667)\n#&gt;      LowerRightMtrs=(4447802.078667,0.000000)\n#&gt;      Projection=HE5_GCTP_SNSOID\n#&gt;      ProjParams=(6371007.181000,0,0,0,0,0,0,0,0,0,0,0,0)\n#&gt;      SphereCode=-1\n#&gt;      GROUP=Dimension\n#&gt;          OBJECT=Dimension_1\n#&gt;              DimensionName=\"dimofone\"\n....\n\n\nProjection metadata\nNotice the Projection and ProjParams rows in the output above. These show details about the projection and projection parameters for our data. On their own, they’re hard to interpret, but a quick look at the VIIRS documentation reveals that this is the NASA code for the sinusoidal projection.\nWe can use a PROJ-string to represent this projection. This will allow us to assign the appropriate projection when we later load our data.\n\n\nPROJ is a set of software tools that support transformations between coordinate reference systems. PROJ-strings are one way to represent the parameters of a specific projection in a way that PROJ can interpret.\nFor the sinusoidal projection, we can use the following:\n\nsinu_proj &lt;- \"+proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs\"\n\n\n\n\n\n\n\nNote\n\n\n\nNote that this is the projection that NASA uses for all its tiled VIIRS (and MODIS) products. Even if you’re working with a different tile, you can use the same projection string to set the CRS of the raster you’re working with.\n\n\n\n\nExtent metadata\nThe UpperLeftPointMtrs and LowerRightMtrs metadata rows show the locations of the upper left and lower right points of the raster grid in the sinusoidal projection.\n\n....\n#&gt;      UpperLeftPointMtrs=(3335851.559000,1111950.519667)\n#&gt;      LowerRightMtrs=(4447802.078667,0.000000)\n....\n\nFrom these, we can determine the extent of our raster grid. To simplify things, we’ll just hard-code these points in this demo:\n\n\nYou could also use regular expressions to extract the numeric values themselves. We use this approach in our iterative workflow later in the post.\n\nul &lt;- c(3335851.559, 1111950.519667)\nlr &lt;- c(4447802.078667, 0)\n\n{terra} expects the extent to be in (xmin, xmax, ymin, ymax) order. In our case, this corresponds to (upper-left x-coordinate, lower-right x-coordinate, lower-right y-coordinate, upper-left y-coordinate).\nWe’ll use terra’s ext() to create a SpatExtent object containing the extent:\n\n# Using (xmin, xmax, ymin, ymax) order\nextent &lt;- ext(ul[1], lr[1], lr[2], ul[2])\n\nextent\n#&gt; SpatExtent : 3335851.559, 4447802.078667, 0, 1111950.519667 (xmin, xmax, ymin, ymax)\n\n\n\n\n\n\n\nNote\n\n\n\nNote that when working with multiple tiles, you’ll need to get the extent information individually for each tile, as each tile contains data for a different geographic area.\nWe’ll show one way to deal with this below, where we update the iterative workflow we demonstrated in our MODIS post to work with VIIRS files.\n\n\n\n\n\nUsing metadata\nWe can use the metadata we’ve explored to improve our data loading workflow and attach correct geographic information to the loaded data.\nFirst, we’ll use the NDVI subdataset name we identified above to load just the NDVI raster, ignoring other bands:\n\nviirs_ndvi &lt;- rast(\n  files[1], \n  subds = \"//HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data_Fields/500_m_16_days_NDVI\",\n  noflip = TRUE\n)\n\nNow, we can assign our sinusoidal projection to our data:\n\ncrs(viirs_ndvi) &lt;- sinu_proj\n\nAs well as the extent we created above:\n\next(viirs_ndvi) &lt;- extent\n\nFinally, we’ll make the scale adjustments that we introduced when working with the global CMG data:\n\n# Rescale NDVI values\nviirs_ndvi &lt;- viirs_ndvi / 10000\n\n# Replace out-of-range values with NA\nm &lt;- matrix(c(-Inf, -1, NA), nrow = 1)\nviirs_ndvi &lt;- classify(viirs_ndvi, m)\n\nNow we can see that our raster seems to have the appropriate CRS, extent, and scale!\n\nviirs_ndvi\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 2400, 2400, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 463.3127, 463.3127  (x, y)\n#&gt; extent      : 3335852, 4447802, 0, 1111951  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#&gt; source(s)   : memory\n#&gt; varname     : 500_m_16_days_NDVI \n#&gt; name        : 500_m_16_days_NDVI \n#&gt; min value   :            -0.9688 \n#&gt; max value   :             0.9994\n\n\n\nShow plot code\nndvi_pal &lt;- list(\n  pal = c(\n    \"#fdfbdc\",\n    \"#f1f4b7\",\n    \"#d3ef9f\",\n    \"#a5da8d\",\n    \"#6cc275\",\n    \"#51a55b\",\n    \"#397e43\",\n    \"#2d673a\",\n    \"#1d472e\" \n  ),\n  values = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1)\n)\n\nggplot() + \n  layer_spatial(viirs_ndvi) +\n  coord_sf(datum = sf::st_crs(viirs_ndvi)) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    title = \"VIIRS NDVI\", \n    subtitle = \"Tile h21v08, January 1-16, 2014\", \n    fill = \"NDVI\",\n    caption = \"Source: NASA Visible Infrared Imaging Radiometer Suite (VIIRS)\"\n  ) +\n  theme_dhs_map() +\n  theme_dhs_base()"
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#identify-tile-codes",
    "href": "posts/2025-03-31-viirs/index.html#identify-tile-codes",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "Identify tile codes",
    "text": "Identify tile codes\nAs we demonstrated last time, we’ll identify each unique tile code from our file names:\n\ntile_codes &lt;- unique(str_extract(files, \"h[0-9]{2}v[0-9]{2}\"))\n\ntile_codes\n#&gt; [1] \"h21v08\" \"h21v09\" \"h22v08\" \"h22v09\"\n\nThis allows us to group our files by tile into a list:\n\ntiles &lt;- map(\n  tile_codes,\n  function(code) files[str_detect(files, code)]\n)\n\ntiles\n#&gt; [[1]]\n#&gt; [1] \"data/VNP13A1/VNP13A1.A2014001.h21v08.002.2024059234751.h5\"\n#&gt; [2] \"data/VNP13A1/VNP13A1.A2014017.h21v08.002.2024060002117.h5\"\n#&gt; \n#&gt; [[2]]\n#&gt; [1] \"data/VNP13A1/VNP13A1.A2014001.h21v09.002.2024059234749.h5\"\n#&gt; [2] \"data/VNP13A1/VNP13A1.A2014017.h21v09.002.2024060002133.h5\"\n#&gt; \n#&gt; [[3]]\n#&gt; [1] \"data/VNP13A1/VNP13A1.A2014001.h22v08.002.2024059234752.h5\"\n#&gt; [2] \"data/VNP13A1/VNP13A1.A2014017.h22v08.002.2024060002157.h5\"\n#&gt; \n#&gt; [[4]]\n#&gt; [1] \"data/VNP13A1/VNP13A1.A2014001.h22v09.002.2024059234755.h5\"\n#&gt; [2] \"data/VNP13A1/VNP13A1.A2014017.h22v09.002.2024060002123.h5\""
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#using-string-matching-to-identify-tile-extent-coordinates",
    "href": "posts/2025-03-31-viirs/index.html#using-string-matching-to-identify-tile-extent-coordinates",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "Using string matching to identify tile extent coordinates",
    "text": "Using string matching to identify tile extent coordinates\nNow, for each of these tiles, we can extract the metadata for the first file using h5read(). (Recall that the two files in each group are differentiated by their timestamps, but share the same geographic extent.)\n\ntile_metadata &lt;- map(\n  tiles,\n  function(t) h5read(t[1], name = \"//HDFEOS INFORMATION/StructMetadata.0\")\n)\n\nThe metadata are in string format. To extract the upper left and lower right coordinates, we’ll need to use regular expressions to pull out the text coordinates and convert to numeric values.\n\nul &lt;- map(\n  tile_metadata,\n  ~ str_match(.x, \"UpperLeftPointMtrs=\\\\((.*?)\\\\)\")[, 2]\n)\n\nul &lt;- map(str_split(ul, \",\"), as.numeric)\n\nul\n#&gt; [[1]]\n#&gt; [1] 3335852 1111951\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 3335852       0\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 4447802 1111951\n#&gt; \n#&gt; [[4]]\n#&gt; [1] 4447802       0\n\n\nlr &lt;- map_chr(\n  tile_metadata,\n  ~ str_match(.x, \"LowerRightMtrs=\\\\((.*?)\\\\)\")[, 2]\n)\n\nlr &lt;- map(str_split(lr, \",\"), as.numeric)\n\nlr\n#&gt; [[1]]\n#&gt; [1] 4447802       0\n#&gt; \n#&gt; [[2]]\n#&gt; [1]  4447802 -1111951\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 5559753       0\n#&gt; \n#&gt; [[4]]\n#&gt; [1]  5559753 -1111951\n\nNow we can convert these points to extents using terra’s conventions. We use map2() to iterate over both sets of points in parallel. That is, we’ll use the first point in the ul list along with the first point in the lr list to construct the first extent, and so on.\n\ntile_ext &lt;- map2(ul, lr, function(u, l) ext(u[1], l[1], l[2], u[2]))\n\ntile_ext\n#&gt; [[1]]\n#&gt; SpatExtent : 3335851.559, 4447802.078667, 0, 1111950.519667 (xmin, xmax, ymin, ymax)\n#&gt; \n#&gt; [[2]]\n#&gt; SpatExtent : 3335851.559, 4447802.078667, -1111950.519667, 0 (xmin, xmax, ymin, ymax)\n#&gt; \n#&gt; [[3]]\n#&gt; SpatExtent : 4447802.078667, 5559752.598333, 0, 1111950.519667 (xmin, xmax, ymin, ymax)\n#&gt; \n#&gt; [[4]]\n#&gt; SpatExtent : 4447802.078667, 5559752.598333, -1111950.519667, 0 (xmin, xmax, ymin, ymax)"
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#load-all-tiles",
    "href": "posts/2025-03-31-viirs/index.html#load-all-tiles",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "Load all tiles",
    "text": "Load all tiles\nNow, we need to load the data for each tile, updating its CRS and extent information with the corresponding extent for that tile. Since we need to iterate over both tiles and extents, we’ll again use map2(). This will load the first tile and apply the first extent, then the second tile with the second extent, and so on.\n\nviirs_tiles &lt;- map2(\n  tiles,\n  tile_ext,\n  function(tile, ext) {\n    # Load raster for the input tile. We select the NDVI subdataset\n    r &lt;- rast(\n      tile, \n      subds = \"//HDFEOS/GRIDS/VIIRS_Grid_16Day_VI_500m/Data_Fields/500_m_16_days_NDVI\",\n      noflip = TRUE\n    )\n    \n    crs(r) &lt;- sinu_proj # Attach sinusoidal projection defined above\n    ext(r) &lt;- ext # Attach this tile's extent\n    \n    r # Return the updated raster for this tile\n  }\n)"
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#mosaic-georeferenced-tiles",
    "href": "posts/2025-03-31-viirs/index.html#mosaic-georeferenced-tiles",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "Mosaic georeferenced tiles",
    "text": "Mosaic georeferenced tiles\nAs we did in our MODIS post, we’ll now mosaic all the tiles together into a single source:\n\nviirs_mosaic &lt;- reduce(viirs_tiles, mosaic)\n\nAnd take care of the NDVI adjustments we described above.\n\nviirs_mosaic &lt;- viirs_mosaic / 10000\n\nm &lt;- matrix(c(-Inf, -1, NA), nrow = 1)\nviirs_mosaic &lt;- classify(viirs_mosaic, m)\n\n\n\nShow plot code\nlibrary(sf)\nlibrary(patchwork)\n\nke_borders &lt;- ipumsr::read_ipums_sf(\"data/geo_ke1989_2014.zip\") |&gt; \n  st_make_valid() |&gt; # Fix minor border inconsistencies\n  st_union() |&gt; \n  st_transform(crs(viirs_mosaic))\n\nke_viirs &lt;- crop(viirs_mosaic, ke_borders)\nke_viirs_mask &lt;- mask(ke_viirs, vect(ke_borders))\n\np1 &lt;- ggplot() + \n  layer_spatial(ke_viirs_mask[[1]]) +\n  layer_spatial(st_simplify(ke_borders, dTolerance = 1000), fill = NA) +\n  coord_sf(datum = sf::st_crs(viirs_ndvi)) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(subtitle = \"January 1-16, 2014\", fill = \"NDVI\") +\n  theme_dhs_map() +\n  theme_dhs_base()\n\np2 &lt;- ggplot() + \n  layer_spatial(ke_viirs_mask[[2]]) +\n  layer_spatial(st_simplify(ke_borders, dTolerance = 1000), fill = NA) +\n  coord_sf(datum = sf::st_crs(viirs_ndvi)) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(subtitle = \"January 17-31, 2014\", fill = \"NDVI\") +\n  theme_dhs_map() +\n  theme_dhs_base()\n\np1 + p2 +\n  plot_layout(guides = \"collect\", ncol = 2) +\n  plot_annotation(\n    title = \"NDVI: Kenya\",\n    caption = \"Source: NASA Visible Infrared Imaging Radiometer Suite (VIIRS)\"\n  ) &\n  theme(legend.position='bottom')\n\n\n\n\n\n\n\n\n\nFortunately, you can avoid most of this processing when working with global data, as the data have a simpler CRS and are already whole. While you’re likely to want to use global CMG data for most purposes, it’s worthwhile to be aware of some of the quirks required for handling VIIRS files when working with multiple tiles. That being said, resist the temptation to always use high-resolution data! Always assess the spatial resolution of your other data sources to determine when it may be more appropriate to rely on the simpler global VIIRS data."
  },
  {
    "objectID": "posts/2025-03-31-viirs/index.html#getting-help",
    "href": "posts/2025-03-31-viirs/index.html#getting-help",
    "title": "From MODIS to VIIRS: The Latest Source for NDVI Data",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html",
    "href": "posts/2025-06-03-forecasting-pt2/index.html",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "",
    "text": "The second blog post in our series describes how to use R to estimate fertility rates and to project a population. Understanding the rate of a population process (e.g., fertility) is a key consideration when thinking about the future. A population projection, on the other hand is the result of predicting a future population level."
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html#fertility-rates",
    "href": "posts/2025-06-03-forecasting-pt2/index.html#fertility-rates",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "Fertility rates",
    "text": "Fertility rates\nFertility is a key population process that influences future population size. A fertility rate simply summarizes the number of children relative to an overall population. Most commonly, fertility rates summarize the number of children per woman in the population. The fertility rate of a given population is the result of various factors that influence people’s fertility decision making. The fertility rate of nation can influence the dependency-ratio, the ratio of working age adults to children and retirees, which has implications for a nation’s social security system.\nIt is important to note that fertility is not the only population process that influences population size: migration and mortality do, too. However, this blog post focuses on fertility, and the first workflow demonstrated will walk you through how to use a nationally representative survey to estimate various fertility rates."
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html#the-general-fertility-rate-gfr",
    "href": "posts/2025-06-03-forecasting-pt2/index.html#the-general-fertility-rate-gfr",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "The general fertility rate (GFR)",
    "text": "The general fertility rate (GFR)\nThe GFR is calculated using the following equation:\n\nGFR=\\frac{\\text{Number of live births in reference period}}{\\text{Number of women aged 15-44 in reference period}}\n\nThat is, the GFR is the total number of births during an interval of time (the reference period) divided by the total number of women-years of exposure during that same reference period. The women under consideration are all women in their reproductive years—here taken as those women between the ages of 15 and 44.2\n\n\n\n\n\n\nNote\n\n\n\nTechincally, all women between the ages of 15 and 49 are eligible to complete the women’s interview. However, because fertility declines sharply after age 44, including these women in the denominator of the GFR may serve to reduce the fertility rate for the overall population, potentially giving a misleading result. This is why the GFR is typically calculated only for women up to age 44.\nIn some of the metrics we introduce later, fertility is split out by age, so all women up to age 49 are included in those calculations.\n\n\nYou can use the fert() function from DHS.rates to calculate the GFR by setting Indicator = \"gfr\". This function will identify the reference period and print out the standard error (SE), the number of observations (N), the weighted number of observations (WN), the standard error design effect (DEFT), the relative standard error (RSE), and the lower and upper confidence interval bounds (LCI and UCI).\n\n# use the fert function to calculate various fertilty rates\n\n# General Fertility Rate\ngeneral_fertility_rate &lt;- fert(\n  kenya22,\n  Indicator = \"gfr\"\n)\n#&gt; \n#&gt;  The current function calculated GFR based on a reference period of 36 months \n#&gt;  The reference period ended at the time of the interview, in 2022.33 OR Feb - Jul 2022 \n#&gt;  The average reference period is 2020.83\n\ngeneral_fertility_rate\n#&gt;          GFR    SE     N    WN  DEFT   RSE     LCI     UCI\n#&gt; [1,] 121.266 1.634 85634 86313 1.496 0.013 118.062 124.469\n\nThe GFR for this sample is just over 121 children per woman, meaning that from 2020-2022 the general fertility rate in Kenya was 121.27 children per women aged 15-49.\n\n\n\n\n\n\nTip\n\n\n\nfert() uses a default reference period of 36 months. Use the Period and PeriodEnd arguments to set your own reference period. See the DHS.rates documentation for details about the available options."
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html#the-age-specific-fertility-rate-asfr",
    "href": "posts/2025-06-03-forecasting-pt2/index.html#the-age-specific-fertility-rate-asfr",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "The age-specific fertility rate (ASFR)",
    "text": "The age-specific fertility rate (ASFR)\nThe ASFR, similar to the GFR, summarizes the number of children being born during a time interval relative to the number of women-years of exposure, but it does so within specific age groups. The typical grouping strategy for this is to break up age into five year categories (15-19, 20-24, 25-29, 30-34, 35-39, 40-44, and 45-49).2\nThe ASFR uses the following equation:\n\n\nASFR = \\frac{(\\text{Number of live births to women age } a) - (\\text{Number of live births to women age } a+4)}{\\text{Women-years during the reference period}  } * 1000\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe ASFR is multiplied by 1,000 because it is interpreted as per 1,000 women.\n\n\nTo calculate the ASFR with DHS.rates, we again use fert(), but this time we set Indicator = \"asfr\":\n\n# Age Specific Fertility Rate\nage_specific_fertility_rate &lt;- fert(\n  kenya22,\n  Indicator = \"asfr\"\n)\n#&gt; \n#&gt;  The current function calculated ASFR based on a reference period of 36 months \n#&gt;  The reference period ended at the time of the interview, in 2022.33 OR Feb - Jul 2022 \n#&gt;  The average reference period is 2020.83\n\nage_specific_fertility_rate\n#&gt;     AGE    ASFR    SE     N    WN  DEFT   RSE     LCI     UCI\n#&gt; 0 15-19  73.169 2.573 18693 18333 1.356 0.035  68.127  78.211\n#&gt; 1 20-24 178.609 4.220 16996 17654 1.523 0.024 170.338 186.879\n#&gt; 2 25-29 172.145 4.384 15357 15695 1.529 0.025 163.553 180.737\n#&gt; 3 30-34 137.389 4.212 14167 14146 1.538 0.031 129.133 145.646\n#&gt; 4 35-39  86.690 3.623 11282 11325 1.369 0.042  79.589  93.792\n#&gt; 5 40-44  34.877 2.764  9139  9158 1.401 0.079  29.459  40.294\n#&gt; 6 45-49   5.314 1.056  4850  4806 1.008 0.199   3.244   7.385\n\nWomen aged 20-24 have the highest ASFR, at 178.61 children per woman, which is consistent with the literature on fertility in sub-Saharan Africa.3\n\n\nPer our discussion earlier, the ASFR does include women between 44-49, but notice how this age category has far fewer births than any other category.\nNow, we will use the ASFR information we have calculated to generate the most commonly referenced fertility rate, the TFR."
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html#the-total-fertility-rate-tfr",
    "href": "posts/2025-06-03-forecasting-pt2/index.html#the-total-fertility-rate-tfr",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "The total fertility rate (TFR)",
    "text": "The total fertility rate (TFR)\nThe TFR is a synthetic measure that summarizes the ASFRs into one age-standardized rate. A TFR can be interpreted as the number of children who would be born per woman if she were to have children according to each age-specific rate throughout her reproductive years. This is summarized in the following equation:\n\nTFR= 5 * \\frac{\\text{sum of each ASFR}}{1000}\n\n\n\n\n\n\n\nNote\n\n\n\nThe TFR is multiplied by 5 because the ASFRs are in five-year intervals.\n\n\nAgain, use the Indicator argument to calculate the total fertility rate with fert():\n\n# Total Fertility Rate\ntotal_fertility_rate &lt;- fert(\n  kenya22,\n  Indicator = \"tfr\",\n)\n#&gt; \n#&gt;  The current function calculated TFR based on a reference period of 36 months \n#&gt;  The reference period ended at the time of the interview, in 2022.33 OR Feb - Jul 2022 \n#&gt;  The average reference period is 2020.83\n\ntotal_fertility_rate\n#&gt;        TFR     N    WN\n#&gt; [1,] 3.441 90484 91119\n\nFertility around the world is in decline, even in high fertility countries such as Kenya. This TFR of 3.4 children per woman is lower than the previous TFR from the 2014 DHS survey of 3.9 children per woman."
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html#comparing-rates-across-categories",
    "href": "posts/2025-06-03-forecasting-pt2/index.html#comparing-rates-across-categories",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "Comparing rates across categories",
    "text": "Comparing rates across categories\nNow let’s consider the TFR at different domain levels. The class argument in the fert() function allows you to calculate rates based on certain characteristics.\nFor instance, if we wanted to compare the fertility rate of women who live in urban areas to those who live in rural areas, we can provide the name of the urban/rural variable in our data—in this case, v025:\n\n# Let's compare the urban and rural total fertility rate\ntotal_fertility_rate_urban_vs_rural &lt;- fert(\n  kenya22,\n  Indicator = \"tfr\",\n  Class = \"v025\"\n)\n#&gt; \n#&gt;  The current function calculated TFR based on a reference period of 36 months \n#&gt;  The reference period ended at the time of the interview, in 2022.33 OR Feb - Jul 2022 \n#&gt;  The average reference period is 2020.83\n\ntotal_fertility_rate_urban_vs_rural\n#&gt;   Class   TFR     N    WN\n#&gt; 1     1 2.842 35442 38137\n#&gt; 2     2 3.941 55042 52982\n\nWe can see that we now have two fertility rates in our output, one for class 1 and another for class 2. These correspond to the variable codes used to distinguish urban and rural categories in our data. The DHS codebook tells us that 1 corresponds to urban records and 2 corresponds to rural records.\nIn this sample, the difference between fertility rates across urban and rural areas of is almost one child, which is in line with literature on fertility in sub-Saharan Africa.4,5"
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html#the-hamilton-perry-method",
    "href": "posts/2025-06-03-forecasting-pt2/index.html#the-hamilton-perry-method",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "The Hamilton-Perry method",
    "text": "The Hamilton-Perry method\nFor our second workflow we use the 2001, 2006, and 2011 women’s files in Uganda to project the total population in 2016. We then compare our population projection to the DHS estimate from the 2016 survey. We source this data from IPUMS DHS.\n\n# load libraries\nlibrary(ipumsr)\nlibrary(stats)\nlibrary(dplyr)\n\n\n# read in data extract from IPUMS DHS\nuganda_dhs &lt;- read_ipums_micro(\"data/idhs_00004.xml\")\n#&gt; Use of data from IPUMS DHS is subject to conditions including that users should cite the data appropriately. Use command `ipums_conditions()` for more details.\n\nWe will use the aggregate() function from base R to sum the POPWT variable to estimate the total population of women in their reproductive years (aged 15-49).\n\nuganda_pop &lt;- aggregate(\n  uganda_dhs$POPWT, \n  by = list(uganda_dhs$YEAR), \n  FUN = \"sum\"\n)\n\nuganda_pop\n#&gt;   Group.1       x\n#&gt; 1    1988 3610171\n#&gt; 2    1995 4478497\n#&gt; 3    2001 5309989\n#&gt; 4    2006 6428314\n#&gt; 5    2011 7763897\n#&gt; 6    2016 9407944\n\nIn the Hamilton-Perry method of population projection you need at least two different observations of a population estimate. When working with more than two observations of population estimates, It is important to keep the interval between years the same. We will use 2001, 2006 and 2011 to project 2016’s population. We will then compare our projeciton to the DHS estimation for 2016 and other published estimates.\n\n# calculate the cohort-change ratio\n\n# from 2001-2006\nccr_2006 &lt;- 6428314 / 5309989\n\n# from 2006-2011\nccr_2011 &lt;- 7763897 / 6428314\n\n# take the average of the two\nccr_mean &lt;- mean(ccr_2006, ccr_2011)\n\n# multiply the average by the population in 2011\npop_est &lt;- ccr_mean * 7763897\n\npop_est\n#&gt; [1] 9399034\n\nThis method estimates that 9,399,034 women in their reproductive years were living in Uganda in 2016. The DHS estimation for this population in 2016 is slightly higher at 9,407,944. Comparing these values can give us an idea of the magnitude of this difference:\n\n# difference between our population projection and the DHS 2016 estimate\n9407944 - pop_est\n#&gt; [1] 8909.882\n\n# calculate the percent difference between the two\n((pop_est - 9407944) / 9407944) * 100\n#&gt; [1] -0.09470594\n\nOur estimate was off by just under 9,000 women. While this number may seem relatively large, the percent difference is less than 1%!"
  },
  {
    "objectID": "posts/2025-06-03-forecasting-pt2/index.html#other-methods",
    "href": "posts/2025-06-03-forecasting-pt2/index.html#other-methods",
    "title": "Estimating the now and predicting the future: Fertility rate estimation and population projection",
    "section": "Other methods",
    "text": "Other methods\n\nCohort-component method\nAnother example of projecting a population is called the cohort component method. The cohort component method utilizes two important demographic concepts: the life table and the demographic equation of population change. This method is considered as the most comprehensive method of population projection, as it essentially maps out the basic demographic equation of population change:7\n\n\n(\\text{population at time } t+1) + (\\text{births at time } t) - (\\text{deaths at time } t) + \\\\ (\\text{in-migration at time } t) - (\\text{out-migration at time } t)\n\n\nThe first part of this blog post demonstrated how to generate various fertility rates. If we had access to data on the other population processes, such as the rate of mortality and migration, we would be able to use the cohort-component method.\n\n\nAnother source of population estimates for middle- and low-income countries is the United Nations Population Division.\n\n\nMathematical model approaches\nAnother method of population projection involves fitting a mathematical model to population data. This approach does not disaggregate data by age and sex, but rather considers the total population.8\nSome examples of the types of models used in this approach are linear, logistic, and exponential. In a linear approach, signified by this equation: Y=a+bX, the slope b would be an indicator of population growth.8 In this equation Y represents the population, X represents time (usually in years), and a the intercept.\nBoth a linear approach and the cohort-change ratio assume that the rate of change within a population is constant. In real life, however, these rates are not constant, as there are period effects, which are caused by events that influence people at a certain point in time.\nA mathematical approach that does not assume a constant rate over time is called the Gompertz model. This model uses an asymptote to indicate an upper limit for population size. The Gompertz model uses the following equation to project a population:\n\nlog(Y) = log(k) + log(a)bx\n\nIn this equation Y represents the population, X is a measure of time, k is an asymptote setting a limit for the population, and a and b are parameters of the population.8"
  },
  {
    "objectID": "posts/2025-09-05-forecasting-pt3/index.html",
    "href": "posts/2025-09-05-forecasting-pt3/index.html",
    "title": "Harnessing CHC-CMIP6 Climate Scenario Data to Explore the Future",
    "section": "",
    "text": "We now return to our series on using methods of future estimation in spatial health research. The third post in this series introduces CHC-CMIP6 data by the Climate Hazards Center at UC Santa Barbara. We will demonstrate how you can read these data into R to create informative visualizations.  Kenya will be our example context, continuing from our previous post in this series\n\nLong-term Climate Projections\nNow that we’ve shown how we can use forecasts, predictions and scenarios to explore future demographic and health patterns, how does climate change fit in?\nWe discussed Subseasonal to seasonal (S2S) forecasts earlier in this series, which extend short-term weather forecasts from two weeks to two years into the future. Climate change patterns, however, may shift over many years, and often, climate researchers are predicting much further into the future. These predictions can assist policymakers in thinking about how what they do today may impact the future.\nThe Intergovernmental Panel on Climate Change (IPCC) is the UN agency that studies climate change, and its annual Conference of Parties (COP) meetings bring together experts in climate change, agriculture, demography, health and other fields to help governments make pledges to increase resources to address climate change. IPCC synthesizes global climate research into comprehensive, authoritative IPCC reports that represent the scientific consensus on climate change, its impacts, and how human activities affect greenhouse gas emissions, providing the foundation for climate policy decisions.\nTo understand how climate change may shift in the long-term, the IPCC and researchers around the world use the Shared Socioeconomic Pathways (SSPs) to understand our possible future worlds. For example, SSP 5-8.5 assumes a world where we fail to cut emissions or switch to clean energy, the population grows, and emissions increase rapidly. SSP 2-4.5 is considered the “middle of the road” scenario, a future where greenhouse gas emissions hover around current levels and perhaps start declining but don’t reach net-zero by 2100. The SSP’s consider challenges to mitigation (bringing down emissions) and adaptation (taking steps to reduce exposure and harm). Using the SSPs, scientists can explore how climate and demographic patterns will shift under each possible future scenario.\nThe figure below summarizes each single SSP, which is considered the starting point. Read more at the link below the figure to understand the different kinds of assumptions applied to each SSP (i.e. the-4.5 in SSP 2-4.5).\n\n\nFigure: The five SSPs and the associated challenges to mitigation and adaptation. (https://climatedata.ca/resource/understanding-shared-socio-economic-pathways-ssps/)\n\n\n\nThe Couple Model Intercomparison Project Phase 6 (CMIP-6)\nThe Coupled Model Intercomparison Project Phase 6 (CMIP6) is a global collaboration among climate modeling centers. CMIP6 simulates the earth’s climate system, including the atmosphere, oceans and land surface, across each SSP. Each model starts by running historical data from the past 100-150 years, to see if they can reproduce past climate trends. Then, they feed the model different possible futures through the SSPs. Lastly, the models simulate the future climate, often up to the year 2100 or even 2300, and simulate how temperatures may rise, rainfall patterns may change, or extreme events may increase. Because many research teams around the world are running these models independently, CMIP6 can explore dozens of different models using the same standardized scenarios for comparability.\nOnce the simulations are done, scientists can compare the models, average the results, and assess uncertainty levels. The final CMIP6-informed climate projections can be used to guide IPCC reports and guidance, as well as country level national determined contributions (NDCs), adaptation plans, and risk assessments, to guide policy decisions on the ground. To be useful at this level, we need spatially and temporally detailed datasets, that downscale the full model down to more localized estimates and patterns.\n\n\n\n\n\n\nWhat is downscaling?\n\n\n\nCMIP6 projections are global, and for use at the national or subnational level are often too coarse.  Downscaling methods can be used to create finer, more locally relevant climate information to capture regional trends and be useful for more local policy. Downscaling can be done using statistical methods (such as bias correction and spatial disaggregation) or dynamic approaches (such as regional climate models). The result is a high-resolution climate projection that can be used by researchers and policymakers.\nCHC researchers downscaled the CMIP6 from global to more local levels, if you are interested in their methodology please read this paper.\n\n\n\n\nWe will be using a dataset created by researchers at the Climate Hazards Center (CHC). We used observed climate data from CHC in these earlier posts: 1) Flexible Workflows with CHIRTS Temperature Data and 2) Attaching CHIRPS Precipitation Data to DHS Surveys. The format of the projected data is the same as the historical data. This is because the projected data is calculated by adding a daily delta to observed historical data, which is the amount of ‘change’ a climate value is expected to experience.\n\n\nCHC-CMIP6 Datasets\nThe Climate Hazards Center uses CMIP6 outputs and tailors them for more applied research needs. They recently downscaled CMIP6 climate projections to produce high-resolution, bias-corrected datasets that are more useful for local applications. They produced daily 0.05ᵒ gridded data of the Climate Hazards InfraRed Temperature with Stations (CHIRTS) temperature product. So far, CHIRTS has been a historical dataset with daily maximum temperature estimates from 1981-2024, but now we can explore future temperature estimates for 2030 and 2050.\n\n\nWe can use CHC-CMIP data to predict child health outcomes. As this blog has demonstrated, the Demographic and Health Surveys have important, nationally representative information on child health and well-being from countries that often lack detailed health metrics. The DHS includes GPS coordinates for communities where households are sampled, and this information can then be used to link with CHC-CMIP6 downscaled projections by location. We will explore this in our next post in this series.\n\n\nHow to Access and Read the Scenario Data into R\nFor this exercise we will work with temperature data for one month, January, which is during the hot season in Kenya for each scenario and time-frame. If you’re interested in working with rainfall data, you can adapt this code, using the previous CHIRPS post (see above) to download projected rainfall data.\n\n\n\n\n\n\nNote\n\n\n\nYou can find the Tmax files for 2030 for 2-4.5 here and 5-8.5 here. You can find the Tmax files for 2050 for 2-4.5 here and 5-8.5 here. Note: both the projections for 2030 and 2050 come from the files labeled for the year of 2016. As we mentioned earlier, the projection data is calculated by adding a daily delta to each observation.\n\n\nOnce those files are downloaded we will create a file path to .tif files.\n\n# Create list of paths to all individual raster files\n\nprojections_files_245_2030&lt;- list.files(\"data/SSP_245_2030_Tmax\", full.names = TRUE)\n\nprojections_files_585_2030&lt;- list.files(\"data/SSP_585_2030_Tmax\", full.names = TRUE)\n\nprojections_files_245_2050&lt;- list.files(\"data/SSP_245_2050_Tmax\", full.names = TRUE)\n\nprojections_files_585_2050&lt;- list.files(\"data/SSP_585_2050_Tmax\", full.names = TRUE)\n\nThen we will load the files as multiple layers in a raster stack, similar to the workflow demonstrated in this earlier post on how to use precipitation data from the Climate Hazards Center.\n\nlibrary(terra)\n\n# Load each set of .tif files into into its own single raster stack\n\nproj_temp_raster_245_2030 &lt;- rast(projections_files_245_2030)\n\nproj_temp_raster_585_2030 &lt;- rast(projections_files_585_2030)\n\nproj_temp_raster_245_2050 &lt;- rast(projections_files_245_2050)\n\nproj_temp_raster_585_2050 &lt;- rast(projections_files_585_2050)\n\nThe scenario data is provided at the global level, so first we must crop this raster to only cover Kenya. We can do this by using a shapefile for Kenya available on from the Humanitarian Data Exchange, which also has county-level geocodes. First, we read in this shapefile.\n\nlibrary(sf)\n\n#read in Kenya ADM1 shapefiles\n\nken_ADM1_borders &lt;- st_read(\n  \"data/geoBoundaries-KEN-ADM1-all/geoBoundaries-KEN-ADM1.shp\",\n  quiet = TRUE)\n\n#make sure the projection systems for our shapefiles and our raster data match\n\ncrs(ken_ADM1_borders)\n#&gt; [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2296)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\ncrs(proj_temp_raster_245_2030)\n#&gt; [1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2296)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\nThen we crop each raster according to the boundaries of the said shapefile.\n\n#crop each raster so we only have the Tmax information for Kenya\n\n#2-4.5-2030\n\nken_Tmax_245_2030&lt;- crop(proj_temp_raster_245_2030, ken_ADM1_borders, snap = \"out\")\n\n#5-8.5- 2030\n\nken_Tmax_585_2030&lt;- crop(proj_temp_raster_585_2030, ken_ADM1_borders, snap = \"out\")\n\n#2-4.5-2050\n\nken_Tmax_245_2050&lt;- crop(proj_temp_raster_245_2050, ken_ADM1_borders, snap = \"out\")\n\n#5-8.5- 2050\n\nken_Tmax_585_2050&lt;- crop(proj_temp_raster_585_2050, ken_ADM1_borders, snap = \"out\")\n\nNow we have cropped our data scenario data to Kenya. If we enter the name of one of the cropped objects we have created you can see the max temperature is 44.85 degrees Celsius.\n\nken_Tmax_245_2030\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 204, 161, 31  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : 33.9, 41.95, -4.750001, 5.449999  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; source(s)   : memory\n#&gt; names       : 2030_~01.01, 2030_~01.02, 2030_~01.03, 2030_~01.04, 2030_~01.05, 2030_~01.06, ... \n#&gt; min values  : -9999.00000, -9999.00000, -9999.00000, -9999.00000, -9999.00000, -9999.00000, ... \n#&gt; max values  :    44.42629,    42.74142,    43.23469,    41.90297,    41.62671,    43.20387, ...\n\nOur minimum value is listed at -9999, indicating missing information. We will need to drop these values by labeling any negative values as NA’s.\n\n# Set pixels with a value less than 0 to NA\n  \nken_Tmax_245_2030[ken_Tmax_245_2030 &lt; 0] &lt;- NA\n\nken_Tmax_245_2050[ken_Tmax_245_2050 &lt; 0] &lt;- NA\n\nken_Tmax_585_2030[ken_Tmax_585_2030 &lt; 0] &lt;- NA\n\nken_Tmax_585_2050[ken_Tmax_585_2050 &lt; 0] &lt;- NA\n\n\n\nHow to Temporally Aggregate the Data\nNow we must temporally aggregate our data. First we will manually add date information to our data since the metadata does not include date information. See how we get NA’s when we use the following code.\n\ntime(ken_Tmax_245_2030)\n#&gt;  [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n#&gt; [26] NA NA NA NA NA NA\n\nFortunately, the original .tif files are labeled by their date, so we can manually input the date information using the following code.\n\n\n#temporally aggregate the scenario data\n\nlibrary(lubridate)\n#&gt; \n#&gt; Attaching package: 'lubridate'\n#&gt; The following objects are masked from 'package:terra':\n#&gt; \n#&gt;     intersect, union\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     date, intersect, setdiff, union\n\n# Convert strings to Date objects for the 2030 and 2050 timeframes\n\nstart2030 &lt;- lubridate::ymd(\"2030-01-01\")\nend2030 &lt;- lubridate::ymd(\"2030-1-31\")\n\nstart2050 &lt;- lubridate::ymd(\"2050-01-01\")\nend2050 &lt;- lubridate::ymd(\"2050-1-31\")\n\n# Set time as a daily sequence of the month of January\n\n#2-4.5:2030\ntime(ken_Tmax_245_2030) &lt;- seq(start2030, end2030, by = \"days\")\n\n#5-8.5: 2030\n\ntime(ken_Tmax_585_2030) &lt;- seq(start2030, end2030, by = \"days\")\n\n#2-4.5:2050\ntime(ken_Tmax_245_2050) &lt;- seq(start2050, end2050, by = \"days\")\n\n#5-8.5: 2050\n\ntime(ken_Tmax_585_2050) &lt;- seq(start2050, end2050, by = \"days\")\n\nNow we can confirm if the code works.\n\ntime(ken_Tmax_245_2030)\n#&gt;  [1] \"2030-01-01\" \"2030-01-02\" \"2030-01-03\" \"2030-01-04\" \"2030-01-05\"\n#&gt;  [6] \"2030-01-06\" \"2030-01-07\" \"2030-01-08\" \"2030-01-09\" \"2030-01-10\"\n#&gt; [11] \"2030-01-11\" \"2030-01-12\" \"2030-01-13\" \"2030-01-14\" \"2030-01-15\"\n#&gt; [16] \"2030-01-16\" \"2030-01-17\" \"2030-01-18\" \"2030-01-19\" \"2030-01-20\"\n#&gt; [21] \"2030-01-21\" \"2030-01-22\" \"2030-01-23\" \"2030-01-24\" \"2030-01-25\"\n#&gt; [26] \"2030-01-26\" \"2030-01-27\" \"2030-01-28\" \"2030-01-29\" \"2030-01-30\"\n#&gt; [31] \"2030-01-31\"\n\nNow that we know we have temporal information, we aggregate our data to the monthly level. In this example we will pull the monthly averages.\n\n#aggregate into monthly averages\n\n#2-4.5:2030\n\nken_Tmax_245_2030_yearmonth&lt;-tapp(ken_Tmax_245_2030, fun=mean, index=\"yearmonth\")\n\n#5-8.5:2030\n\nken_Tmax_585_2030_yearmonth&lt;-tapp(ken_Tmax_585_2030, fun=mean, index=\"yearmonth\")\n\n#2-4.5:2050\n\nken_Tmax_245_2050_yearmonth&lt;-tapp(ken_Tmax_245_2050, fun=mean, index=\"yearmonth\")\n\n#5-8.5:2050\n\nken_Tmax_585_2050_yearmonth&lt;-tapp(ken_Tmax_585_2050, fun=mean, index=\"yearmonth\")\n\n\n\nAggregate to the Spatial Level\nNow that our data is temporally aggregated, we must aggregate it at the spatial level. Like we have used a DHS cluster buffer zone in the past, now we will transform Demographic and Health Survey (DHS) cluster GPS information into a 10-kilometer buffer zone. We use the same survey as the last post in this series: Kenya 2022.\nFirst, we read in our DHS GPS data, which you can access on the DHS website.\n\nken_DHS_gps&lt;-st_read(\"data/DHS_clusters/KEGE8AFL.shp\")\n#&gt; Reading layer `KEGE8AFL' from data source \n#&gt;   `C:\\Users\\Rebecca\\Documents\\Projects\\dhs-research-hub\\posts\\2025-09-05-forecasting-pt3\\data\\DHS_clusters\\KEGE8AFL.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 1691 features and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 33.96339 ymin: -4.633603 xmax: 41.87537 ymax: 4.930862\n#&gt; Geodetic CRS:  WGS 84\n\nNext, we transform this into a 10-kilometer buffer zone. We use the UTM 37N reference system because it is best for Kenya.\n\n# Project cluster locations to the UTM 37N reference system\n\nken_DHS_gps &lt;- st_transform(ken_DHS_gps, crs = 21097)\n\n#create buffer zone\n\nken_DHS_buffer &lt;- st_buffer(ken_DHS_gps, dist = 10000)\n\n#transform it back to ESPG 4326 for degrees of longitude and latitude\n\nken_DHS_buffer &lt;- st_transform(ken_DHS_gps, crs = 4326)\n\nNow we will use the extract() function from the terra package to get spatial mean of the Tmax per DHS cluster buffer zone.\n\n\nCode\n#2-4.5:2030\n\nken_Tmax_245_2030_spatial_mean &lt;- terra::extract(\n  ken_Tmax_245_2030_yearmonth, # Extract values for each month \n  ken_DHS_buffer,      # Use Kenya 2022 DHS buffer zone\n  weights = TRUE,\n  fun = \"mean\",         #get the spatial mean\n  na.rm = TRUE,\n  bind=TRUE)\n\n#convert to a spatial frame\nken_Tmax_245_2030_spatial_mean_sf&lt;-st_as_sf(ken_Tmax_245_2030_spatial_mean)\n\n#5-8.5:2030\n\nken_Tmax_585_2030_spatial_mean &lt;- terra::extract(\n  ken_Tmax_585_2030_yearmonth, # Extract values for each month \n  ken_DHS_buffer,      # Use Kenya 2022 DHS buffer zone\n  weights = TRUE,\n  fun = \"mean\",           #get the spatial mean\n  na.rm = TRUE,\n  bind=TRUE)\n\n#convert to a spatial frame\nken_Tmax_585_2030_spatial_mean_sf&lt;-st_as_sf(ken_Tmax_585_2030_spatial_mean)\n\n#2-4.5:2050\n\nken_Tmax_245_2050_spatial_mean &lt;- terra::extract(\n  ken_Tmax_245_2050_yearmonth, # Extract values for each month \n  ken_DHS_buffer,      # Use Kenya 2022 DHS buffer zone\n  weights = TRUE,\n  fun = \"mean\",           #get the spatial mean\n  na.rm = TRUE,\n  bind=TRUE)\n\n#convert to a spatial frame\nken_Tmax_245_2050_spatial_mean_sf&lt;-st_as_sf(ken_Tmax_245_2050_spatial_mean)\n\n#5-8.5:2050\n\nken_Tmax_585_2050_spatial_mean &lt;- terra::extract(\n  ken_Tmax_585_2050_yearmonth, # Extract values for each month \n  ken_DHS_buffer,      # Use Kenya 2022 DHS cluster GPS file\n  weights = TRUE,\n  fun = \"mean\",           #get the spatial mean\n  na.rm = TRUE,\n  bind=TRUE)\n\n#convert to a spatial frame\nken_Tmax_585_2050_spatial_mean_sf&lt;-st_as_sf(ken_Tmax_585_2050_spatial_mean)\n\n\n\n\nVisualization Demo\nNow, let’s visualize the scenario data using both maps and graphics. From the above code, we have generated four different vectors of data: one for each scenario in 2030 and 2050. Next we will do some basic recoding and reformatting to ensure our variables are properly labeled before we merge the vectors, which we will need to do in order to visualize these differences on one graph.\nFirst, we will convert our spatial frames, which we created above, to dataframe. Then we will label our temporal data based on which SSP it came from.\n\n\nCode\n#recodes\n\nlibrary(tidyverse)\n\n#2-4.5:2030\nken_Tmax_245_2030_spatial_mean_df &lt;- as.data.frame(ken_Tmax_245_2030_spatial_mean)\n\nken_Tmax_245_2030_spatial_mean_df&lt;-ken_Tmax_245_2030_spatial_mean_df%&gt;%\n  rename(ym_203001_245=ym_203001)\n\n#5-8.5:2030\n\nken_Tmax_585_2030_spatial_mean_df &lt;- as.data.frame(ken_Tmax_585_2030_spatial_mean)\n\nken_Tmax_585_2030_spatial_mean_df&lt;-ken_Tmax_585_2030_spatial_mean_df%&gt;%\n  rename(ym_203001_585=ym_203001)\n\n#2-4.5:2050\nken_Tmax_245_2050_spatial_mean_df &lt;- as.data.frame(ken_Tmax_245_2050_spatial_mean)\n\nken_Tmax_245_2050_spatial_mean_df&lt;-ken_Tmax_245_2050_spatial_mean_df%&gt;%\n  rename(ym_205001_245=ym_205001)\n\n#5-8.5:2050\n\nken_Tmax_585_2050_spatial_mean_df &lt;- as.data.frame(ken_Tmax_585_2050_spatial_mean)\n\nken_Tmax_585_2050_spatial_mean_df&lt;-ken_Tmax_585_2050_spatial_mean_df%&gt;%\n  rename(ym_205001_585=ym_205001)\n \n\n\nNow that our data is labeled based on which SSP it came from, we combine all of the scenario data into one dataframe. If we were to skip the step in which we labeled the temporal data per SSP, we could mix up the 2030 and 2050 data.\n\n#merge frames\n\ncombined_SSPs_2030&lt;-left_join(ken_Tmax_245_2030_spatial_mean_df, ken_Tmax_585_2030_spatial_mean_df)\n\ncombined_SSPs_2050&lt;-left_join(ken_Tmax_245_2050_spatial_mean_df, ken_Tmax_585_2050_spatial_mean_df)\n\ncombined_SSP_all&lt;-left_join(combined_SSPs_2030, combined_SSPs_2050)\n\nAfter merging the data together, we will reshape the data to a long format, so our data is at the year-month level and is disaggregated per scenario. This means we have four observations, or rows, per DHS cluster.\n\n#pivot the dataframe to make date long\n\ncombined_SSPs_long &lt;- combined_SSP_all %&gt;%\n  pivot_longer(\n    cols = contains (\"ym_\"),     # specify the date columns as what need to be pivoted\n    names_to = \"Date\",        # new column for date\n    values_to = \"Tmax\"           # new column for Tmax\n  )\n\n#print dataframe to illustrate the structure\ncombined_SSPs_long%&gt;%\n  select(DHSCLUST, Tmax, Date)%&gt;%\n  head(n=12)\n#&gt; # A tibble: 12 × 3\n#&gt;    DHSCLUST  Tmax Date         \n#&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;        \n#&gt;  1        1  37.0 ym_203001_245\n#&gt;  2        1  37.0 ym_203001_585\n#&gt;  3        1  37.4 ym_205001_245\n#&gt;  4        1  37.7 ym_205001_585\n#&gt;  5        2  36.9 ym_203001_245\n#&gt;  6        2  37.0 ym_203001_585\n#&gt;  7        2  37.4 ym_205001_245\n#&gt;  8        2  37.6 ym_205001_585\n#&gt;  9        3  36.9 ym_203001_245\n#&gt; 10        3  37.0 ym_203001_585\n#&gt; 11        3  37.4 ym_205001_245\n#&gt; 12        3  37.7 ym_205001_585\n\nFollowing the reshaping, we will create a variable to differentiate which scenario the data comes from: 2-4.5 or 5-8.5. Then we will recode the date to remove the SSP labels that we added to the end.\n\n#create a dichotomous variable for each scenario\n\ncombined_SSPs_long &lt;- combined_SSPs_long %&gt;%\n  mutate(SSP = case_when(\n    str_detect(Date, \"585\") ~ \"585\",\n    str_detect(Date, \"245\") ~ \"245\"\n  ))\n\n#recode date\n\ncombined_SSPs_long &lt;- combined_SSPs_long %&gt;%\n  mutate(year_month = str_extract(Date, \"\\\\d{6}\"))%&gt;%\n  mutate(year_month = str_replace(str_extract(year_month, \"\\\\d{6}\"), \"(\\\\d{4})(\\\\d{2})\", \"\\\\1-\\\\2\"))\n\nIn order to create a summary visualization, we need to make some trade-offs. There are 1692 different DHS clusters. In this example, we select the first five DHS cluster buffer zones, and create a line graph showing the differences in the projected Tmax for 2030 and 2050 per SSP.\n\n\nCode\n#select the first 5 DHS cluster locations for simplicity\n\ncombined_SSPs_long_select&lt;-combined_SSPs_long%&gt;%\n  filter(DHSCLUST&lt;=5)\n\n#convert DHS cluster to a factor variable\n\ncombined_SSPs_long_select$DHSCLUSTfactor&lt;-as.factor(combined_SSPs_long_select$DHSCLUST)\n\nlibrary(ggplot2)\n\n#ggplot code\n\nSSPplot &lt;- ggplot(combined_SSPs_long_select, aes(x = year_month, y = Tmax, color = DHSCLUSTfactor, group = DHSCLUSTfactor)) +\n  geom_line(size = .75) +  \n  geom_point(size = 2, alpha = 0.7) +  # Optional: add points for clarity\n  facet_wrap(~ SSP, ncol = 2, labeller = label_both) +\n  scale_color_brewer(palette = \"Dark2\") +  # Higher contrast palette\n  scale_y_continuous(breaks = seq(32, 40, by = 0.05)) +\n  labs(\n    title = \"Scenario Temperature Data by DHS Cluster,January 2030 & 2050\",\n    subtitle = \"Comparing SSP 2-4.5 vs SSP 5-8.5\",\n    x = \"Date\",\n    y = \"Temperature (°C)\",\n    color = \"County\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    strip.text = element_text(face = \"bold\"),\n    panel.spacing = unit(1, \"lines\"),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\nSSPplot\n\n\n\n\n\n\n\n\n\nThis graph shows us cluster-level variation. Cluster 2 and 5 have the lowest projected values, while 1 and 4 have the highest.\nLet’s look at the fifth cluster alone to zoom into the differences in temperature by scenario.\n\n\nCode\ncombined_SSPs_long_select_5&lt;-combined_SSPs_long%&gt;%\n  filter(DHSCLUST==\"5\")\n\n\nSSPplot2&lt;-ggplot(combined_SSPs_long_select_5,aes(x = year_month, y = Tmax, color = SSP, group = SSP)) +\n  geom_line(size = 1.2) +\n  scale_color_manual(values = c(\"245\" = \"#1b9e77\", \"585\" = \"#d95f02\")) +  \n  scale_y_continuous(breaks = seq(36, 40, by =0.15)) +  # Even more fine y-axis \n  labs(\n    title = \"Average Temperature Over Time by SSP in DHS CLUSTER #5\",\n    x = \"Date\",\n    y = \"Temperature (°C)\",\n    color = \"SSP\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(face = \"bold\"),\n    plot.title = element_text(face = \"bold\")\n  )\n\nSSPplot2\n\n\n\n\n\n\n\n\n\nNow let’s map out this data. We will create a separate map for each year per scenario. First, let’s create a map for each SSP at 2030.\n\n\nCode\nlibrary(ggspatial)\n#&gt; Warning: package 'ggspatial' was built under R version 4.5.1\n\ncombined_SSPs_long_spatial&lt;-left_join(ken_DHS_buffer, combined_SSPs_long)\n  \n#filter for only the 2030 values\ncombined_SSPs_long_spatial_2030&lt;-combined_SSPs_long_spatial%&gt;%\n  filter(year_month==\"2030-01\")\n\n\nmap2030&lt;-ggplot(combined_SSPs_long_spatial_2030) +\n  geom_sf(aes(color = Tmax), size = 4, alpha = 0.8) +  # Use color for points\n  geom_sf(data = ken_ADM1_borders, color = \"black\", fill = NA, linewidth = 0.4) +\n  facet_wrap(~ SSP, ncol = 2, labeller = label_both) +\n  scale_color_viridis_c(option = \"plasma\", direction = -1) +\n  labs(\n    title = \"Projected Tmax by DHS Cluster in Kenya in January 2030, SSP 2-4.5 & SSP 5-8.5\",\n    subtitle = \"SSP\",\n    color = \"Tmax (°C)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    legend.position = \"bottom\",\n    panel.spacing = unit(1, \"lines\")\n  )\n\nmap2030\n\n\n\n\n\n\n\n\n\nBoth SSPs predict varied temperatures in the West, and hotter temperatures in the East, at different levels of extremity. Now let’s create a map for each SSP at 2050.\n\n\nCode\n#filter for only the 2050 values\ncombined_SSPs_long_spatial_2050&lt;-combined_SSPs_long_spatial%&gt;%\n  filter(year_month==\"2050-01\")\n\n\nmap2050&lt;-ggplot(combined_SSPs_long_spatial_2050) +\n  geom_sf(aes(color = Tmax), size = 4, alpha = 0.8) +  # Use color for points\n  geom_sf(data = ken_ADM1_borders, color = \"black\", fill = NA, linewidth = 0.4) +\n  facet_wrap(~SSP, ncol = 2, labeller = label_both) +\n  scale_color_viridis_c(option = \"plasma\", direction = -1) +\n  labs(\n    title = \"Projected Tmax by DHS Cluster in Kenya, January 2050, SSP 2-4.5 & SSP 5-8.5\",\n    subtitle = \"SSP\",\n    color = \"Tmax (°C)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    strip.text = element_text(face = \"bold\"),\n    legend.position = \"bottom\",\n    panel.spacing = unit(1, \"lines\")\n  )\n\nmap2050\n\n\n\n\n\n\n\n\n\nAt first glance, the geographic patterns and changes in temperature may not seem very different, but even slight increases will certainly have major and cascading impacts on health and agriculture if no steps are taken to adapt. The agricultural sector is very vulnerable to temperature variations that can alter growing seasons, reduce crop yields, and threaten food security (IPCC report) - under severe climate scenarios without adaptation, crop yield losses could range from 7% to 23%1. Human health is also at risk. Increased temperatures can directly cause heat stress, adverse pregnancy outcomes and poor mental health, while indirectly heat increases food insecurity or risk of infectious diseases that in turn affect health outcomes2. Overall, climate adaptation measures will be required to reduce the impacts of increasing temperatures. Climate scenarios can help identify where and how soon this intervention will be required.\n\n\nConclusion\nThis post introduced you to one source of downscaled climate scenario data, CHC-CMIP6. We downloaded the data, cropped it to a specific geographic area, then aggregated it to a specific time frame. Lastly, we demonstrated how to create graphics that visualize this data. Our next post will go a step further by combining this data on future temperature conditions with child health outcomes, to explore if there is an association. We will again use CHC-CMIP6 scenario data and combine it with the 2022 DHS data from Kenya to model future levels of child malnutrition. While our previous post showed how short-term projections can inform early warning systems for more immediate action, using our scenarios can help with longer term planning, policy development and resource allocation to develop the infrastructure investments and nutrition sensitive programs that can improve health and resilience.\n\n\n\n\n\nReferences\n\n1. Yuan, X., Li, S., Chen, J., Yu, H., Yang, T., Wang, C., Huang, S., Chen, H., & Ao, X. (2024). Impacts of global climate change on agricultural production: A comprehensive review. Agronomy, 14(7), 1360.\n\n\n2. Ebi, K. L., Capon, A., Berry, P., Broderick, C., Dear, R. de, Havenith, G., Honda, Y., Kovats, R. S., Ma, W., Malik, A., et al. (2021). Hot weather and heat extremes: Health risks. The Lancet, 398(10301), 698–708."
  }
]