[
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "License",
    "section": "",
    "text": "1.1. “Contributor” means each individual or legal entity that creates, contributes to the creation of, or owns Covered Software.\n1.2. “Contributor Version” means the combination of the Contributions of others (if any) used by a Contributor and that particular Contributor’s Contribution.\n1.3. “Contribution” means Covered Software of a particular Contributor.\n1.4. “Covered Software” means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof.\n1.5. “Incompatible With Secondary Licenses” means\n(a) that the initial Contributor has attached the notice described\n    in Exhibit B to the Covered Software; or\n\n(b) that the Covered Software was made available under the terms of\n    version 1.1 or earlier of the License, but not also under the\n    terms of a Secondary License.\n1.6. “Executable Form” means any form of the work other than Source Code Form.\n1.7. “Larger Work” means a work that combines Covered Software with other material, in a separate file or files, that is not Covered Software.\n1.8. “License” means this document.\n1.9. “Licensable” means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently, any and all of the rights conveyed by this License.\n1.10. “Modifications” means any of the following:\n(a) any file in Source Code Form that results from an addition to,\n    deletion from, or modification of the contents of Covered\n    Software; or\n\n(b) any new file in Source Code Form that contains any Covered\n    Software.\n1.11. “Patent Claims” of a Contributor means any patent claim(s), including without limitation, method, process, and apparatus claims, in any patent Licensable by such Contributor that would be infringed, but for the grant of the License, by the making, using, selling, offering for sale, having made, import, or transfer of either its Contributions or its Contributor Version.\n1.12. “Secondary License” means either the GNU General Public License, Version 2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General Public License, Version 3.0, or any later versions of those licenses.\n1.13. “Source Code Form” means the form of the work preferred for making modifications.\n1.14. “You” (or “Your”) means an individual or a legal entity exercising rights under this License. For legal entities, “You” includes any entity that controls, is controlled by, or is under common control with You. For purposes of this definition, “control” means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity.\n\n\n\n2.1. Grants\nEach Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:\n\nunder intellectual property rights (other than patent or trademark) Licensable by such Contributor to use, reproduce, make available, modify, display, perform, distribute, and otherwise exploit its Contributions, either on an unmodified basis, with Modifications, or as part of a Larger Work; and\nunder Patent Claims of such Contributor to make, use, sell, offer for sale, have made, import, and otherwise transfer either its Contributions or its Contributor Version.\n\n2.2. Effective Date\nThe licenses granted in Section 2.1 with respect to any Contribution become effective for each Contribution on the date the Contributor first distributes such Contribution.\n2.3. Limitations on Grant Scope\nThe licenses granted in this Section 2 are the only rights granted under this License. No additional rights or licenses will be implied from the distribution or licensing of Covered Software under this License. Notwithstanding Section 2.1(b) above, no patent license is granted by a Contributor:\n\nfor any code that a Contributor has removed from Covered Software; or\nfor infringements caused by: (i) Your and any other third party’s modifications of Covered Software, or (ii) the combination of its Contributions with other software (except as part of its Contributor Version); or\nunder Patent Claims infringed by Covered Software in the absence of its Contributions.\n\nThis License does not grant any rights in the trademarks, service marks, or logos of any Contributor (except as may be necessary to comply with the notice requirements in Section 3.4).\n2.4. Subsequent Licenses\nNo Contributor makes additional grants as a result of Your choice to distribute the Covered Software under a subsequent version of this License (see Section 10.2) or under the terms of a Secondary License (if permitted under the terms of Section 3.3).\n2.5. Representation\nEach Contributor represents that the Contributor believes its Contributions are its original creation(s) or it has sufficient rights to grant the rights to its Contributions conveyed by this License.\n2.6. Fair Use\nThis License is not intended to limit any rights You have under applicable copyright doctrines of fair use, fair dealing, or other equivalents.\n2.7. Conditions\nSections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in Section 2.1.\n\n\n\n3.1. Distribution of Source Form\nAll distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients’ rights in the Source Code Form.\n3.2. Distribution of Executable Form\nIf You distribute Covered Software in Executable Form then:\n\nsuch Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and\nYou may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients’ rights in the Source Code Form under this License.\n\n3.3. Distribution of a Larger Work\nYou may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. If the Larger Work is a combination of Covered Software with a work governed by one or more Secondary Licenses, and the Covered Software is not Incompatible With Secondary Licenses, this License permits You to additionally distribute such Covered Software under the terms of such Secondary License(s), so that the recipient of the Larger Work may, at their option, further distribute the Covered Software under the terms of either this License or such Secondary License(s).\n3.4. Notices\nYou may not remove or alter the substance of any license notices (including copyright notices, patent notices, disclaimers of warranty, or limitations of liability) contained within the Source Code Form of the Covered Software, except that You may alter any license notices to the extent required to remedy known factual inaccuracies.\n3.5. Application of Additional Terms\nYou may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, You may do so only on Your own behalf, and not on behalf of any Contributor. You must make it absolutely clear that any such warranty, support, indemnity, or liability obligation is offered by You alone, and You hereby agree to indemnify every Contributor for any liability incurred by such Contributor as a result of warranty, support, indemnity or liability terms You offer. You may include additional disclaimers of warranty and limitations of liability specific to any jurisdiction.\n\n\n\nIf it is impossible for You to comply with any of the terms of this License with respect to some or all of the Covered Software due to statute, judicial order, or regulation then You must: (a) comply with the terms of this License to the maximum extent possible; and (b) describe the limitations and the code they affect. Such description must be placed in a text file included with all distributions of the Covered Software under this License. Except to the extent prohibited by statute or regulation, such description must be sufficiently detailed for a recipient of ordinary skill to be able to understand it.\n\n\n\n5.1. The rights granted under this License will terminate automatically if You fail to comply with any of its terms. However, if You become compliant, then the rights granted under this License from a particular Contributor are reinstated (a) provisionally, unless and until such Contributor explicitly and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor fails to notify You of the non-compliance by some reasonable means prior to 60 days after You have come back into compliance. Moreover, Your grants from a particular Contributor are reinstated on an ongoing basis if such Contributor notifies You of the non-compliance by some reasonable means, this is the first time You have received notice of non-compliance with this License from such Contributor, and You become compliant prior to 30 days after Your receipt of the notice.\n5.2. If You initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a Contributor Version directly or indirectly infringes any patent, then the rights granted to You by any and all Contributors for the Covered Software under Section 2.1 of this License shall terminate.\n5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user license agreements (excluding distributors and resellers) which have been validly granted by You or Your distributors under this License prior to termination shall survive termination.\n\n\n\nCovered Software is provided under this License on an “as is”\nbasis, without warranty of any kind, either expressed, implied, or\nstatutory, including, without limitation, warranties that the\nCovered Software is free of defects, merchantable, fit for a\nparticular purpose or non-infringing. The entire risk as to the\nquality and performance of the Covered Software is with You.\nShould any Covered Software prove defective in any respect, You\n(not any Contributor) assume the cost of any necessary servicing,\nrepair, or correction. This disclaimer of warranty constitutes an\nessential part of this License. No use of any Covered Software is\nauthorized under this License except under this disclaimer.\n\n\n\nUnder no circumstances and under no legal theory, whether tort\n(including negligence), contract, or otherwise, shall any\nContributor, or anyone who distributes Covered Software as\npermitted above, be liable to You for any direct, indirect,\nspecial, incidental, or consequential damages of any character\nincluding, without limitation, damages for lost profits, loss of\ngoodwill, work stoppage, computer failure or malfunction, or any\nand all other commercial damages or losses, even if such party\nshall have been informed of the possibility of such damages. This\nlimitation of liability shall not apply to liability for death or\npersonal injury resulting from such party’s negligence to the\nextent applicable law prohibits such limitation. Some\njurisdictions do not allow the exclusion or limitation of\nincidental or consequential damages, so this exclusion and\nlimitation may not apply to You.\n\n\n\nAny litigation relating to this License may be brought only in the courts of a jurisdiction where the defendant maintains its principal place of business and such litigation shall be governed by laws of that jurisdiction, without reference to its conflict-of-law provisions. Nothing in this Section shall prevent a party’s ability to bring cross-claims or counter-claims.\n\n\n\nThis License represents the complete agreement concerning the subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not be used to construe this License against a Contributor.\n\n\n\n10.1. New Versions\nMozilla Foundation is the license steward. Except as provided in Section 10.3, no one other than the license steward has the right to modify or publish new versions of this License. Each version will be given a distinguishing version number.\n10.2. Effect of New Versions\nYou may distribute the Covered Software under the terms of the version of the License under which You originally received the Covered Software, or under the terms of any subsequent version published by the license steward.\n10.3. Modified Versions\nIf you create software not governed by this License, and you want to create a new license for such software, you may create and use a modified version of this License if you rename the license and remove any references to the name of the license steward (except to note that such modified license differs from this License).\n10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses\nIf You choose to distribute Source Code Form that is Incompatible With Secondary Licenses under the terms of this version of the License, the notice described in Exhibit B of this License must be attached.\n\n\n\nThis Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\nIf it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice.\nYou may add additional accurate notices of copyright ownership.\n\n\n\nThis Source Code Form is “Incompatible With Secondary Licenses”, as defined by the Mozilla Public License, v. 2.0."
  },
  {
    "objectID": "LICENSE.html#definitions",
    "href": "LICENSE.html#definitions",
    "title": "License",
    "section": "",
    "text": "1.1. “Contributor” means each individual or legal entity that creates, contributes to the creation of, or owns Covered Software.\n1.2. “Contributor Version” means the combination of the Contributions of others (if any) used by a Contributor and that particular Contributor’s Contribution.\n1.3. “Contribution” means Covered Software of a particular Contributor.\n1.4. “Covered Software” means Source Code Form to which the initial Contributor has attached the notice in Exhibit A, the Executable Form of such Source Code Form, and Modifications of such Source Code Form, in each case including portions thereof.\n1.5. “Incompatible With Secondary Licenses” means\n(a) that the initial Contributor has attached the notice described\n    in Exhibit B to the Covered Software; or\n\n(b) that the Covered Software was made available under the terms of\n    version 1.1 or earlier of the License, but not also under the\n    terms of a Secondary License.\n1.6. “Executable Form” means any form of the work other than Source Code Form.\n1.7. “Larger Work” means a work that combines Covered Software with other material, in a separate file or files, that is not Covered Software.\n1.8. “License” means this document.\n1.9. “Licensable” means having the right to grant, to the maximum extent possible, whether at the time of the initial grant or subsequently, any and all of the rights conveyed by this License.\n1.10. “Modifications” means any of the following:\n(a) any file in Source Code Form that results from an addition to,\n    deletion from, or modification of the contents of Covered\n    Software; or\n\n(b) any new file in Source Code Form that contains any Covered\n    Software.\n1.11. “Patent Claims” of a Contributor means any patent claim(s), including without limitation, method, process, and apparatus claims, in any patent Licensable by such Contributor that would be infringed, but for the grant of the License, by the making, using, selling, offering for sale, having made, import, or transfer of either its Contributions or its Contributor Version.\n1.12. “Secondary License” means either the GNU General Public License, Version 2.0, the GNU Lesser General Public License, Version 2.1, the GNU Affero General Public License, Version 3.0, or any later versions of those licenses.\n1.13. “Source Code Form” means the form of the work preferred for making modifications.\n1.14. “You” (or “Your”) means an individual or a legal entity exercising rights under this License. For legal entities, “You” includes any entity that controls, is controlled by, or is under common control with You. For purposes of this definition, “control” means (a) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (b) ownership of more than fifty percent (50%) of the outstanding shares or beneficial ownership of such entity."
  },
  {
    "objectID": "LICENSE.html#license-grants-and-conditions",
    "href": "LICENSE.html#license-grants-and-conditions",
    "title": "License",
    "section": "",
    "text": "2.1. Grants\nEach Contributor hereby grants You a world-wide, royalty-free, non-exclusive license:\n\nunder intellectual property rights (other than patent or trademark) Licensable by such Contributor to use, reproduce, make available, modify, display, perform, distribute, and otherwise exploit its Contributions, either on an unmodified basis, with Modifications, or as part of a Larger Work; and\nunder Patent Claims of such Contributor to make, use, sell, offer for sale, have made, import, and otherwise transfer either its Contributions or its Contributor Version.\n\n2.2. Effective Date\nThe licenses granted in Section 2.1 with respect to any Contribution become effective for each Contribution on the date the Contributor first distributes such Contribution.\n2.3. Limitations on Grant Scope\nThe licenses granted in this Section 2 are the only rights granted under this License. No additional rights or licenses will be implied from the distribution or licensing of Covered Software under this License. Notwithstanding Section 2.1(b) above, no patent license is granted by a Contributor:\n\nfor any code that a Contributor has removed from Covered Software; or\nfor infringements caused by: (i) Your and any other third party’s modifications of Covered Software, or (ii) the combination of its Contributions with other software (except as part of its Contributor Version); or\nunder Patent Claims infringed by Covered Software in the absence of its Contributions.\n\nThis License does not grant any rights in the trademarks, service marks, or logos of any Contributor (except as may be necessary to comply with the notice requirements in Section 3.4).\n2.4. Subsequent Licenses\nNo Contributor makes additional grants as a result of Your choice to distribute the Covered Software under a subsequent version of this License (see Section 10.2) or under the terms of a Secondary License (if permitted under the terms of Section 3.3).\n2.5. Representation\nEach Contributor represents that the Contributor believes its Contributions are its original creation(s) or it has sufficient rights to grant the rights to its Contributions conveyed by this License.\n2.6. Fair Use\nThis License is not intended to limit any rights You have under applicable copyright doctrines of fair use, fair dealing, or other equivalents.\n2.7. Conditions\nSections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted in Section 2.1."
  },
  {
    "objectID": "LICENSE.html#responsibilities",
    "href": "LICENSE.html#responsibilities",
    "title": "License",
    "section": "",
    "text": "3.1. Distribution of Source Form\nAll distribution of Covered Software in Source Code Form, including any Modifications that You create or to which You contribute, must be under the terms of this License. You must inform recipients that the Source Code Form of the Covered Software is governed by the terms of this License, and how they can obtain a copy of this License. You may not attempt to alter or restrict the recipients’ rights in the Source Code Form.\n3.2. Distribution of Executable Form\nIf You distribute Covered Software in Executable Form then:\n\nsuch Covered Software must also be made available in Source Code Form, as described in Section 3.1, and You must inform recipients of the Executable Form how they can obtain a copy of such Source Code Form by reasonable means in a timely manner, at a charge no more than the cost of distribution to the recipient; and\nYou may distribute such Executable Form under the terms of this License, or sublicense it under different terms, provided that the license for the Executable Form does not attempt to limit or alter the recipients’ rights in the Source Code Form under this License.\n\n3.3. Distribution of a Larger Work\nYou may create and distribute a Larger Work under terms of Your choice, provided that You also comply with the requirements of this License for the Covered Software. If the Larger Work is a combination of Covered Software with a work governed by one or more Secondary Licenses, and the Covered Software is not Incompatible With Secondary Licenses, this License permits You to additionally distribute such Covered Software under the terms of such Secondary License(s), so that the recipient of the Larger Work may, at their option, further distribute the Covered Software under the terms of either this License or such Secondary License(s).\n3.4. Notices\nYou may not remove or alter the substance of any license notices (including copyright notices, patent notices, disclaimers of warranty, or limitations of liability) contained within the Source Code Form of the Covered Software, except that You may alter any license notices to the extent required to remedy known factual inaccuracies.\n3.5. Application of Additional Terms\nYou may choose to offer, and to charge a fee for, warranty, support, indemnity or liability obligations to one or more recipients of Covered Software. However, You may do so only on Your own behalf, and not on behalf of any Contributor. You must make it absolutely clear that any such warranty, support, indemnity, or liability obligation is offered by You alone, and You hereby agree to indemnify every Contributor for any liability incurred by such Contributor as a result of warranty, support, indemnity or liability terms You offer. You may include additional disclaimers of warranty and limitations of liability specific to any jurisdiction."
  },
  {
    "objectID": "LICENSE.html#inability-to-comply-due-to-statute-or-regulation",
    "href": "LICENSE.html#inability-to-comply-due-to-statute-or-regulation",
    "title": "License",
    "section": "",
    "text": "If it is impossible for You to comply with any of the terms of this License with respect to some or all of the Covered Software due to statute, judicial order, or regulation then You must: (a) comply with the terms of this License to the maximum extent possible; and (b) describe the limitations and the code they affect. Such description must be placed in a text file included with all distributions of the Covered Software under this License. Except to the extent prohibited by statute or regulation, such description must be sufficiently detailed for a recipient of ordinary skill to be able to understand it."
  },
  {
    "objectID": "LICENSE.html#termination",
    "href": "LICENSE.html#termination",
    "title": "License",
    "section": "",
    "text": "5.1. The rights granted under this License will terminate automatically if You fail to comply with any of its terms. However, if You become compliant, then the rights granted under this License from a particular Contributor are reinstated (a) provisionally, unless and until such Contributor explicitly and finally terminates Your grants, and (b) on an ongoing basis, if such Contributor fails to notify You of the non-compliance by some reasonable means prior to 60 days after You have come back into compliance. Moreover, Your grants from a particular Contributor are reinstated on an ongoing basis if such Contributor notifies You of the non-compliance by some reasonable means, this is the first time You have received notice of non-compliance with this License from such Contributor, and You become compliant prior to 30 days after Your receipt of the notice.\n5.2. If You initiate litigation against any entity by asserting a patent infringement claim (excluding declaratory judgment actions, counter-claims, and cross-claims) alleging that a Contributor Version directly or indirectly infringes any patent, then the rights granted to You by any and all Contributors for the Covered Software under Section 2.1 of this License shall terminate.\n5.3. In the event of termination under Sections 5.1 or 5.2 above, all end user license agreements (excluding distributors and resellers) which have been validly granted by You or Your distributors under this License prior to termination shall survive termination."
  },
  {
    "objectID": "LICENSE.html#disclaimer-of-warranty",
    "href": "LICENSE.html#disclaimer-of-warranty",
    "title": "License",
    "section": "",
    "text": "Covered Software is provided under this License on an “as is”\nbasis, without warranty of any kind, either expressed, implied, or\nstatutory, including, without limitation, warranties that the\nCovered Software is free of defects, merchantable, fit for a\nparticular purpose or non-infringing. The entire risk as to the\nquality and performance of the Covered Software is with You.\nShould any Covered Software prove defective in any respect, You\n(not any Contributor) assume the cost of any necessary servicing,\nrepair, or correction. This disclaimer of warranty constitutes an\nessential part of this License. No use of any Covered Software is\nauthorized under this License except under this disclaimer."
  },
  {
    "objectID": "LICENSE.html#limitation-of-liability",
    "href": "LICENSE.html#limitation-of-liability",
    "title": "License",
    "section": "",
    "text": "Under no circumstances and under no legal theory, whether tort\n(including negligence), contract, or otherwise, shall any\nContributor, or anyone who distributes Covered Software as\npermitted above, be liable to You for any direct, indirect,\nspecial, incidental, or consequential damages of any character\nincluding, without limitation, damages for lost profits, loss of\ngoodwill, work stoppage, computer failure or malfunction, or any\nand all other commercial damages or losses, even if such party\nshall have been informed of the possibility of such damages. This\nlimitation of liability shall not apply to liability for death or\npersonal injury resulting from such party’s negligence to the\nextent applicable law prohibits such limitation. Some\njurisdictions do not allow the exclusion or limitation of\nincidental or consequential damages, so this exclusion and\nlimitation may not apply to You."
  },
  {
    "objectID": "LICENSE.html#litigation",
    "href": "LICENSE.html#litigation",
    "title": "License",
    "section": "",
    "text": "Any litigation relating to this License may be brought only in the courts of a jurisdiction where the defendant maintains its principal place of business and such litigation shall be governed by laws of that jurisdiction, without reference to its conflict-of-law provisions. Nothing in this Section shall prevent a party’s ability to bring cross-claims or counter-claims."
  },
  {
    "objectID": "LICENSE.html#miscellaneous",
    "href": "LICENSE.html#miscellaneous",
    "title": "License",
    "section": "",
    "text": "This License represents the complete agreement concerning the subject matter hereof. If any provision of this License is held to be unenforceable, such provision shall be reformed only to the extent necessary to make it enforceable. Any law or regulation which provides that the language of a contract shall be construed against the drafter shall not be used to construe this License against a Contributor."
  },
  {
    "objectID": "LICENSE.html#versions-of-the-license",
    "href": "LICENSE.html#versions-of-the-license",
    "title": "License",
    "section": "",
    "text": "10.1. New Versions\nMozilla Foundation is the license steward. Except as provided in Section 10.3, no one other than the license steward has the right to modify or publish new versions of this License. Each version will be given a distinguishing version number.\n10.2. Effect of New Versions\nYou may distribute the Covered Software under the terms of the version of the License under which You originally received the Covered Software, or under the terms of any subsequent version published by the license steward.\n10.3. Modified Versions\nIf you create software not governed by this License, and you want to create a new license for such software, you may create and use a modified version of this License if you rename the license and remove any references to the name of the license steward (except to note that such modified license differs from this License).\n10.4. Distributing Source Code Form that is Incompatible With Secondary Licenses\nIf You choose to distribute Source Code Form that is Incompatible With Secondary Licenses under the terms of this version of the License, the notice described in Exhibit B of this License must be attached."
  },
  {
    "objectID": "LICENSE.html#exhibit-a---source-code-form-license-notice",
    "href": "LICENSE.html#exhibit-a---source-code-form-license-notice",
    "title": "License",
    "section": "",
    "text": "This Source Code Form is subject to the terms of the Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\nIf it is not possible or desirable to put the notice in a particular file, then You may include the notice in a location (such as a LICENSE file in a relevant directory) where a recipient would be likely to look for such a notice.\nYou may add additional accurate notices of copyright ownership."
  },
  {
    "objectID": "LICENSE.html#exhibit-b---incompatible-with-secondary-licenses-notice",
    "href": "LICENSE.html#exhibit-b---incompatible-with-secondary-licenses-notice",
    "title": "License",
    "section": "",
    "text": "This Source Code Form is “Incompatible With Secondary Licenses”, as defined by the Mozilla Public License, v. 2.0."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "Enforcement Guidelines",
    "text": "Enforcement Guidelines\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n1. Correction\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n2. Warning\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n3. Temporary Ban\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n4. Permanent Ban\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "blog-post-workflow.html",
    "href": "blog-post-workflow.html",
    "title": "Blog Post Workflow",
    "section": "",
    "text": "To work on the blog, you’ll need the following:\n\nR\nRStudio\nGit\nA few particular R packages\n\nIf you’re working on your own machine, follow the instructions below to get this software installed.\nThe MPC also hosts a server instance of R and RStudio, which you can access using your UMN login credentials. If you prefer to work on the server, then R, RStudio, and Git will already be installed. However, you will still need to install the indicated R packages and configure your Git setup (if you haven’t done so before), so we still suggest reading through these instructions.\n\n\nTo install R, visit the Comprehensive R Archive Network (CRAN) and choose the appropriate download link for your operating system.\n\nRStudio is an interactive development environment (IDE) for R that provides an interface and code editing tools that make it much, much easier to write and edit R code.\nThe DHS Research Hub is organized as an RStudio Project, which requires RStudio. You can download RStudio here.\n\nWhen you’ve got RStudio set up, install these R packages by running the following in the RStudio console:\n\n# For writing blog posts\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\n# For help setting up Git\ninstall.packages(\"usethis\")\ninstall.packages(\"gert\")\ninstall.packages(\"gitcreds\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf you have trouble installing any of these packages, try to install them in a fresh RStudio session (in the RStudio toolbar, select Session &gt; Restart R).\n\n\n\n\nThe DHS Research Hub blog materials exist in both a public and private format. When you’re working on the blog, you’ll be working on the private site, which is hosted on the UMN Enterprise GitHub server at https://github.umn.edu/mpc/dhs-research-hub.\nThe UMN GitHub server is accessible only to people affiliated with UMN. By working in this environment, we can develop and edit new posts without having to worry about them becoming visible on the public site prematurely. Also, it allows us to store files necessary for certain posts (e.g. IPUMS data files) without making those files publicly visible. Members of the blog “admin” team will be responsible for migrating completed posts to the public version of the site.\nSince you’re affiliated with UMN, you automatically have access to an account on UMN GitHub. To initialize an account, visit https://github.umn.edu and log in with your University Internet ID and password.\n\nTo interact with GitHub, you’ll need to install Git on your local machine. Git is the version control software that allows us to track the blog’s history and manage line-by-line changes to files as we edit new posts.\n\nMacOS comes with Git already installed. To confirm, you can check its location by running which git in the Mac Terminal. You can also check the version you have installed by running git --version.\nIf for some reason Git is not installed, use the Install Git using Homebrew instructions to install it.\n\nUsers of other operating systems should download the appropriate git for their operating system.\n\nNext, we’ll link RStudio and Git. This adds a new tab to your RStudio interface where you can see your files being tracked by Git in a convenient point-and-click format.\nIn the RStudio toolbar, select Tools &gt; Global Options and locate the Git/SVN tab. Ensure that the box shown below is checked, and then enter the location of the executable file. To find the git executable\n\n\non MacOS: run which git in Terminal\n\non Windows: look for git.exe in your file explorer (most likely in Program Files)\n\n\n\n\n\n\n\n\n\n\nFinally, we’ll provide Git with the username and email associated with our UMN GitHub account. We’ll also need to generate a Personal Access Token (PAT) for our account. The PAT functions like a password that allows you to interact with your UMN GitHub account from R and the command line.\nIf you’ve installed the packages listed above, you can use R code (rather than the command line) to configure your credentials. This is what we demonstrate below.\nFirst, set the username and email address associated with your UMN GitHub account. For example, I’d run:\n\ngert::git_config_global_set(\"user.name\", \"Finn Roberts\")\ngert::git_config_global_set(\"user.email\", \"robe2037@umn.edu\")\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t necessarily need to store your credentials to use Git, but if you don’t, you’ll have to enter them in a popup window each time you interact with GitHub from R or the command line.\n\n\nNext, create a PAT for your account. Run the following in your RStudio console to launch a webpage where you can configure a new PAT:\n\nusethis::create_github_token(host = \"https://github.umn.edu\")\n\nYou can leave the default boxes checked and click the green Generate Token button. This should display a long string of digits—this is your new PAT. Don’t close this page yet! Return to your RStudio console and run:\n\ngitcreds::gitcreds_set(\"https://github.umn.edu\")\n\nThis will prompt you to enter a new token. Follow the instructions to copy and paste the PAT you just generated in your browser and press Enter. From now on, RStudio and Git will be able to access your UMN GitHub account automatically.\n\n\n\n\n\n\nNote\n\n\n\nIf you have a personal GitHub account at https://github.com you could repeat this process substituting \"https://github.com\" for \"https://github.umn.edu\", and Git will automatically choose the right credentials based on the repository associated with your project.\n\n\n\nNow that we have Git configured, we can download (or clone) a copy of the blog materials from UMN GitHub.\nOpen RStudio and navigate to File &gt; New Project, then select Version Control:\n\n\n\n\n\n\n\n\nChoose Git to clone the project from a GitHub repository:\n\n\n\n\n\n\n\n\nOn the next menu page, enter the address for the UMN GitHub repository: https://github.umn.edu/mpc/dhs-research-hub/\nHit Tab and the project directory name should populate automatically. If not, enter dhs-research-hub as the directory name:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to clone the private version of the repository (the one located at github.umn.edu), not the public version (located at github.com).\n\n\nIn the third field, choose the location where you would like to store the blog materials on your computer. This can be anywhere that makes sense with your personal file organization approach.\n\n\nWhen choosing a place to save this project, do not save to a network drive. This seems to cause RStudio to crash!\nFinally, click Create Project. After a short bit, RStudio will relaunch and open the new project. If you adjust the windows to show the Files (left) and Git (right) tabs, you should see something like this:\n\n\n\nYou have now downloaded a copy of the DHS Research Hub blog to your computer!\nMoreover, because you’ve connected these files to a GitHub repository, the RStudio Project will now keep track of changes you make to the files in this folder, and it will prompt you to upload your changes back to GitHub: as you add, edit, or delete files, a list of changes will appear in the Git tab.\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that RStudio recognizes the project is being used with Git, make sure to open the .Rproj file before working on any of the blog materials.\nTo open the project, simply click on the dhs-research-hub.Rproj file or navigate to File &gt; Open Project. (You only need to do this once at the start of your R session.)"
  },
  {
    "objectID": "blog-post-workflow.html#set-up-r-and-rstudio",
    "href": "blog-post-workflow.html#set-up-r-and-rstudio",
    "title": "Blog Post Workflow",
    "section": "",
    "text": "To install R, visit the Comprehensive R Archive Network (CRAN) and choose the appropriate download link for your operating system.\n\nRStudio is an interactive development environment (IDE) for R that provides an interface and code editing tools that make it much, much easier to write and edit R code.\nThe DHS Research Hub is organized as an RStudio Project, which requires RStudio. You can download RStudio here.\n\nWhen you’ve got RStudio set up, install these R packages by running the following in the RStudio console:\n\n# For writing blog posts\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"knitr\")\n\n# For help setting up Git\ninstall.packages(\"usethis\")\ninstall.packages(\"gert\")\ninstall.packages(\"gitcreds\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf you have trouble installing any of these packages, try to install them in a fresh RStudio session (in the RStudio toolbar, select Session &gt; Restart R)."
  },
  {
    "objectID": "blog-post-workflow.html#configure-git-and-access-blog-materials",
    "href": "blog-post-workflow.html#configure-git-and-access-blog-materials",
    "title": "Blog Post Workflow",
    "section": "",
    "text": "The DHS Research Hub blog materials exist in both a public and private format. When you’re working on the blog, you’ll be working on the private site, which is hosted on the UMN Enterprise GitHub server at https://github.umn.edu/mpc/dhs-research-hub.\nThe UMN GitHub server is accessible only to people affiliated with UMN. By working in this environment, we can develop and edit new posts without having to worry about them becoming visible on the public site prematurely. Also, it allows us to store files necessary for certain posts (e.g. IPUMS data files) without making those files publicly visible. Members of the blog “admin” team will be responsible for migrating completed posts to the public version of the site.\nSince you’re affiliated with UMN, you automatically have access to an account on UMN GitHub. To initialize an account, visit https://github.umn.edu and log in with your University Internet ID and password.\n\nTo interact with GitHub, you’ll need to install Git on your local machine. Git is the version control software that allows us to track the blog’s history and manage line-by-line changes to files as we edit new posts.\n\nMacOS comes with Git already installed. To confirm, you can check its location by running which git in the Mac Terminal. You can also check the version you have installed by running git --version.\nIf for some reason Git is not installed, use the Install Git using Homebrew instructions to install it.\n\nUsers of other operating systems should download the appropriate git for their operating system.\n\nNext, we’ll link RStudio and Git. This adds a new tab to your RStudio interface where you can see your files being tracked by Git in a convenient point-and-click format.\nIn the RStudio toolbar, select Tools &gt; Global Options and locate the Git/SVN tab. Ensure that the box shown below is checked, and then enter the location of the executable file. To find the git executable\n\n\non MacOS: run which git in Terminal\n\non Windows: look for git.exe in your file explorer (most likely in Program Files)\n\n\n\n\n\n\n\n\n\n\nFinally, we’ll provide Git with the username and email associated with our UMN GitHub account. We’ll also need to generate a Personal Access Token (PAT) for our account. The PAT functions like a password that allows you to interact with your UMN GitHub account from R and the command line.\nIf you’ve installed the packages listed above, you can use R code (rather than the command line) to configure your credentials. This is what we demonstrate below.\nFirst, set the username and email address associated with your UMN GitHub account. For example, I’d run:\n\ngert::git_config_global_set(\"user.name\", \"Finn Roberts\")\ngert::git_config_global_set(\"user.email\", \"robe2037@umn.edu\")\n\n\n\n\n\n\n\nNote\n\n\n\nYou don’t necessarily need to store your credentials to use Git, but if you don’t, you’ll have to enter them in a popup window each time you interact with GitHub from R or the command line.\n\n\nNext, create a PAT for your account. Run the following in your RStudio console to launch a webpage where you can configure a new PAT:\n\nusethis::create_github_token(host = \"https://github.umn.edu\")\n\nYou can leave the default boxes checked and click the green Generate Token button. This should display a long string of digits—this is your new PAT. Don’t close this page yet! Return to your RStudio console and run:\n\ngitcreds::gitcreds_set(\"https://github.umn.edu\")\n\nThis will prompt you to enter a new token. Follow the instructions to copy and paste the PAT you just generated in your browser and press Enter. From now on, RStudio and Git will be able to access your UMN GitHub account automatically.\n\n\n\n\n\n\nNote\n\n\n\nIf you have a personal GitHub account at https://github.com you could repeat this process substituting \"https://github.com\" for \"https://github.umn.edu\", and Git will automatically choose the right credentials based on the repository associated with your project.\n\n\n\nNow that we have Git configured, we can download (or clone) a copy of the blog materials from UMN GitHub.\nOpen RStudio and navigate to File &gt; New Project, then select Version Control:\n\n\n\n\n\n\n\n\nChoose Git to clone the project from a GitHub repository:\n\n\n\n\n\n\n\n\nOn the next menu page, enter the address for the UMN GitHub repository: https://github.umn.edu/mpc/dhs-research-hub/\nHit Tab and the project directory name should populate automatically. If not, enter dhs-research-hub as the directory name:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMake sure to clone the private version of the repository (the one located at github.umn.edu), not the public version (located at github.com).\n\n\nIn the third field, choose the location where you would like to store the blog materials on your computer. This can be anywhere that makes sense with your personal file organization approach.\n\n\nWhen choosing a place to save this project, do not save to a network drive. This seems to cause RStudio to crash!\nFinally, click Create Project. After a short bit, RStudio will relaunch and open the new project. If you adjust the windows to show the Files (left) and Git (right) tabs, you should see something like this:\n\n\n\nYou have now downloaded a copy of the DHS Research Hub blog to your computer!\nMoreover, because you’ve connected these files to a GitHub repository, the RStudio Project will now keep track of changes you make to the files in this folder, and it will prompt you to upload your changes back to GitHub: as you add, edit, or delete files, a list of changes will appear in the Git tab.\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that RStudio recognizes the project is being used with Git, make sure to open the .Rproj file before working on any of the blog materials.\nTo open the project, simply click on the dhs-research-hub.Rproj file or navigate to File &gt; Open Project. (You only need to do this once at the start of your R session.)"
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html",
    "href": "posts/2024-04-15-chirts-metrics/index.html",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "",
    "text": "In our previous technical post, we showed how to reduce a daily time series of climate data for an entire country into a single digestible metric that can be attached to DHS survey data for use in an analysis.\nHowever, as we mentioned then, a long-run average isn’t always the most appropriate way to aggregate raster values across time. As an example, imagine we were interested in the impacts of extreme heat on child birth weight. The proximate conditions in the several months preceding each child’s birth are likely to be more consequential for that child’s health than the average conditions over many years.1"
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#dhs-boundaries",
    "href": "posts/2024-04-15-chirts-metrics/index.html#dhs-boundaries",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "DHS boundaries",
    "text": "DHS boundaries\nFor this series, we’ll use the 2012 DHS sample for Mali. We won’t be working with the survey data until our next post, but we will be using the integrated administrative boundary files from IPUMS in our maps.\nYou can download the boundary data directly from IPUMS DHS by clicking the shapefile link under the Mali section of this table. We’ve placed this shapefile in the data/gps directory within our project.\nPreviously, we unzipped the shapefile and loaded it with st_read() from the sf package. However, since IPUMS often distributes shapefiles in zip archives, ipumsr provides read_ipums_sf() as a way to read a shapefile directly without manually extracting the compressed files. We’ll use this convenience to load our boundary data into an sf object:\n\nml_borders &lt;- read_ipums_sf(\"data/gps/geo_ml1995_2018.zip\")\n\nml_borders\n#&gt; Simple feature collection with 8 features and 3 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -12.23888 ymin: 10.14781 xmax: 4.267383 ymax: 25.00108\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 8 × 4\n#&gt;   CNTRY_NAME ADMIN_NAME    DHSCODE                                      geometry\n#&gt;   &lt;chr&gt;      &lt;chr&gt;           &lt;int&gt;                                 &lt;POLYGON [°]&gt;\n#&gt; 1 Mali       Kayes               1 ((-9.330095 15.50158, -9.320187 15.50138, -9…\n#&gt; 2 Mali       Ségou               4 ((-3.962041 13.50099, -3.963767 13.50299, -3…\n#&gt; 3 Mali       Mopti               5 ((-0.7440053 15.06439, -0.9481987 14.88898, …\n#&gt; 4 Mali       Tombouctou          6 ((-0.005279168 21.87488, -0.006277 21.87317,…\n#&gt; 5 Mali       Bamako              9 ((-7.932848 12.68226, -7.932015 12.68209, -7…\n#&gt; 6 Mali       Sikasso             3 ((-4.472905 12.71992, -4.472977 12.71908, -4…\n#&gt; 7 Mali       Koulikoro           2 ((-9.076561 15.50138, -9.000042 15.50046, -8…\n#&gt; 8 Mali       Gao and Kidal       7 ((-0.4509285 15.0843, -0.450753 15.0864, -0.…\n\nOur boundary file includes borders for individual administrative units within Mali. However, it’s often useful to have a single external border for spatial operations and mapping.\nTo combine internal borders, we can use st_union() from sf. However, in this case, we first need to simplify our file so that st_union() works properly.\n\n\n\n\n\n\nTip\n\n\n\nFor some spatial files, small misalignments may cause problems for certain spatial operations. Often, you’ll notice these issues in your maps if errant lines or points appear in unexpected places. You can check whether a file is topologically valid with st_is_valid().\n\n\nWe use st_make_valid() to correct some of these issues. Then, we can combine our internal geometries with st_union() and simplify our external border slightly.\n\n# Validate internal borders\nml_borders_neat &lt;- st_make_valid(ml_borders)\n\n# Collapse internal borders to get single country border\nml_borders_out &lt;- ml_borders_neat |&gt; \n  st_union() |&gt; \n  st_simplify(dTolerance = 1000) |&gt; \n  st_as_sf()\n\nNow, we have both detailed internal borders as well as a single country border."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#chirts",
    "href": "posts/2024-04-15-chirts-metrics/index.html#chirts",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "CHIRTS",
    "text": "CHIRTS\nFor our temperature data, we’ll use the Climate Hazards Center Infrared Temperature with Stations (CHIRTS) dataset.2 CHIRTS provides daily estimates for several temperature metrics at a 0.05° (~5 kilometer) raster resolution. CHIRTS provides estimates of the following measures:\n\nDaily maximum air temperature (2 meters above ground)\nDaily minimum air temperature (2 meters above ground)\nDaily average relative humidity\nDaily average heat index\n\nThe most appropriate metric will depend on the nature of your research. For instance, relevant temperature metrics for studying the effects of heat on the human body are likely different from those used for studying agricultural productivity.\nFor health research, it’s also worth considering the specifics of the population of interest, as they may employ adaptive strategies or be at increased risk of heat exposure due to common pre-existing conditions or lifestyle features.3\nSince this post focuses specifically on R techniques (and not on methodological considerations), we’ll keep it simple and use daily maximum air temperature.\nThere are two ways to go about obtaining the CHIRTS data: either via manual download or via the chirps R package.\nManual download\nCHIRTS data for Africa can be downloaded directly from the Climate Hazards Center.\nData are distributed as NetCDF files, a common format for distributing scientific raster data structures. NetCDF files contain metadata about the file contents (for instance, about the temporal or spatial dimensions of the data), which will be useful later when we aggregate data to the monthly level.\nYou’ll notice that the files—each of which contains a full year’s worth of data for the entire continent of Africa—contain 3.3 Gigabytes of data apiece. For this demonstration, we’ll therefore only download a single year of data.\n\n\n\n\n\n\nClimatological Normals\n\n\n\nWhen dealing with climate data, it’s often necessary to have a long time series of data to establish a stable climatological normal, or baseline, to which to compare current observations. 30-year normals are commonly used, but their use has recently been questioned due to the rapidly changing conditions associated with climate change.4\nBecause of the space and time required to obtain data to calculate normals at a high resolution, we won’t be creating normals from CHIRTS in this post, but you may see them in the literature.\n\n\nWe’re working with the 2012 Mali sample from IPUMS DHS for this example, so we’ll download the Tmax.2012.nc file from the CHC listing. We’ve placed this file in the data directory.\nFortunately, we don’t need to learn any new tools to handle this file, as support for NetCDF is already built into terra. We can easily load the raster with rast():\n\nml_chirts &lt;- rast(\"data/Tmax.2012.nc\")\n\nIn this post we’ll be particularly interested in the temporal dimension of our raster data. Typically, a raster stack will represent time in layers, where each layer represents the recorded values at a particular point in time. terra’s SpatRaster objects have a built-in representation of time, which can be accessed with the time() function:\n\ntime(ml_chirts)\n#&gt;   [1] \"2012-01-01\" \"2012-01-02\" \"2012-01-03\" \"2012-01-04\" \"2012-01-05\"\n#&gt;   [6] \"2012-01-06\" \"2012-01-07\" \"2012-01-08\" \"2012-01-09\" \"2012-01-10\"\n#&gt;  [11] \"2012-01-11\" \"2012-01-12\" \"2012-01-13\" \"2012-01-14\" \"2012-01-15\"\n....\n\nWe can see that the temporal information contained in the NetCDF file was automatically included when we loaded this raster into R. Each of these dates correspond to a layer in the SpatRaster. This temporal representation will become useful when we aggregate temperature to the monthly level.\nCrop CHIRTS raster\nDownloading data from the CHC provides a raster for the entire African continent. We can greatly speed up our future processing by cropping this raster to our area of interest using the Mali borders that we loaded above.\nFirst, we’ll add a 10 kilometer buffer around the country border so that we retain the CHIRTS data just outside of the country as well. That way, if any DHS clusters fall in the border regions of the country, we will still be able to calculate temperature metrics in their general vicinity.\nWe’ve covered buffering in the past if you need to refresh your memory on this process.\n\n# Transform to UTM 29N coordinates, buffer, and convert back to WGS84\nml_borders_buffer &lt;- ml_borders_out |&gt; \n  st_transform(crs = 32629) |&gt; \n  st_buffer(dist = 10000) |&gt;\n  st_transform(crs = 4326)\n\nFinally, we can crop the CHIRTS raster to our buffered border region with terra’s crop():\n\nml_chirts &lt;- crop(ml_chirts, ml_borders_buffer, snap = \"out\")\n\nAccess via the chirps package\nYou may recall from our previous technical post that CHC data can be obtained via the chirps package in R. This package provides access to CHIRTS data as well.\nYou can obtain CHIRTS from the chirps package by providing a spatial boundary representing the area of interest for which the CHIRTS raster data should be obtained. You’ll also need to specify a temporal range and temperature variable.\nIn this case, we’ll use the buffered administrative borders for Mali that we downloaded earlier, specify the 2012 time range, and select the \"Tmax\" variable:\n\nml_chirts2 &lt;- chirps::get_chirts(\n  vect(ml_borders_buffer), \n  dates = c(\"2012-01-01\", \"2012-12-31\"),\n  var = \"Tmax\"\n)\n\n\n\nWe convert our administrative borders to a terra SpatVector object with vect() because this is the spatial structure expected by get_chirts().\nIn contrast to the NetCDF files provided when downloading CHIRTS data directly, obtaining data via the chirps package does not provide any temporal metadata:\n\ntime(ml_chirts2)\n#&gt;   [1] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n#&gt;  [26] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n#&gt;  [51] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA\n....\n\nSince we know that we have a full year of data, we can construct the temporal metadata manually. We’ll use the lubridate package to make this a little easier:\n\n# Convert strings to Date objects specifying year-month-day (ymd) format:\nstart &lt;- lubridate::ymd(\"2012-01-01\")\nend &lt;- lubridate::ymd(\"2012-12-31\")\n\n# Set time as a daily sequence of dates for all of 2012\ntime(ml_chirts2) &lt;- seq(start, end, by = \"days\")\n\ntime(ml_chirts2)\n#&gt;   [1] \"2012-01-01\" \"2012-01-02\" \"2012-01-03\" \"2012-01-04\" \"2012-01-05\"\n#&gt;   [6] \"2012-01-06\" \"2012-01-07\" \"2012-01-08\" \"2012-01-09\" \"2012-01-10\"\n#&gt;  [11] \"2012-01-11\" \"2012-01-12\" \"2012-01-13\" \"2012-01-14\" \"2012-01-15\"\n....\n\n\n\n\n\n\n\nCaution\n\n\n\nManually attaching time units works in this case because we know the CHIRTS data have no gaps. However, there’s no built-in check to ensure that you’re attaching the correct date to each layer of the raster stack, so you’ll want to be sure you know what time units are truly represented by each layer before assigning them manually.\n\n\nAt this point, we should have a daily raster for the region around Mali. We can take a peek by mapping the temperature distribution on a single day of our CHIRTS time series:\n\n\nWe’re not going to explicitly demonstrate how we produce our maps in this post since some are fairly complicated to set up. If you’re curious, you can peek at the collapsed code blocks to see how each of our maps are produced.\n\nShow plot functions# Add map scale bar and update guides\ntheme_dhs_map &lt;- function(show_scale = TRUE, continuous = TRUE) {\n  if (show_scale) {\n    scale &lt;- annotation_scale(\n      aes(style = \"ticks\", location = \"br\"), \n      text_col = \"#999999\",\n      line_col = \"#999999\",\n      height = unit(0.2, \"cm\")\n    )\n  } else {\n    scale &lt;- NULL\n  }\n  \n  if (continuous) {\n    guide &lt;- guides(\n      fill = guide_colorbar(draw.llim = FALSE, draw.ulim = FALSE)\n    )\n  } else {\n    guide &lt;- guides(\n      fill = guide_colorsteps(draw.llim = FALSE, draw.ulim = FALSE)\n    )\n  }\n  \n  list(scale, guide)\n}\n\n# Define custom palette functions so we can easily reproduce\n# color schemes across our maps in this post\nchirts_palettes &lt;- function() {\n  list(\n    main = c(\"#bad3e8\", \"#ffd3a3\", \"#da5831\", \"#872e38\"),\n    diff = c(\"#5B3794\", \"#8F4D9F\", \"#B76AA8\", \"#D78CB1\", \"#F1B1BE\", \"#F8DCD9\")\n  )\n}\n\n# Continuous fill scale for a selected palette\nscale_chirts_c &lt;- function(pal = \"main\", ...) {\n  pal &lt;- chirts_palettes()[[pal]]\n  colorRampPalette(pal, ...)\n}\n\n# ggplot2 layer for continuous scale for selected palette\nscale_fill_chirts_c &lt;- function(pal = \"main\", na.value = NA, ...) {\n  pal &lt;- scale_chirts_c(pal)\n  ggplot2::scale_fill_gradientn(colors = pal(256), na.value = na.value, ...)\n}\n\n# ggplot2 layer for binned scale for selected palette\nscale_fill_chirts_b &lt;- function(pal = \"main\", na.value = NA, ...) {\n  pal &lt;- scale_chirts_c(pal)\n  ggplot2::scale_fill_stepsn(colors = pal(256), na.value = na.value, ...)\n}\n\n\n\nShow plot code# Split raster into two at the country border to allow for differential \n# highlighting\nr_in &lt;- mask(ml_chirts[[1]], ml_borders_out, inverse = FALSE)\nr_out &lt;- mask(ml_chirts[[1]], ml_borders_out, inverse = TRUE)\n\n# Plot\nggplot() +\n  layer_spatial(r_in, alpha = 0.9, na.rm = TRUE) +\n  layer_spatial(r_out, alpha = 0.3, na.rm = TRUE) +\n  layer_spatial(ml_borders_neat, fill = NA, color = \"#eeeeee\") +\n  layer_spatial(ml_borders_out, fill = NA, color = \"#7f7f7f\", linewidth = 0.5) +\n  labs(\n    title = \"Daily Maximum Temperature: Mali\",\n    subtitle = \"January 1, 2012\",\n    fill = \"Temperature (°C)\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  ) +\n  scale_fill_chirts_c(limits = c(18, 50)) +\n  theme_dhs_map()"
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#average-monthly-temperature",
    "href": "posts/2024-04-15-chirts-metrics/index.html#average-monthly-temperature",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "1. Average monthly temperature",
    "text": "1. Average monthly temperature\nFor our first metric, we’ll take a simple approach and calculate an average monthly temperature.\nPreviously, we introduced terra’s mean() method, which allows you to calculate a single mean across all layers of a SpatRaster object. In this case, we also want to calculate a mean, but we need to adapt our approach so we can do so for each month independently.\nEnter terra’s tapp() function. tapp() allows you to apply a function to groups of raster layers. You can indicate which layers should be grouped together with the index argument. For instance, to independently calculate the mean of the first three layers and the second three layers, we could use the following index:\n\ntapp(\n  ml_chirts[[1:6]], # Select the first 6 layers to demo\n  fun = mean, \n  index = c(1, 1, 1, 2, 2, 2)\n)\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 2  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 \n#&gt; source(s)   : memory\n#&gt; names       :       X1,       X2 \n#&gt; min values  : 20.32146, 21.20080 \n#&gt; max values  : 35.77333, 35.86034\n\nNotice that this produces an output SpatRaster with 2 layers: the first represents the mean of the first 3 layers in the input, and the second represents the mean of the next 3 layers.\nHowever, manually identifying the index layers for each month of the year would be tedious and error-prone. Not only would we have to type out code to handle 366 days of data, but we would also have to contend with the fact that months vary in length. And depending on your time frame and region of interest, you may also need to account for leap years or entirely different calendars!\nFortunately, terra provides us with an easier way. Because we have a time component to our data, we can use the temporal metadata already attached to our raster as the index. For instance, to aggregate by month, simply use index = \"months\":\n\nml_chirts_mean &lt;- tapp(ml_chirts, fun = mean, index = \"months\")\n\nml_chirts_mean\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 12  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 \n#&gt; source(s)   : memory\n#&gt; names       :      m_1,      m_2,      m_3,      m_4,      m_5,      m_6, ... \n#&gt; min values  : 20.13009, 21.27224, 27.66812, 29.93578, 29.76501, 26.91157, ... \n#&gt; max values  : 35.44701, 38.60472, 40.72202, 42.85499, 44.10707, 47.46994, ... \n#&gt; time (mnts) : Jan to Dec\n\nAs expected, we now have 12 layers in our output SpatRaster (see the dimensions component of the output above). Each layer contains the average daily maximum temperature (in degrees Celsius) for the given month, as shown below.\n\nShow plot codelibrary(patchwork)\n\n# Helper to split raster layers into a list for small-multiple panel mapping\nsplit_raster &lt;- function(r) {\n  purrr::map(seq_len(nlyr(r)), function(i) r[[i]])\n}\n\n# Function to build individual panels for a small-multiple map using \n# continuous color scheme\nchirts_panel_continuous &lt;- function(x, \n                                    panel_title = \"\",\n                                    show_scale = TRUE,\n                                    ...) {\n  r_in &lt;- mask(x, ml_borders_out, inverse = FALSE)\n  \n  ggplot() + \n    layer_spatial(r_in, alpha = 0.9, na.rm = TRUE) +\n    layer_spatial(ml_borders_neat, fill = NA, color = \"#eeeeee\") +\n    layer_spatial(ml_borders_out, fill = NA, color = \"#7f7f7f\") +\n    labs(subtitle = panel_title, fill = \"Temperature (°C)\") +\n    scale_fill_chirts_c(...) +\n    theme_dhs_map(show_scale = show_scale) +\n    theme(\n      axis.text.x = element_blank(), \n      axis.text.y = element_blank(),\n      plot.subtitle = element_text(hjust = 0.5, size = 12),\n      panel.grid = element_blank()\n    )\n}\n\n# Split raster by layer\nr &lt;- split_raster(ml_chirts_mean)\n\n# Show scale only on final panel\nshow_scale &lt;- c(rep(FALSE, length(r) - 1), TRUE)\n\n# Panel labels\nmonths &lt;- c(\"January\", \"February\", \"March\", \"April\", \n            \"May\", \"June\", \"July\", \"August\",\n            \"September\", \"October\", \"November\", \"December\")\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_continuous(x, y, z, limits = c(18, 50))\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Average Monthly Temperature: Mali 2012\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\nNote that the CHC does provide a CHIRTS product that has been pre-aggregated to the monthly level. For projects relying on average monthly temperature, this is likely a better option than manually aggregating more fine-grained CHIRTS data.\nHowever, the advantage to working with daily CHIRTS data is that we have the flexibility to calculate our own custom monthly temperature metrics that aren’t necessarily provided out-of-the-box. We’ll demonstrate one in the next section."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#days-above",
    "href": "posts/2024-04-15-chirts-metrics/index.html#days-above",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "2. Days above a temperature threshold",
    "text": "2. Days above a temperature threshold\nAverage temperature may be a straightforward monthly temperature metric, but it doesn’t do a very good job of capturing acute temperature anomalies. When it comes to human health, evidence suggests that temperatures above certain thresholds are associated with physiological impairments, though the precise threshold depends on several factors.3 Regardless, these extreme events may be masked when we average across an entire month.\nTo explore this possibility, we’ll calculate the proportion of days in each month that exceed a certain raw temperature threshold. We’ll use 35°C as our threshold, which represents a value near the upper end of Mali’s temperature distribution1 and is similar to other commonly used (though occasionally questionable) thresholds.3\n\n\nTypically, you would want a more thorough justification of your threshold temperature. This post is not intended to represent a real analysis, so we select 35°C for the purposes of demonstration.\nLogical raster operations\nWe can exploit the fact that terra supports logical operations on SpatRaster objects to help us calculate this metric. For instance, we can compare the entire raster to a set value with the familiar &gt; operator:\n\nml_bin &lt;- ml_chirts &gt; 35\n\nThis produces a binary raster, where each pixel in each layer receives a logical value based on whether it is above or below 35 degrees Celsius (note that the min values and max values for each layer are now TRUE or FALSE):\n\nml_bin\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 366  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 \n#&gt; source      : spat_pIE8f2XA3mxHxoA_77362.tif \n#&gt; names       : Tmax_1, Tmax_2, Tmax_3, Tmax_4, Tmax_5, Tmax_6, ... \n#&gt; min values  :  FALSE,  FALSE,  FALSE,  FALSE,  FALSE,  FALSE, ... \n#&gt; max values  :   TRUE,   TRUE,   TRUE,   TRUE,   TRUE,   TRUE, ... \n#&gt; time (days) : 2012-01-01 to 2012-12-31\n\nWe can use this to count the number of days above the 35°C threshold for each pixel in our raster. We simply need to sum the binary raster layers within each month to count the number of days that exceed the threshold at each pixel location.\n\n\nWhen treating logical values as numeric, TRUE is treated as a 1 and FALSE is treated as a 0.\nHowever, we also need to account for the fact that not all months contain the same number of days. We therefore produce a proportion of each month’s days that meet our temperature threshold by dividing by the number of days in each month.\nFor binary data, this turns out to be the same as calculating a mean, so we can use a similar approach as we did when calculating average monthly temperature. The difference is that we now provide our binary raster (ml_bin) to the tapp() function:\n\nml_prop &lt;- tapp(ml_bin, fun = mean, index = \"months\")\n\nOnce again, we end up with a SpatRaster with 12 layers. However, in this case, each raster pixel reflects the proportion of days above 35° in that month.\n\nShow plot code# Function to build individual panels for a small-multiple map using \n# binned color scheme\nchirts_panel_binned &lt;- function(x, \n                                panel_title = \"\",\n                                fill_label = \"\",\n                                show_scale = TRUE,\n                                ...) {\n  r_in &lt;- mask(x, ml_borders_out, inverse = FALSE)\n  \n  ggplot() + \n    layer_spatial(r_in, alpha = 0.9, na.rm = TRUE) +\n    layer_spatial(ml_borders_neat, fill = NA, color = \"#eeeeee\") +\n    layer_spatial(ml_borders_out, fill = NA, color = \"#7f7f7f\") +\n    labs(subtitle = panel_title, fill = fill_label) +\n    scale_fill_chirts_b(...) +\n    theme_dhs_map(show_scale = show_scale) +\n    theme(\n      axis.text.x = element_blank(), \n      axis.text.y = element_blank(),\n      plot.subtitle = element_text(hjust = 0.5, size = 12),\n      panel.grid = element_blank(),\n      legend.ticks = element_blank()\n    )\n}\n\n# Split raster by layer\nr &lt;- split_raster(ml_prop)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    n.breaks = 8, \n    limits = c(0, 1), \n    fill_label = \"Proportion of Days Above 35°C\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Proportion of Days Above Threshold\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\nThis approach reveals a bit more detail about the consistency of the temperature exposure during certain months. As we can see, some months are spent almost entirely above 35°C across the country. This continual exposure may be more strongly related to health than the averages we calculated in the section above."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#heatwaves-consecutive-days-above-a-temperature-threshold",
    "href": "posts/2024-04-15-chirts-metrics/index.html#heatwaves-consecutive-days-above-a-temperature-threshold",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "3. Heatwaves: Consecutive days above a temperature threshold",
    "text": "3. Heatwaves: Consecutive days above a temperature threshold\nContinual exposure to high temperatures likely has a more pronounced health impact than isolated hot days. Accordingly, most heatwave definitions attempt to identify sequences of days that meet certain temperature criteria.\nWe can build upon our previous temperature metric to produce a simple heatwave definition that identifies all days that belong to a sequence of 3+ days in a row that all meet the 35°C threshold used earlier.\nHow should we go about identifying sequences of days? To simplify things, let’s pull out the daily CHIRTS values for a single pixel in our raster to use as an example:\n\n# Extract values from a single pixel of CHIRTS data\npx1 &lt;- as.numeric(ml_chirts[1, 1])\n\npx1\n#&gt;   [1] 22.08196 22.25259 24.63318 25.46410 25.11430 24.69748 25.71723 27.64781\n#&gt;   [9] 25.00261 22.27929 24.83206 22.42412 24.87811 28.53550 27.20606 26.75240\n#&gt;  [17] 22.12105 22.98229 20.55801 19.10251 20.74286 22.91010 22.37615 22.23903\n....\n\nAs we demonstrated above, it’s easy to identify the layers that are above a given threshold:\n\npx1_bin &lt;- px1 &gt; 35\n\npx1_bin\n#&gt;   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt;  [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n#&gt;  [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n....\n\nBut how do we extract sequences from this vector? One option is to exploit the features of run-length encoding. Run-length encoding represents a vector of values as a sequence of runs of a single value and the length of that run.\nWe can use the rle() function from base R to convert to run-length encoding:\n\npx1_rle &lt;- rle(px1_bin)\n\npx1_rle\n#&gt; Run Length Encoding\n#&gt;   lengths: int [1:23] 81 1 16 1 30 11 3 3 7 1 ...\n#&gt;   values : logi [1:23] FALSE TRUE FALSE TRUE FALSE TRUE ...\n\nThe output shows us that the px1_bin vector starts with a run of 81 FALSE values, then has a run of 1 TRUE value, then 16 FALSE values, and so on. As you can see, run-length encoding provides us both with information about the values and the sequencing of our input vector.\nIf we define a heatwave as any sequence of 3+ days above the temperature threshold, each heatwave will be represented by entries with a value of TRUE (days that exceeded the threshold) and a length (number of days in a row) of 3 or more. We can use logical operations to identify whether each run is a heatwave or not:\n\n# Identify days that were above threshold and belonged to a 3+ day sequence\nis_heatwave &lt;- px1_rle$values & (px1_rle$lengths &gt;= 3)\n\nis_heatwave\n#&gt;  [1] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE  TRUE\n#&gt; [13] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE\n\nSumming this vector will give us the number of unique heatwave events during the year for this sample pixel:\n\nsum(is_heatwave)\n#&gt; [1] 8\n\nWe could also get the proportion of days that are within those heatwaves by summing the lengths of all the heatwave events and dividing by the number of total days:\n\n# Extract lengths for each heatwave event\nheatwave_lengths &lt;- px1_rle$lengths[is_heatwave]\n\nheatwave_lengths\n#&gt; [1]  11   3   4 101   4   8   3   7\n\n# Proportion of days belonging to a heatwave\nsum(heatwave_lengths) / length(px1)\n#&gt; [1] 0.3852459\n\nCustom functions\nIf we want to calculate the proportion of heatwave days across our entire raster, we can again return to tapp(). However, there’s no built-in function (like mean) that we can use to do the processing we walked through above.\nInstead, we must provide our own custom function to indicate the processing that tapp() should perform on each set of layers. To define a function, we use the function() keyword along with several arguments.\n\n\n\n\n\n\nNote\n\n\n\nA function’s arguments are the input parameters that the user can set when calling the function.\n\n\nAt its simplest, all this requires is copying the code we’ve already written above (we’ve consolidated the code into single lines in some cases):\n\nprop_heatwave &lt;- function(temps) {\n  # Convert to RLE of days above threshold\n  bin_rle &lt;- rle(temps &gt;= 35)\n  \n  # Identify heatwave events based on sequence length\n  is_heatwave &lt;- bin_rle$values & (bin_rle$lengths &gt;= 3)\n  \n  # Count heatwave days and divide by total number of days\n  sum(bin_rle$lengths[is_heatwave]) / length(temps)\n}\n\nNote that we do the exact same processing as we did in our step-by-step walkthrough. The only difference is that instead of using the px1 variable (which contains the values for a single pixel), we instead use a more general argument, which we call temps. temps stands in for any arbitrary input vector that the function user can provide (we’ve named it temps to help make it clear that this should be a vector of temperature values). This means that we can easily run the same calculation on different vectors:\n\n# Extract 2 pixels for demonstration\npx1 &lt;- as.numeric(ml_chirts[1, 1])\npx2 &lt;- as.numeric(ml_chirts[1, 2])\n\n# Calculate heatwave proportions for each pixel\nprop_heatwave(px1)\n#&gt; [1] 0.3852459\n\nprop_heatwave(px2)\n#&gt; [1] 0.3852459\n\nWriting a more flexible function\nRight now, the user of the function has no way to modify the temperature threshold or sequence length used in the heatwave definition, because those values (35 and 3) are hard-coded into our function.\nIf we move these values to the function arguments, we allow the user to decide what values these parameters should take. Here is a modified version of prop_heatwave() that does this:\n\nprop_heatwave &lt;- function(temps, thresh, n_seq) {\n  # Convert to RLE of days above threshold\n  bin_rle &lt;- rle(temps &gt;= thresh)\n  \n  # Identify heatwave events based on sequence length\n  is_heatwave &lt;- bin_rle$values & (bin_rle$lengths &gt;= n_seq)\n  \n  # Count heatwave days and divide by total number of days\n  sum(bin_rle$lengths[is_heatwave]) / length(temps)\n}\n\nOur function now has a thresh argument and an n_seq argument. Where we previously would have compared our input temps vector to the threshold of 35, we now compare it to the value the user provides to thresh. Similarly, where we previously would have used sequences of length 3 or more, we now use the sequence length the user provides to n_seq.\nUsing the values of 35 and 3 produces the same heatwave proportion as above:\n\nprop_heatwave(px1, thresh = 35, n_seq = 3)\n#&gt; [1] 0.3852459\n\nBut now we can easily change our inputs to calculate modified heatwave definitions. For instance, to find the proportion of days in 4+ day heatwaves of at least 37°C:\n\nprop_heatwave(px1, thresh = 37, n_seq = 4)\n#&gt; [1] 0.3114754\n\nThis example demonstrates how function arguments can be used to produce a more flexible function that can be easily applied across a range of input parameters. Building functions in this way has a bit of an up-front cost, but it often pays for itself by making your future analysis much more robust and scalable.\nScaling up\nNow that we have a function to calculate our heatwave definition, we can provide it to tapp() with our desired temperature threshold and sequence parameters.\n\n# Apply our custom heatwave counter to each month\nml_heatwave_prop &lt;- tapp(\n  ml_chirts, \n  fun = function(x) prop_heatwave(x, thresh = 35, n_seq = 3), \n  index = \"months\"\n)\n\n\n\n\n\n\n\nAnonymous Function Syntax\n\n\n\nOur fun argument above is written in anonymous function syntax. It may look a little complex, but remember that tapp() expects you to provide a function to its fun argument. Whichever function you provide should be a function of the vector of values for each pixel in the input raster.\nfunction(x) prop_heatwave(x, ...) indicates that we want to provide each of these vectors x to our prop_heatwave() function (as the temps argument). The thresh and n_seq arguments are fixed across all pixels. (In this case, x is just a placeholder to reference the vector inputs to our function. We could just as easily use another name, but x is traditional and concise.)\nWhy didn’t we use this syntax in previous sections? Well, it turns out that fun = mean was simply a shorthand. Writing fun = function(x) mean(x) would have also worked!\n\n\nLet’s see what the distribution of heatwave days looks like under our latest definition:\n\nShow plot code# Split raster by layer\nr &lt;- split_raster(ml_heatwave_prop)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    n.breaks = 8, \n    limits = c(0, 1),\n    fill_label = \"Proportion of days in a heatwave\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Proportion of heatwave days\",\n    subtitle = \"Heatwaves as sequences of 3+ days of 35°C\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\nBecause we’ve built a flexible prop_heatwave() function, we can easily calculate a different heatwave definition. For heatwaves of 4+ days of 37°C+, for instance:\n\nml_heatwave_prop2 &lt;- tapp(\n  ml_chirts, \n  fun = function(x) prop_heatwave(x, thresh = 37, n_seq = 4), \n  index = \"months\"\n)\n\n\nShow plot code# Split raster by layer\nr &lt;- split_raster(ml_heatwave_prop2)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    n.breaks = 8, \n    limits = c(0, 1),\n    fill_label = \"Proportion of days in a heatwave\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Proportion of heatwave days\",\n    subtitle = \"Heatwaves as sequences of 4+ days of 37°C\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\nIt can be difficult to see the differences side-by-side, but we can subtract the output rasters to easily examine the differences between the two definitions:\n\nml_heatwave_diff &lt;- ml_heatwave_prop2 - ml_heatwave_prop\n\n\nShow plot code# Convert 0 values to NA for transparency\nNAflag(ml_heatwave_diff) &lt;- 0\n\n# Split raster into layers\nr &lt;- split_raster(ml_heatwave_diff)\n\n# Create map panels\npanels &lt;- purrr::pmap(\n  list(r, months, show_scale),\n  function(x, y, z) chirts_panel_binned(\n    x, \n    y, \n    z, \n    pal = \"diff\", \n    n.breaks = 8, \n    limits = c(-1, 0),\n    fill_label = \"Difference in proportion of days in heatwave\"\n  )\n)\n\n# Plot\nwrap_plots(panels) +\n  plot_layout(guides = \"collect\", ncol = 4) +\n  plot_annotation(\n    title = \"Difference in heatwave proportion metrics\",\n    subtitle = \"37°C 4+ days vs. 35°C 3+ days heatwave definitions\",\n    caption = \"Source: Climate Hazards Center Infrared Temperature with Stations\"\n  )\n\n\n\n\n\n\n\nAs we can see, modifying our definition does indeed produce meaningful differences in the prevalence of heatwave days in various parts of the country. Which definition is appropriate will depend on your particular research questions and aims. However, building custom functions into your workflow will make it much easier to quickly compare different definitions, facilitating sensitivity analyses and robustness checks."
  },
  {
    "objectID": "posts/2024-04-15-chirts-metrics/index.html#getting-help",
    "href": "posts/2024-04-15-chirts-metrics/index.html#getting-help",
    "title": "Flexible Workflows with CHIRTS Temperature Data",
    "section": "Getting Help",
    "text": "Getting Help\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html",
    "href": "posts/2024-02-23-rainfall-insights/index.html",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "",
    "text": "One barrier to climate change and health research is that many surveys—including the The Demographic and Health Surveys Program (DHS)—do not collect detailed information on local environmental conditions in survey areas. Fortunately, organizations like IPUMS DHS are now taking steps to use DHS enumeration cluster coordinates to link a limited set of climate variables—including monthly and long-term average precipitation—to individual and household records within the DHS.1\nHowever, in many cases, these pre-calculated time series may not meet researchers’ needs because they reflect only average conditions and not the experience of the specific surveyed households. To obtain a more nuanced understanding of a survey response’s environmental context, some researchers may choose to calculate and link climate variables themselves. We introduced some methods for doing so in our recent post, where we demonstrated how to integrate raw precipitation data from the Climate Hazards Center InfraRed Precipitation with Station Dataset (CHIRPS) with survey data from IPUMS DHS.\nThis post aims to serve as a guide for new researchers wading into research on the health impacts of climate events. While we focus on rainfall, it is important to note that there are significant interactions between rainfall and other variables such as temperature and vegetation. Furthermore, rainfall alone may be a crude measure; for instance, Grace et al. (2021) highlight multiple and nuanced ways in which precipitation can impact health.2 Nevertheless, this blog post outlines key concepts to consider when thinking through the health impacts of precipitation patterns, selecting a data source, and operationalizing precipitation data. We draw examples from recent publications that utilize DHS data; however, their inclusion does not constitute an endorsement of the publications themselves."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#impacts-of-rainfall-on-human-health",
    "href": "posts/2024-02-23-rainfall-insights/index.html#impacts-of-rainfall-on-human-health",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Impacts of rainfall on human health",
    "text": "Impacts of rainfall on human health\nRainfall can impact human health in both direct and indirect ways, with important consequences when linking precipitation data to survey responses. For instance, researchers interested in indirect mechanisms may need to incorporate a time lag between climate events and survey response dates, since the impact of rainfall events in one period may not appear until later.3\nBelow, we outline several of the most common pathways through which precipitation impacts health outcomes.\n\nDisasters and precipitation extremes\nDirect and immediate impacts on human health\nExtreme rainfall can result in flooding or landslides, which may lead to death or injury. Furthermore, extreme events often contribute to indirect health impacts through the other pathways described below.\n\n\nAgriculture and nutrition\nIndirect impacts on human health\nRainfall directly impacts crop growth and agricultural productivity, which then affects food security and nutrition. It takes time for plants to grow, so the impacts on health and well-being may not be immediate.\nPrecipitation is not the only variable that can be used to explore these pathways; Normalized Difference Vegetation Index (NDVI) can also be used as a proxy for food security.2\n\n\nInfectious diseases\nIndirect impacts on human health\nRainfall can impact the transmission of enteric disease like cholera (through its effects on sanitation practices, water quality, and hygiene) as well as vector-borne diseases like malaria and dengue (through its effects on water supply and vector habitats).\nIdeally, researchers would be able to explore these pathways through more proximate measures of transmission risks like disease incidence. However, in resource-poor settings these data may not be available at fine temporal and spatial resolutions.\n\n\nCrime and violence\nIndirect impacts on human health\nRainfall shocks may be linked to crime and violence through their impacts on economic activities, stress, and competition for limited resources. For instance, rainfall impacts on economic security may lead to herder and farmer conflicts."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#choosing-a-rainfall-data-source",
    "href": "posts/2024-02-23-rainfall-insights/index.html#choosing-a-rainfall-data-source",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Choosing a rainfall data source",
    "text": "Choosing a rainfall data source\nThe specific research question and estimation strategy should play a central role in determining the most suitable rainfall data source. Researchers have various factors to consider when making this choice, including the available time span4,5, the manner in which data were collected (for instance, accurate ground-based station data versus satellite data),6 and the extent of coverage for the geographic location of interest.7\n\nData accuracy\nIt’s important for researchers to recognize the trade-offs between data accuracy and coverage. In many cases, more accurate products will have more limited geographic coverage, so researchers with a small area of interest may be able to use a more accurate data product.\n\n\nTemporal resolution\nResearchers must also consider the temporal resolution of the data (e.g. hourly, daily, monthly, etc.). An appropriate temporal resolution is likely to depend on the characteristics of the survey data that will be joined to the environmental data. For instance, it may not be necessary to use hourly precipitation data when linking to the DHS, since survey response dates are often provided only at the monthly level. However, it is always possible to aggregate fine-grained data to a larger temporal scale, and fine-grained data may provide more flexibility in the way aggregation is carried out.\n\n\nSpatial resolution\nThe majority of rainfall data is available in raster format, wherein precipitation data are stored in a grid of cells, each with a particular precipitation value. If the grid cells are very large and encompass many DHS clusters, there won’t be enough variation to exploit in an analysis. On the other hand, if the grid cells are less than 10 kilometers across (a plausible buffer size around DHS cluster coordinates), then their values will need to be aggregated within each DHS cluster region.\n\n\nData accessibility\nOf course, data availability and accessibility often drive data source decisions. Researchers may opt to use data that they or collaborators have previously used or have easy access to in order to reduce the difficulty of setting up an analysis with a new source.\n\nComparison of Selected Precipitation Data Sources\n\n\nName\nFinest Resolution\nGeo. Range\nTemp. Range\nTime Step\nRainfall Data Source\n\n\n\n\nCHIRPS\n0.05°\n50°S-50°N (all longitudes)\n1981-near present\nDaily, pentad, dekad, monthly, 2-monthly, 3-monthly, annual\nWeather station records and geostationary satellite observations\n\n\nCRU TS\n0.5°\nAll land except Antarctica\n1901-2022\nMonthly\nWeather station records\n\n\nUDEL-TS\n0.5°\n89.75°N-89.75°S, 0.25°E-359.75°E\n1901-2014\nMonthly\nWeather station records\n\n\nGPCC\n0.25°\n90.0°N-90.0°S, 0.0°E-360.0°E\n1891-2019\nMonthly\nWeather station records\n\n\nERA5\n0.25°\nGlobal\n1940-present\nHourly, monthly\nReanalysis (model estimates from satellite data assimilation)"
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#measuring-rainfall",
    "href": "posts/2024-02-23-rainfall-insights/index.html#measuring-rainfall",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Measuring rainfall",
    "text": "Measuring rainfall\nScholars frequently aggregate total rainfall over an interval (for instance, to calculate annual, seasonal, or monthly precipitation summaries). Additionally, researchers often generate long-term rainfall averages for use in identifying anomalies and extreme rainfall events.3 It is imperative that researchers justify their choice of temporal aggregation.\nIn this section, we summarize several publications that highlight different techniques for measuring rainfall.\n\nTotal precipitation\nStudies that simply use raw precipitation totals are able to test whether additional rainfall has a negative or positive impact on the outcome of interest.8\n\nTotal precipitation during the growing season\nSome studies restrict their focus to a region’s growing season to better assess the impacts of climate conditions on agricultural production and—indirectly—nutrition and food security.9\n\n\nTotal rainy season precipitation\nSpecific details of the geographical area of interest may inform the manner in which data are aggregated. For instance, Randell, Gray, and Shayo (2022) take region-specific rainfall patterns into account in research on Tanzania, where depending on the region there might be one (Msimu) or two rainy seasons (Masika and Vuli) during the year.10\n\n\n\nRainfall variability and anomalies\nMeasures of rainfall variability consider how rainfall at a specific time and location compares to the long-term average rainfall in that same location. However, several measures of variability exist, and information should be provided to explain how the reference period was chosen and how results change if the reference period changes.\n\nAnnual variability\n\nRainfall deviation percentile can be used to quantify how a specific rainfall event compares to historical data for that same location. For instance, in Epstein et al. (2023), the 50th percentile represents a year with median rainfall levels compared to 29 previous years; numbers closer to 0 represent drier than average years and closer to 1 represent wetter years.11\nRainfall Z-scores quantify how many standard deviations a rainfall event differs from the mean. Negative Z-scores indicate below-average rainfall while positive Z-scores indicate above-average rainfall.12\nStandardized Precipitation Index (SPI) is similar to the Z-score but first corrects for the skew found in rainfall distributions by transforming the data using a gamma distribution.5\n\n\n\nSeasonal variability\nRainfall is often seasonal, and depending on the health pathway of interest, researchers should think critically about how seasons may impact their analyses. Rainfall during the dry season is likely to have a different effect than rainfall in the wet season, for instance.13\n\nRandell et al. (2021) calculate Z-scores for 2015 monsoon rainfall based on monsoon rainfalls during 1980-2015 reference period.14\nOmiat and Shively (2020) calculate deviation during the main rainfall season of the survey year and of the previous year.15\nAbiona (2017) calculates the percentage deviation from the mean agricultural season average. Specifically, this was generated using the natural logarithm of the current agricultural season minus the 30-year historical average for the same locality.4\n\n\n\nClimate variability during specific windows in the respondent’s life course\nEnvironmental analyses should be tied to survey responses during specific exposure windows to determine the impact of extremes.3 For instance, research on the impact of precipitation on weight at birth may want to focus on precipitation anomalies during the months of gestation. Or, if gestational length is unknown, researchers may choose to use anomalies during the 12 months prior to birth. It is often necessary to make approximations like this based on the available data.\n\nIn studying in-utero rainfall variability, Le and Nguyen (2021) use “the deviation of the nine-month in-utero rainfall from the long run average of total rainfall during those nine months”.6 In this case, the long-run average was based on data from 1981-2018. The authors further dichotomized their results in order to more easily summarize the impact of wet versus dry shocks.\n\n\n\n\nPrecipitation extremes: Floods and droughts\nIt is uncommon for researchers to use rainfall data alone to define a flood event. Instead, an indicator for flood is generated based on precipitation anomalies. Consequently, we recommend using the term extreme rainfall rather than flood event when relying solely on rainfall data.\nResearchers often use rainfall data alone to identify droughts, though in these instances they are really capturing meteorological droughts.16 Depending on the type of drought of interest, researchers may want to use a combination of temperature, rainfall, and other data sources to identify droughts associated with low soil moisture (agricultural droughts), low ground water, or surface runoff. As with all environmental metrics, researchers should think critically about time scales when operationalizing drought; a drought lasting 1 month is likely to have a very different effect than a drought lasting 6 or 12 months.\nSpecific drought indices are also available to identify drought conditions. The Standardized Precipitation Evaporation Index (SPEI) takes into account rainfall as well as evaporation, and the Palmer Drought Severity Index (PDSI) includes rainfall, evapotranspiration, and runoff.\nBelow we highlight some ways researchers using DHS data have used rainfall data to quantify the impacts of extreme precipitation. These examples illustrate that there are a wide range of cutoffs used to identify droughts. While this makes comparisons difficult, it’s appropriate that drought should be defined specific to the place, time, and water use patterns of a given region.\n\nExtreme rainfall: rainfall deviation ≥ 90th percentile17\nFlood: rainfall deviation &gt; 75th percentile4\nDrought: rainfall deviation is &lt; 25th percentile4\nDrought: binary variable for rainfall ≤ 15th percentile18\nDrought: SPI values &lt; -1.55\nDrought: classified as ordinal categorical variable:19\n\nSevere (≤ 10 percentile)\nMild/moderate (&gt; 10th percentile to ≤ 30th percentile)\nNone (&gt; 30th percentile)"
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#context-matters",
    "href": "posts/2024-02-23-rainfall-insights/index.html#context-matters",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Context matters",
    "text": "Context matters\n\nHow might the same shock have different effects in different regions?\nStudies that span geographic areas with heterogeneous climatic zones and other characteristics often stratify or interact rainfall with regional characteristics. This is because the same rainfall event most likely does not have the same impact on outcomes in areas with different geographic features (for instance, in arid versus non arid areas,20 rural versus urban areas,21 or on individuals with different livelihoods22).\nThis highlights the most important point when working with climate data: no one-size-fits-all approach exists. The manner in which data are selected, operationalized, and analyzed must be consistently informed by the physical and cultural specifics of the geographic region under consideration.\nWe hope that these drops of wisdom are a good starting point for your research."
  },
  {
    "objectID": "posts/2024-02-23-rainfall-insights/index.html#getting-help",
    "href": "posts/2024-02-23-rainfall-insights/index.html#getting-help",
    "title": "Droplets of Insights for Integrating DHS and Rainfall Data",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html",
    "href": "posts/2024-02-01-getting-started-with-r/index.html",
    "title": "An Introduction to R",
    "section": "",
    "text": "The IPUMS DHS Climate Change and Health Research Hub is designed to introduce spatial data concepts for researchers who work with population health survey data.\nTo do so, the blog will provide both conceptual content describing ways in which spatial data can be appropriately included in population research as well as technical content demonstrating how to implement some of these approaches using statistical software.\nTechnical content will rely primarily on the R programming language. Not only is R one of the most popular platforms for data analysis around the world, but its open-source nature aligns well with IPUMS values. R is freely available for Windows, MacOS, and a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux)."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html#getting-started-with-r",
    "href": "posts/2024-02-01-getting-started-with-r/index.html#getting-started-with-r",
    "title": "An Introduction to R",
    "section": "Getting started with R",
    "text": "Getting started with R\nTo download R, visit the Comprehensive R Archive Network (CRAN) and choose the appropriate download link for your operating system.\n\n\n\n\n\n\nR Version Requirements\n\n\n\nThis blog is designed to work with R 4.1.0 and later. The 4.1.0 release included several notable updates that are used in this blog, including the introduction of the native pipe operator |&gt;.\nIf you’ve previously downloaded R, you’ll need to update your R version to run some of the code presented in this blog.\n\n\nThere are countless free resources available for learning R, from foundational knowledge to niche topics. Here are a few of our favorite resources to help you get started:\n\n\nR for Data Science for beginners\n\nAdvanced R for a deeper dive\n\nRSpatial and Geocomputation with R for analysis with spatial data\n\nggplot2 for data visualization\n\nMastering Shiny for interactive applications\n\nR Markdown: The Definitive Guide for producing annotated code, word documents, presentations, web pages, and more\n\nR-bloggers for regular news and tutorials\n\nAdditionally, you can always get help on Stack Overflow and—for packages hosted on GitHub—their GitHub issues page. No matter what question you have, you’re unlikely to be the first person to encounter it, so it’s always worth checking to see whether your problem has been solved in the past."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html#essential-packages",
    "href": "posts/2024-02-01-getting-started-with-r/index.html#essential-packages",
    "title": "An Introduction to R",
    "section": "Essential packages",
    "text": "Essential packages\nSeveral packages will come into frequent use on this blog.\nipumsr\nThe ipumsr package contains functions that make it easier to load IPUMS data into R.\n\n\n\n   © IPUMS (MPL-2.0) \nFor IPUMS DHS, the most relevant of these is read_ipums_micro(), which allows you to load a fixed-width data file along with associated metadata.\nipumsr also contains functions to interact with variable metadata after loading, like ipums_var_info(), ipums_val_labels(), and so on.\nAs mentioned above, ipumsr also contains client tools for interacting with the IPUMS API. This allows users to request and download IPUMS data entirely within their R environment. While not available for IPUMS DHS, users of supported IPUMS collections can learn more from the API workflows introduced on the ipumsr website\ntidyverse\nThe tidyverse package actually refers to a family of related packages. Installing tidyverse will actually install each of these component packages:\n\n\n\n   © RStudio, Inc. (MIT) \n\n\nggplot2 for data visualization\n\ndplyr for data manipulation\n\ntidyr for data tidying\n\nreadr for data import\n\npurrr for functional programming\n\ntibble for tibbles (a modern re-imagining of data frames)\n\nstringr for string manipulation\n\nforcats for factor handling\n\nIt’s possible to call library(tidyverse) to load all the packages in the tidyverse collection, but in most cases it’s best to individually load the specific packages you need for a given R script. This allows you and other users to more easily identify the specific packages required to run your code. It also makes your code more accessible—other users won’t have to have the entire tidyverse collection installed to run your code, only the specific packages that are actually required.\nIn general, this blog will follow the tidyverse style guide where possible. So-called “tidy” conventions are designed with the express purpose of making code and console output more human readable.\n\n\n\n\n\n\nTip\n\n\n\nSometimes, human readability imposes a performance cost: in our experience, IPUMS DHS datasets are small enough that this is not an issue. For larger datasets, we recommend exploring the data.table package instead.\n\n\nsf\n\n\n\n   © Edzer Pebesma (GPL-2 | MIT) \nsf, which stands for simple features, is the main R package for working with spatial vector data.\nsf represents spatial data in a “tidy” format that resembles those used by tidyverse packages mentioned above. sf objects contain tabular data along with a record of the geometry associated with each individual record.\nThis format makes it easy to perform spatial operations, attach spatial information to non-spatial data sources (like DHS surveys), and generate maps.\nterra\n\n\n\n   © Robert J. Hijmans et al. (GPL &gt;=3) \nterra provides a general framework for working with spatial data in both raster and vector format.\nWhile sf provides an alternative approach to working with vector data, terra’s raster handling stands alone and provides robust methods to quickly index, aggregate, and manipulate raster data.\nBecause of its speed and simplicity, terra has superseded the long-lived raster package, which is being retired. You may still see online resources that reference the raster package, but we suggest relying only on terra."
  },
  {
    "objectID": "posts/2024-02-01-getting-started-with-r/index.html#getting-help",
    "href": "posts/2024-02-01-getting-started-with-r/index.html#getting-help",
    "title": "An Introduction to R",
    "section": "Getting Help",
    "text": "Getting Help\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html",
    "href": "posts/2024-04-09-spatial-harmonization/index.html",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "",
    "text": "The Demographic and Health Surveys (DHS) are the leading source of population and health data for low and middle-income countries. The data from the nationally representative DHS samples can be disaggregated by regions and, in some cases, by sub-regions, such as districts. However, changes in the boundaries of reported regions are common, due to either shifts in the survey sampling strategy or due to political changes within countries. Researchers using DHS data to study change over time need to hold space constant by using regions with a consistent geographic footprint to get meaningful results.\nIPUMS DHS has addressed the issue of changing boundaries by providing users with both spatially harmonized (or integrated) and sample-specific geographic variables and GIS boundaries. Harmonized variables provide consistent geographic units for a country across the sample years, facilitating analytical comparisons across time. Sample-specific variables retain all geographic detail from each sample and are not consistent over time.\n\n\nDHS survey boundaries are obtained from the DHS Program’s Spatial Data Repository and overlaid on a GIS software system. When geographic boundaries of a later sample do not align with units of the previous DHS survey for that country because of boundary changes, we create larger aggregated units that are stable over time. We refer to this process as the spatial harmonization of geographic boundaries.\nIf administrative units split or merge, the harmonized unit will have the boundaries of the largest version of the unit. If a territory is redistributed between two or more units, the units are combined. Some detail is always lost in the process of spatial harmonization because administrative units are merged to find the geographic least common denominator.\nIn a few cases, boundaries have been reorganized to such an extent that harmonization is nearly impossible. In such cases (e.g., for Cameroon, Guinea, Jordan, and Mali), we have typically created sets of consistent units spanning different year ranges and placed them in different integrated variables. Sometimes early DHS surveys labeled regions through vague descriptive terms (e.g., “Coast” or “Mountains”) rather than naming recognized political units. In such cases, we have provided only sample-specific geographic variables (e.g. for Congo Brazzaville, Cote D’Ivoire, and Yemen)."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#the-spatial-harmonization-process",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#the-spatial-harmonization-process",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "",
    "text": "DHS survey boundaries are obtained from the DHS Program’s Spatial Data Repository and overlaid on a GIS software system. When geographic boundaries of a later sample do not align with units of the previous DHS survey for that country because of boundary changes, we create larger aggregated units that are stable over time. We refer to this process as the spatial harmonization of geographic boundaries.\nIf administrative units split or merge, the harmonized unit will have the boundaries of the largest version of the unit. If a territory is redistributed between two or more units, the units are combined. Some detail is always lost in the process of spatial harmonization because administrative units are merged to find the geographic least common denominator.\nIn a few cases, boundaries have been reorganized to such an extent that harmonization is nearly impossible. In such cases (e.g., for Cameroon, Guinea, Jordan, and Mali), we have typically created sets of consistent units spanning different year ranges and placed them in different integrated variables. Sometimes early DHS surveys labeled regions through vague descriptive terms (e.g., “Coast” or “Mountains”) rather than naming recognized political units. In such cases, we have provided only sample-specific geographic variables (e.g. for Congo Brazzaville, Cote D’Ivoire, and Yemen)."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#countries-and-samples",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#countries-and-samples",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "Countries and samples",
    "text": "Countries and samples\nIn this demonstration, we will highlight the sub-Saharan countries of Malawi, Tanzania, Mozambique, Zambia, and Zimbabwe. The table below lists these countries and their sample years and notes the changes in boundaries for each country. For Malawi, we use sub-regions or the second level administrative units; here, 28 sub-regions in 2010 increased to 32 sub-regions in 2016. For Tanzania and Zambia, the regional boundaries changed from one sample to the next. For Zimbabwe, there were no changes in regional boundaries from one sample year to another. We also display a single sample for Mozambique in our year 1 maps. Note that, for the purposes of this demonstration, we do not show all the sample years available in IPUMS DHS, but rather only selected sample years.\n\nSummary of samples used in this demonstration\n\n\n\n\n\n\n\n\nCountry\nSurvey Year 1\nSurvey Year 2\nBoundary changes?\n\n\n\n\nMalawi\n2010\n2016\nYes—28 sub-regions to 32 sub-regions\n\n\nTanzania\n2010\n2015\nYes—26 regions to 30 regions\n\n\nMozambique\n2011\n–\nNo—single year with 11 regions\n\n\nZambia\n2007\n2013\nYes—9 regions to 10 regions\n\n\nZimbabwe\n2010\n2015\nNo—10 regions in both years"
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#visualizing-the-percent-of-households-using-improved-drinking-water",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#visualizing-the-percent-of-households-using-improved-drinking-water",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "Visualizing the percent of households using improved drinking water",
    "text": "Visualizing the percent of households using improved drinking water\nAll maps shown below represent the percentage of households that use drinking water from improved sources. For the countries listed in the table above, paired maps display results from two time periods. Map A shows results from the earlier survey (i.e., 2010 Malawi, 2010 Tanzania, 2011 Mozambique, 2007 Zambia, and 2010 Zimbabwe) and Map B displays results from the later survey (i.e., 2016 Malawi, 2015 Tanzania, 2013 Zambia, and 2015 Zimbabwe). Shades of the color blue represent spatially harmonized maps; shades of the color green appear on sample-specific maps. In general, the darker the color, the higher the percentage of households using improved sources of drinking water.\nFigure 1 displays two sample-specific GIS maps, meaning the spatial boundaries for the two time periods for a given country are not the same. These maps show that, overall, the percentage of households using improved drinking water increased. The rate of increase differs, however, both between countries and within countries.\nWhile very broad conclusions about change based on sample-specific geography may be sound, a closer look at the maps shows problems with using sample-specific geography. We have circled three areas affected by changes in geography. Specifically, the changes consist of splitting of boundaries in eastern Zambia (the largest circle), reorganization of boundaries in northern Tanzania (the top circle), and separation of rural and urban areas in central Malawi (the right-hand circle).\n\n\n\n\nFigure 1: Percent of households that use improved sources of drinking water. Note that the maps do not show consistent spatial boundaries. The red circles indicate areas with changes in geographic boundaries.Map A shows 2010 data from Malawi, Tanzania, and Zimbabwe; 2011 data from Mozambique; and 2007 data from Zambia.Map B shows 2015 data from Tanzania and Zimbabwe; 2016 data from Malawi, and 2013 data from Zambia.\n\n\n\n\nWhen spatial harmonization is necessary\nFigure 2 displays the part of Zambia where two units split up to form three units. A new political region, Muchinga, was formed from parts of the Northern and Eastern regions. Although the names of the Northern and Eastern regions remained the same, their spatial footprint changed from 2007 to 2013.\nIf we focus on our metric of households with access to improved drinking water, it initially appears that the percentage in the Eastern region rose dramatically from 16% to 72%, while the change in the neighboring Northern region was from 19% to 30%.\nGiven changes in boundaries, the apparent increase in clean water access in the Eastern region is misleading. It is quite possible that rural areas from the former Eastern region in 2007 formed much of Muchinga in 2013, with the latter having only 35% of households with access to improved drinking water. The actual change for the Eastern region may be a shift toward more urban composition as it shed rural territory, rather than a sanitation revolution.\nThe solution to the problem of changing boundaries is imposing integrated geography for this section of Zambia. In this case, the three regions of Northern, Muchinga, and Eastern are combined into one region, with results mapped in Figure 3. Here, we see more modest improvement in this aggregated region, with the percentage of households with access to an improved water source changing from 18% to 52% between 2007 and 2013.\n\n\n\n\nFigure 2: Percent of households that use improved sources of drinking water. The maps highlight changes in sample-specific boundaries. Map A shows Zambia in 2007 and Map B shows Zambia in 2013. Muchinga was formed from parts of Northern and Eastern.\n\n\n\n\n\nFigure 3: Percent of households that use improved sources of drinking water. The maps highlight spatially harmonized boundaries. Map A shows Zambia in 2007 and Map B shows Zambia in 2013. In this map, Eastern, Northern, and Muchinga were combined to form a spatially consistent boundary.\n\n\n\nAs Figure 4 shows, between 2010 and 2015, boundaries in Tanzania were redrawn such that three units (Kagera, Mwanza, and Shinyanga) were reorganized to form five units (Kagera, Mwanza, Shinyanga, Simiyu, and Geita). While it is mechanically possible to map our metric for these various regions in the two years, boundary changes preclude our drawing meaningful conclusions about progress within sub-national units. The Tanzanian case illustrates a general principle: holding space constant is critical in analyzing change at sub-national levels, because units that have changed boundaries cannot be compared across time in a meaningful way.\nCreating integrated geography for this part of Tanzania required aggregating smaller units into a single larger unit, as show in Figure 5. Focusing on the single combined region, we see that the share of households with an improved source of drinking water increased from only 40% to 56% between 2010 and 2015.\n\n\n\n\nFigure 4: Percent of households that use improved sources of drinking water. The maps highlight changes in sample-specific boundaries. Map A shows Tanzania in 2010 with three regions (Kagera, Mwanza, Shinyanga). Map B shows Tanzania in 2015 with 5 regions (Kagera, Mwanza, Shinyanga, Simiyu, and Geita) in the same spatial area.\n\n\n\n\n\nFigure 5: Percent of households that use improved sources of drinking water. The maps highlight spatially harmonized boundaries. Map A shows Tanzania in 2010, and Map B shows Tanzania in 2015. The 5 units Kagera, Mwanza, Shinyanga, Simiyu, and Geita were combined to form a single spatially consistent unit.\n\n\n\n\n\nGains and losses from using integrated geography\nFor spatial harmonization, we hold boundaries constant over time. In our prior examples, sub-national boundaries were merged to override any boundary changes that occurred between DHS samples. While that approach enables an apples‐to‐apples temporal comparison of places, detail that might be useful for the analysis gets lost.\nBoundary changes in Malawi illustrate this point. Consider Figure 6 and Figure 7. Map A from 2010 shows 74% households using an improved source of drinking water in Nkhata Bay, with the number inching up to 76% in 2016. The blue spatially harmonized maps in Figure 7 also show a lack of change. However, Figure 7 hides the advantaged sanitary position of Mzuzu City, which split from Nkhata Bay, with 99% of households in that city having improved drinking water in 2016. Similar results mark other urban areas, such as Lilongwe city, Zomba city, and Blantyre city (Zomba and Blantyre not displayed on the map). Taking full advantage of sample-specific geographic detail demonstrates that, in this instance, apparent regional progress in safe water access was largely localized in urban areas. As this case illustrates, sample-specific geography often provides greater detail and should be used in conjunction with spatially harmonized geography.\n\n\n\n\nFigure 6: Percent of households that use improved sources of drinking water. The maps highlight changes in sample-specific boundaries from 2010 to 2016. Map A shows Malawi in 2010 and Map B shows Malawi in 2016.\n\n\n\n\n\nFigure 7: Percent of households that use improved sources of drinking water. The maps highlight spatially harmonized boundaries. Map A shows Malawi in 2010 and Map B shows Malawi in 2016. In this map the sub-regions Nkhatabay and Likoma, Lilongwe and Lilongwe city, and Mzimba and Mzuzu city are combined.\n\n\n\n\n\nWhen geographic boundaries don’t change\nWhile Zambia, Tanzania, and Malawi all had regional boundary changes between DHS samples, sometimes regional boundaries stay the same across two or more DHS surveys. This was the case for Zimbabwe between 2010 and 2015, as shown in Figure 8, which displays all the countries used in this demonstration. For instances like Zimbabwe, IPUMS DHS still offers multi-year “integrated geography” variables (without modifying boundaries), for the convenience of researchers and to ensure that geographic footprints as well as regional labels stay the same across time. In other cases, only a single sample is available for a country in the DHS, for all years or within a limited time span. This is exemplified by Mozambique, with a single sample from 2011 (shown in Map A but not included in Map B).\n\n\n\n\nFigure 8: Percent of households that use improved sources of drinking water. Note that both the maps show spatially harmonized boundaries.Map A shows 2010 data from Malawi, Tanzania, and Zimbabwe, 2011 data from Mozambique, and 2007 data from Zambia.Map B shows 2015 data from Tanzania and Zimbabwe, 2016 data from Malawi, and 2013 data from Zambia."
  },
  {
    "objectID": "posts/2024-04-09-spatial-harmonization/index.html#getting-help",
    "href": "posts/2024-04-09-spatial-harmonization/index.html#getting-help",
    "title": "Demystifying Spatially Harmonized Geography in IPUMS DHS",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html",
    "href": "posts/2024-02-02-download-dhs-data/index.html",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "",
    "text": "The Demographic and Health Surveys Program (DHS) is the leading source of population health data for low- and middle-income countries around the world. IPUMS DHS disseminates a harmonized version of the DHS survey results in which variables are integrated across time and space, facilitating comparative and longitudinal analysis. Furthermore, IPUMS DHS provides a web interface and streamlined documentation to make the data discovery and download process easier."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#browse-data",
    "href": "posts/2024-02-02-download-dhs-data/index.html#browse-data",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Browse data",
    "text": "Browse data\nUsers can browse the available data using the IPUMS DHS data selection interface, which includes sample and variable availability, descriptions, codes, and more.\nFor more information about how to use the interface, see the IPUMS DHS user guide."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#select-data",
    "href": "posts/2024-02-02-download-dhs-data/index.html#select-data",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Select data",
    "text": "Select data\nOnce you’ve selected the samples and variables you want to include in your data extract, click View Cart to review your selections. If you’re satisfied with the contents of your extract, click Create Data Extract.\n\n\n\n\n\n\n\n\nIPUMS DHS allows you to select one of several output file formats. On this blog, we will use the default fixed-width (.dat) file option, which is the format expected by the data-reading functions provided in ipumsr.\n\n\n\n\n\n\n\n\nClick Submit Extract to submit your extract for processing on the IPUMS servers. You’ll receive an email when your extract is complete and ready to download."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#download-data",
    "href": "posts/2024-02-02-download-dhs-data/index.html#download-data",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Download data",
    "text": "Download data\nClick the green download button to download the compressed data file for your extract.\nYou will also need to download an associated metadata (DDI) file. This is an XML file that contains parsing instructions for the fixed-width data file as well as descriptive information about the variables contained in an extract.\n\n\nDDI stands for Data Documentation Initiative, an international standard for documenting data obtained in survey research.\nYou can do so by right clicking the DDI link and selecting Save link as….\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe specific text included in the dropdown may differ based on the browser that you are using. For instance, Safari displays the option Download Linked File As….\nThe important thing is that you download the DDI file in .xml format, not .html format."
  },
  {
    "objectID": "posts/2024-02-02-download-dhs-data/index.html#getting-help",
    "href": "posts/2024-02-02-download-dhs-data/index.html#getting-help",
    "title": "Obtaining Data from IPUMS DHS",
    "section": "Getting Help",
    "text": "Getting Help\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html",
    "href": "posts/2024-02-03-climate-theory/index.html",
    "title": "Developing Robust Conceptual Models in Climate Change and Health Research",
    "section": "",
    "text": "Unpacking the relationship between climate change and health requires not only technical skills, but also a strong theoretical foundation. Climate change, health, and their relationships are all complex. Researchers begin by conceptualizing—for their specific questions—1) the nature of the climate event, 2) the details of the health outcome, and 3) the temporal and spatial relationships between the climate and health phenomena of interest.\nBeyond these three elements, contextual factors are crucial for identifying the complex causal pathways among climate change and health. Figure 1 illustrates how the pathway from a climate event, its manifestation in the environment, and its impact on various health outcomes is embedded within larger contexts. As suggested by the gray box on the right, individuals will be impacted by climate change in unique ways based on their demographic characteristics and social determinants that affect their lived experiences. The gray box on the left illustrates how environmental and institutional contexts will also influence relationships between climatic events and health. Contexts influence the impact of exposures on individuals as well as their mitigation and adaptation strategies in response to climate risks."
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html#building-a-conceptual-model",
    "href": "posts/2024-02-03-climate-theory/index.html#building-a-conceptual-model",
    "title": "Developing Robust Conceptual Models in Climate Change and Health Research",
    "section": "Building a conceptual model",
    "text": "Building a conceptual model\nBelow, we provide the steps in creating a tailored conceptual model for climate change and health research.\n\n1. Assessing the temporal and spatial nature of climate risk\nRisks arising from climate change vary in their temporal and spatial scales. In terms of timing, risks may be acute or protracted. Acute risks arise when one-time exposure to a climate event is sufficient to trigger health outcomes, for example, when flooding makes water unsafe for drinking. Protracted risks, in contrast, arise from sustained exposure to climate phenomenon, such as the inhalation of dust over the course of enduring dry periods. The impact of climate change is also spatially variable. While some climate events affect wide areas, others’ scope is limited to discrete locations. Determining the type of risk depends on both the nature of the climate event and the health outcome studied. Identifying the temporal and spatial dimensions of risk provide the basis for decisions concerning temporal aggregation of climate data, as well as the length of expected lags between initial exposure to a climate event and the manifestation of the health outcome.\n\n\n2. Incorporating social and behavioral context\nCertain climate events are known to have deleterious effects on humans. For example, exposure to extremely high temperatures during the first trimester of pregnancy increases the risk of low birth-weight babies.2 Basic biological truths such as this are insufficient, however, to fully assess the impact of climate events on individual health because individuals have dramatically different access to resources, opportunities, and constraints. A pregnant person who lives in an air-conditioned house will experience extreme temperatures very differently than a person who is houseless. Demographic characteristics, such as age, gender, wealth, and partnership status matter when assessing climate impacts.\nFurthermore, the characteristics of societies (e.g., rural versus urban, gender roles, livelihoods) matter. Consider rising temperatures in nomadic versus other types of communities.3 For both, temperature change may make it harder to access water. In nomadic communities, the result could be men driving herds further from the community and thus staying away longer, while in other communities, the result could be that children tasked with collecting water find the task more physically demanding. Who and how individuals are affected by water shortages will vary depending on community norms and roles. Community characteristics will also affect the instigation and nature of mitigation and adaptation efforts. While analyses that map the nature of climate change over time provide a useful starting point, they cannot fully capture the individual- and community-specific impacts of climate change.3\n\n\n3. Understanding environmental and institutional contexts\nEnvironmental and institutional contexts are also influential in shaping causal pathways between climate change and health. The physical environment, such as altitude and the demarcation of seasons, can provide some level of protection from some climatic events. For instance, a small reduction in annual rainfall can have a devastating effect in an arid location where there is a close correspondence between a rainy season and agricultural production.4 The same reduction in rainfall might have only a minor impact in a location where annual precipitation is diffused throughout the year and is less closely tied to the agricultural season. Understanding such differences is important for selecting accurate measurements. In the former location, the rain shortfall might best be measured through the length of the rainy season. In the latter location, considering rainfall deviation from a monthly or annual average could be more appropriate.4\nInstitutional contexts include the built environment, such as the presence of roads, wells, and irrigation systems, as well as organizations designed to facilitate human capabilities, such as political, healthcare, and education systems. Local governments’ capacities to intervene to protect populations faced with climate risks can reduce negative health outcomes. Healthcare systems affect the accessibility of treatment (both physically and financially), while education systems provide individuals with the knowledge of when and how to respond to climatic events. Climate change and health research that fails to capture these contextual effects can lead to faulty conclusions."
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html#conclusion",
    "href": "posts/2024-02-03-climate-theory/index.html#conclusion",
    "title": "Developing Robust Conceptual Models in Climate Change and Health Research",
    "section": "Conclusion",
    "text": "Conclusion\nTheorizing the core relationship among specific climate events and specific health outcomes is the first step in developing a conceptual model but is insufficient to fully capture complex causal pathways. Individual characteristics, social determinants of health, and environmental and institutional environments are also critical. Informed and clearly identified spatial and temporal measurement of climate exposures is necessary, and these exposures should be thoughtfully, appropriately, and explicitly linked to the particular health outcomes of interest. Community-focused expertise and stakeholder engagement is vital to fully understand and incorporate how broader contexts interact with local circumstances to uniquely influence how climate change is experienced.\n\nFor more information\nTo learn more about the technical modeling implications of climate-change-and-health conceptual models, see Dorélien and Grace (2023).4\nTo see a specific model of the impact of climate change on women’s reproductive health in Africa, see Grace (2017).3\nFor further delineation of the elements in climate-change-and-health models (in the U.S. context), see Balbus et al. (2016).5"
  },
  {
    "objectID": "posts/2024-02-03-climate-theory/index.html#getting-help",
    "href": "posts/2024-02-03-climate-theory/index.html#getting-help",
    "title": "Developing Robust Conceptual Models in Climate Change and Health Research",
    "section": "Getting Help",
    "text": "Getting Help\n\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html",
    "href": "posts/2024-05-31-ts-join/index.html",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "",
    "text": "In our last technical post, we demonstrated how to calculate several different monthly temperature metrics with the ultimate goal of linking these values to individual child records in the 2012 Mali DHS survey.\nHowever, in contrast to the technique we used in our CHIRPS post, we can’t simply join our temperature data onto our DHS sample by enumeration cluster alone because each child in our sample has a different birth date, and therefore different temperature exposure prior to birth.\nTo account for this, we need to join our temperature data to each child survey record both by enumeration cluster (which contains the spatial information) as well as the child’s birth date (which contains the temporal information).\nFirst, we’ll identify the monthly CHIRTS data corresponding to each of our enumeration cluster areas, producing a time series of temperature values for each cluster. Next, we’ll identify the time series of temperature values for the 9 months prior to each child’s birth and use it to calculate trimester-specific temperature exposure values for each child. Finally, we’ll use the joined data to build a simple model predicting birth weight outcomes from temperature exposure.\nThis post will build on our previous post where we built several monthly temperature metrics. If you haven’t had the chance to read through that post yet, we encourage you to start there.\nBefore getting started, we’ll load the necessary packages for this post:\nlibrary(ipumsr)\nlibrary(sf)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(terra)"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#dhs-survey-data",
    "href": "posts/2024-05-31-ts-join/index.html#dhs-survey-data",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "DHS survey data",
    "text": "DHS survey data\nWe will obtain 2012 Mali data from IPUMS DHS. In this extract, we’ve selected the following variables:\n\n\nHEIGHTFEM: Height of woman in centimeters\n\nKIDSEX: Sex of child\n\nKIDDOBCMC: Child’s date of birth (CMC)\n\nBIRTHWT: Birth weight in kilos\n\nBIRTHWTREF: Source of weight at birth (health card or recall)\n\nCheck out our walkthrough for downloading IPUMS DHS data if you need help producing a similar extract.\nWe’ll load our data using ipumsr:\n\nml_dhs &lt;- read_ipums_micro(\"data/idhs_00021.xml\")\n#&gt; Use of data from IPUMS DHS is subject to conditions including that users should cite the data appropriately. Use command `ipums_conditions()` for more details.\n\nml_dhs\n#&gt; # A tibble: 10,326 × 48\n#&gt;    SAMPLE            SAMPLESTR     COUNTRY    YEAR IDHSPID IDHSHID DHSID IDHSPSU\n#&gt;    &lt;int+lbl&gt;         &lt;chr+lbl&gt;     &lt;int+lbl&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;\n#&gt;  1 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  2 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  3 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  4 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  5 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  6 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  7 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  8 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt;  9 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt; 10 46605 [Mali 2012] 46605 [Mali … 466 [Mal…  2012 46605 … 46605 … ML20… 4.66e10\n#&gt; # ℹ 10,316 more rows\n#&gt; # ℹ 40 more variables: IDHSSTRATA &lt;dbl&gt;, CASEID &lt;chr&gt;, HHID &lt;chr&gt;, PSU &lt;dbl&gt;,\n#&gt; #   STRATA &lt;dbl&gt;, DOMAIN &lt;dbl&gt;, HHNUM &lt;dbl&gt;, CLUSTERNO &lt;dbl&gt;, LINENO &lt;int&gt;,\n#&gt; #   BIDX &lt;int&gt;, PERWEIGHT &lt;dbl&gt;, KIDWT &lt;dbl&gt;, AWFACTT &lt;dbl&gt;, AWFACTU &lt;dbl&gt;,\n#&gt; #   AWFACTR &lt;dbl&gt;, AWFACTE &lt;dbl&gt;, AWFACTW &lt;dbl&gt;, DVWEIGHT &lt;dbl&gt;,\n#&gt; #   URBAN &lt;int+lbl&gt;, GEO_ML1987_2018 &lt;int+lbl&gt;, GEO_ML1995_2018 &lt;int+lbl&gt;,\n#&gt; #   GEO_ML2012 &lt;int+lbl&gt;, AGE &lt;int&gt;, AGE5YEAR &lt;int+lbl&gt;, RESIDENT &lt;int+lbl&gt;, …"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#dhs-boundaries",
    "href": "posts/2024-05-31-ts-join/index.html#dhs-boundaries",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "DHS boundaries",
    "text": "DHS boundaries\nWe’ll use the Mali borders that we prepared in our previous CHIRTS post. We describe the process there, but we’ve reproduced the code in the following collapsed block if you need to refresh your memory. As a reminder, you can obtain IPUMS integrated geography files from this table.\n\nBorder preparationml_borders &lt;- read_ipums_sf(\"data/gps/geo_ml1995_2018.zip\")\n\n# Validate internal borders\nml_borders_neat &lt;- st_make_valid(ml_borders)\n\n# Collapse internal borders to get single country border\nml_borders_out &lt;- ml_borders_neat |&gt; \n  st_union() |&gt; \n  st_simplify(dTolerance = 1000) |&gt; \n  st_as_sf()\n\n# Transform to UTM 29N coordinates, buffer, and convert back to WGS84\nml_borders_buffer &lt;- ml_borders_out |&gt; \n  st_transform(crs = 32629) |&gt; \n  st_buffer(dist = 10000) |&gt;\n  st_transform(crs = 4326)"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#dhs-enumeration-clusters",
    "href": "posts/2024-05-31-ts-join/index.html#dhs-enumeration-clusters",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "DHS enumeration clusters",
    "text": "DHS enumeration clusters\nWe can download the enumeration cluster coordinates from the DHS Program. We first described this process in our CHIRPS post. Simply follow the same instructions, substituting Mali where we previously used Burkina Faso.\nYou should obtain a file called MLGE6BFL.zip, which can be loaded with read_ipums_sf():\n\nml_clust &lt;- read_ipums_sf(\"data/MLGE6BFL.zip\")\n\nAs we’ve done in the past, we’ll buffer our cluster coordinates so we capture the climate effects of the general region around each survey location\n\nml_clust_buffer &lt;- ml_clust |&gt; \n  st_transform(crs = 32629) |&gt; \n  st_buffer(dist = 10000) |&gt;\n  st_transform(crs = 4326)"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#chirts",
    "href": "posts/2024-05-31-ts-join/index.html#chirts",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "CHIRTS",
    "text": "CHIRTS\nFor this post, we’ll use the monthly heatwave proportions that we calculated at the end of our previous CHIRTS post.\nHowever for the purposes of demonstration, in that post we used a single year of CHIRTS data. Now that we’re planning to attach temperature data to our DHS survey data, we need to consider the full range of birth dates represented in our sample.\nExtending CHIRTS time series\nThe DHS contains records for children born within 5 years of the survey date to women between 15 and 49 years of age. Thus, for the 2012 Mali sample, we will have records for children ranging from 2008-2012. Further, since we want to identify monthly climate records for the time preceding each birth, we will also need to have data for the year prior to the earliest birth in the sample.\nThat means that for our 2012 Mali sample, we’ll actually need CHIRTS data for 2007-2012.\nFortunately, the pipeline we set up in our previous post can be easily scaled to accommodate additional years of data. Recall that we built a dedicated function to calculate monthly heatwave proportions:\n\nprop_heatwave &lt;- function(temps, thresh, n_seq) {\n  # Convert to RLE of days above threshold\n  bin_rle &lt;- rle(temps &gt;= thresh)\n  \n  # Identify heatwave events based on sequence length\n  is_heatwave &lt;- bin_rle$values & (bin_rle$lengths &gt;= n_seq)\n  \n  # Count heatwave days and divide by total number of days\n  sum(bin_rle$lengths[is_heatwave]) / length(temps)\n}\n\nAll we need to do, then, is apply this function to input CHIRTS data for 2007 to 2012, rather than just for 2012. To do so, we’ve downloaded all 6 years of data manually as described previously and placed each file in a data/chirts directory.\nWe can load them by first obtaining the file path to each file with list.files(). Then, we use the familiar rast() to load all 6 files into a single SpatRaster.\n\nchirts_files &lt;- list.files(\"data/chirts\", full.names = TRUE)\n\nml_chirts &lt;- rast(chirts_files)\n\nAs we did last time, we’ll crop the CHIRTS raster to the region directly around Mali:\n\nml_chirts &lt;- crop(ml_chirts, ml_borders_buffer, snap = \"out\")\n\nNow we can use the tapp() function introduced in our previous post to calculate monthly heatwave proportions for each month/year combination. The only difference here is that we use index = \"yearmonths\" instead of index = \"months\" since we have multiple years of data. This ensures that we don’t aggregate months together across years.\n\n# Calculate monthly proportion of heatwave days\nml_chirts_heatwave &lt;- tapp(\n  ml_chirts,\n  function(x) prop_heatwave(x, thresh = 35, n_seq = 3),\n  index = \"yearmonths\" # New index to account for multiple years\n)\n\nAs expected, we now have a SpatRaster with 72 layers: one for each month/year combination from 2007-2012. Each layer contains the proportion of days that met our heatwave threshold of 3+ consecutive days exceeding 35°C.\n\nml_chirts_heatwave\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 301, 335, 72  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -12.35, 4.399999, 10.05, 25.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 \n#&gt; source      : ml_chirts_heatwave.nc \n#&gt; names       : ml_ch~ave_1, ml_ch~ave_2, ml_ch~ave_3, ml_ch~ave_4, ml_ch~ave_5, ml_ch~ave_6, ... \n#&gt; time (ymnts): 2007-Jan to 2012-Dec\n\n\n\n\n\n\n\nCaution\n\n\n\nThis post was built using terra 1.7-78. Older versions of terra contain a bug that may produce incorrect time units when index = \"yearmonths\".\nIf you’re running an older version of terra, we suggest updating the package with install.packages(\"terra\"). Otherwise, you may be able to temporarily avoid the issue by manually setting the correct time values:\n\n# Manual update of offset date values\nyear_months &lt;- seq(\n  lubridate::ym(\"2007-01\"), \n  lubridate::ym(\"2012-12\"), \n  by = \"months\"\n)\n\ntime(ml_chirts_heatwave) &lt;- year_months"
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#reconcile-date-representations",
    "href": "posts/2024-05-31-ts-join/index.html#reconcile-date-representations",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Reconcile date representations",
    "text": "Reconcile date representations\nTo deal with the temporal component of the join, we’ll need to identify the relevant months of data for each child in our DHS sample. Each child’s birth date recorded in the KIDDOBCMC variable:\n\nml_dhs$KIDDOBCMC\n#&gt;     [1] 1311 1301 1316 1356 1305 1337 1301 1318 1337 1313 1354 1313 1326 1312\n#&gt;    [15] 1330 1316 1354 1311 1335 1338 1305 1321 1305 1347 1315 1354 1309 1323\n#&gt;    [29] 1324 1349 1315 1297 1334 1317 1336 1334 1338 1304 1323 1307 1354 1311\n....\n\nYou might have expected that these would be dates, but we actually have a series of 4-digit numbers. This is because dates are encoded as century month codes in the DHS. A century month code (CMC) encodes time as the number of months that have elapsed since 1900. CMCs are useful for calculating intervals of time, because they can be easily added and subtracted.\nHowever, the time encoded in our CHIRTS heatwave data aren’t encoded in CMC format, so we’ll need to reconcile these two time representations.\nWorking with CMCs\nSince converting between CMCs and traditional dates is a common and well-defined task, we’ll build some helper functions to handle the conversion. That way, we can easily convert back and forth without having to remember the CMC conversion formula each time.\nFortunately, the description of the KIDDOBCMC variable describes the arithmetic required to convert. We’ve translated that text into the following function, which takes an input CMC and converts it to year-month format:\n\n\nYou can view the description for any variable in your extract using the ipums_var_desc() function from ipumsr.\n\ncmc_to_ym &lt;- function(cmc) {\n  year &lt;- floor((cmc - 1) / 12) + 1900\n  month &lt;- cmc - ((year - 1900) * 12)\n  \n  year_month &lt;- paste(year, month, sep = \"-\")\n  \n  lubridate::ym(year_month)\n}\n\nSimilarly, we’ll create a function that goes in the reverse direction:\n\nym_to_cmc &lt;- function(date) {\n  year &lt;- lubridate::year(date)\n  month &lt;- lubridate::month(date)\n  \n  (year - 1900) * 12 + month\n}\n\nFor instance, a CMC of 1307 turns out to be the same as November, 2008:\n\ncmc_to_ym(1307)\n#&gt; [1] \"2008-11-01\"\n\nAnd vice versa:\n\nym_to_cmc(\"2008-11-01\")\n#&gt; [1] 1307\n\nNow we have an easy way to reconcile the temporal information in our two data sources."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#attaching-heatwaves-to-dhs-clusters",
    "href": "posts/2024-05-31-ts-join/index.html#attaching-heatwaves-to-dhs-clusters",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Attaching heatwaves to DHS clusters",
    "text": "Attaching heatwaves to DHS clusters\nTo extract the average heatwave proportions for each DHS enumeration cluster, we’ll use terra’s extract(), which we’ve introduced previously.\n\n# Extract mean heatwave proportions for each DHS cluster region\nchirts_clust &lt;- extract(\n  ml_chirts_heatwave, \n  ml_clust_buffer, \n  fun = mean,\n  weights = TRUE\n) |&gt; \n  as_tibble()\n\nchirts_clust\n#&gt; # A tibble: 413 × 73\n#&gt;       ID ml_chirts_heatwave_1 ml_chirts_heatwave_2 ml_chirts_heatwave_3\n#&gt;    &lt;int&gt;                &lt;dbl&gt;                &lt;dbl&gt;                &lt;dbl&gt;\n#&gt;  1     1                0.210                0.498                0.806\n#&gt;  2     2                0.226                0.714                0.903\n#&gt;  3     3                0.270                0.910                1    \n#&gt;  4     4                0.226                0.857                0.968\n#&gt;  5     5                0.258                0.898                0.996\n#&gt;  6     6                0.226                0.841                0.935\n#&gt;  7     7                0.169                0.679                0.903\n#&gt;  8     8                0.161                0.5                  0.892\n#&gt;  9     9                0.226                0.858                0.973\n#&gt; 10    10                0.194                0.627                0.924\n#&gt; # ℹ 403 more rows\n#&gt; # ℹ 69 more variables: ml_chirts_heatwave_4 &lt;dbl&gt;, ml_chirts_heatwave_5 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_6 &lt;dbl&gt;, ml_chirts_heatwave_7 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_8 &lt;dbl&gt;, ml_chirts_heatwave_9 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_10 &lt;dbl&gt;, ml_chirts_heatwave_11 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_12 &lt;dbl&gt;, ml_chirts_heatwave_13 &lt;dbl&gt;,\n#&gt; #   ml_chirts_heatwave_14 &lt;dbl&gt;, ml_chirts_heatwave_15 &lt;dbl&gt;, …\n\nWide vs. long format\nextract() provides data in wide format, where each column represents our temperature values (in this case, mean proportion of heatwave days in a given month) and each row represents a DHS enumeration cluster.\nTo join on our DHS survey data, we’ll want our data to be in long format, where each row represents a single month of temperature data for a single DHS cluster. We can accomplish this conversion with the pivot_longer() function from the tidyr package.\n\n\n\n   © RStudio, Inc. (MIT) \nFirst, we’ll rename the columns in our extracted data. Currently, they’re listed in incremental order, which isn’t very intuitive. Instead, we’ll rename them using the CMC code of the month that each column represents. We can use our helper function to convert these dates to CMC format.\nWe’ll also update the \"ID\" column to be named \"DHSID\" for consistency with the name used in the DHS survey to represent the enumeration cluster ID:\n\n# Sequence of months for the time range in our data\nyear_months &lt;- seq(\n  lubridate::ym(\"2007-01\"), \n  lubridate::ym(\"2012-12\"), \n  by = \"months\"\n)\n\n# Change layer names\nnames(chirts_clust) &lt;- c(\"DHSID\", ym_to_cmc(year_months))\n\nNext, we need to convert the incremental ID numbers for each cluster to their corresponding DHSID code. The ID values in chirts_clust represent the clusters extracted from ml_clust in index order. Thus, we simply need to reassign the incremental ID codes in chirts_clust with the IDs in ml_clust.\n\n# Convert index numbers to corresponding DHSID codes\nchirts_clust$DHSID &lt;- ml_clust$DHSID\n\nchirts_clust\n#&gt; # A tibble: 413 × 73\n#&gt;    DHSID  `1285` `1286` `1287` `1288` `1289` `1290`  `1291` `1292` `1293` `1294`\n#&gt;    &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1 ML201…  0.210  0.498  0.806  1      1      0.935 0.358        0 0.318   1    \n#&gt;  2 ML201…  0.226  0.714  0.903  1      0.999  0.825 0.346        0 0.0355  0.857\n#&gt;  3 ML201…  0.270  0.910  1      0.999  1      0.604 0            0 0       0.150\n#&gt;  4 ML201…  0.226  0.857  0.968  1      1      0.718 0.186        0 0       0.159\n#&gt;  5 ML201…  0.258  0.898  0.996  0.988  1      0.599 0            0 0       0.141\n#&gt;  6 ML201…  0.226  0.841  0.935  1      1      0.572 0            0 0       0.373\n#&gt;  7 ML201…  0.169  0.679  0.903  1      1      0.933 0.305        0 0.427   0.778\n#&gt;  8 ML201…  0.161  0.5    0.892  1      1      0.967 0.581        0 0.5     0.901\n#&gt;  9 ML201…  0.226  0.858  0.973  0.991  1      0.569 0.00296      0 0       0.144\n#&gt; 10 ML201…  0.194  0.627  0.924  1      1      1.00  0.572        0 0.385   0.957\n#&gt; # ℹ 403 more rows\n#&gt; # ℹ 62 more variables: `1295` &lt;dbl&gt;, `1296` &lt;dbl&gt;, `1297` &lt;dbl&gt;, `1298` &lt;dbl&gt;,\n#&gt; #   `1299` &lt;dbl&gt;, `1300` &lt;dbl&gt;, `1301` &lt;dbl&gt;, `1302` &lt;dbl&gt;, `1303` &lt;dbl&gt;,\n#&gt; #   `1304` &lt;dbl&gt;, `1305` &lt;dbl&gt;, `1306` &lt;dbl&gt;, `1307` &lt;dbl&gt;, `1308` &lt;dbl&gt;,\n#&gt; #   `1309` &lt;dbl&gt;, `1310` &lt;dbl&gt;, `1311` &lt;dbl&gt;, `1312` &lt;dbl&gt;, `1313` &lt;dbl&gt;,\n#&gt; #   `1314` &lt;dbl&gt;, `1315` &lt;dbl&gt;, `1316` &lt;dbl&gt;, `1317` &lt;dbl&gt;, `1318` &lt;dbl&gt;,\n#&gt; #   `1319` &lt;dbl&gt;, `1320` &lt;dbl&gt;, `1321` &lt;dbl&gt;, `1322` &lt;dbl&gt;, `1323` &lt;dbl&gt;, …\n\nNow we’re ready to convert to long format data. pivot_longer() will convert a set of columns in our wide data to two columns in our long data. The names of these columns will be stored in one of the output columns, and their associated values will be stored in the other.\nBelow, we indicate that we want to pivot all columns except the DHSID column (the data are already long on DHSID). We also indicate that we want the new column of names to be called \"CHIRTSCMC\" and the new column of values to be called \"PROPHEATWAVE\". Finally, we use the names_transform argument to convert all the names to numeric format, so they will be interpretable as CMCs.\n\n# Convert to long format for each cluster/month combination\nchirts_clust &lt;- pivot_longer(\n  chirts_clust,\n  cols = -DHSID,\n  names_to = \"CHIRTSCMC\",\n  values_to = \"PROPHEATWAVE\",\n  names_transform = as.numeric\n)\n\nchirts_clust\n#&gt; # A tibble: 29,736 × 3\n#&gt;    DHSID          CHIRTSCMC PROPHEATWAVE\n#&gt;    &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 ML201200000001      1285        0.210\n#&gt;  2 ML201200000001      1286        0.498\n#&gt;  3 ML201200000001      1287        0.806\n#&gt;  4 ML201200000001      1288        1    \n#&gt;  5 ML201200000001      1289        1    \n#&gt;  6 ML201200000001      1290        0.935\n#&gt;  7 ML201200000001      1291        0.358\n#&gt;  8 ML201200000001      1292        0    \n#&gt;  9 ML201200000001      1293        0.318\n#&gt; 10 ML201200000001      1294        1    \n#&gt; # ℹ 29,726 more rows\n\nAs you can see, each row now corresponds to a cluster/month combination, and the corresponding heatwave value is stored in the PROPHEATWAVE column."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#joining-on-dhs-survey-data",
    "href": "posts/2024-05-31-ts-join/index.html#joining-on-dhs-survey-data",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Joining on DHS survey data",
    "text": "Joining on DHS survey data\nBefore we join our heatwave data on our DHS survey data, we need to do some housekeeping. Some of the variables we want to include in our model include missing values or implied decimals, which we’ll want to clean up before we try to join the DHS survey with our CHIRTS data.\nDHS survey data preparation\nFirst, we’ll make a new column that stores a unique ID for each child in the sample by combining the ID for each woman and the index for each of her births. This will make it easier to ensure that we join data independently for each child in the sample:\n\n# str_squish() removes excess whitespace in the ID strings\nml_dhs &lt;- ml_dhs |&gt; \n  mutate(KIDID = stringr::str_squish(paste(IDHSPID, BIDX)))\n\nWe’ll also want to make sure to recode several of our variables that include missing values. We can check the missing value codes for a variable using ipums_val_labels(). For instance, for our key outcome variable, BIRTHWT, we see that all values over 9996 are missing in some way:\n\nipums_val_labels(ml_dhs$BIRTHWT)\n#&gt; # A tibble: 5 × 2\n#&gt;     val lbl                  \n#&gt;   &lt;int&gt; &lt;chr&gt;                \n#&gt; 1  9995 9995+                \n#&gt; 2  9996 Not weighed at birth \n#&gt; 3  9997 Don't know           \n#&gt; 4  9998 Missing              \n#&gt; 5  9999 NIU (not in universe)\n\nLooking at these values, you may be surprised to see 4-digit weights. To investigate further, we can display detailed variable information with ipums_var_desc():\n\nipums_var_desc(ml_dhs$BIRTHWT)\n#&gt; [1] \"For children born in the three to five years before the survey, BIRTHWT (M19) reports the child's birthweight in kilos with three implied decimal places (or, alternatively stated, in grams with no decimal places). Children who were not weighed are coded 9996.\"\n\nNote that the description mentions that there are 3 implied decimal places. So, we’ll recode BIRTHWT to be NA in the cases where the value is more than 9996, and we’ll divide by 1000 otherwise. We’ll do a similar process with a few other variables as well:\n\nml_dhs &lt;- ml_dhs |&gt; \n  mutate(\n    BIRTHWT = if_else(BIRTHWT &gt;= 9996, NA, BIRTHWT / 1000),\n    HEIGHTFEM = if_else(HEIGHTFEM &gt;= 9994, NA, HEIGHTFEM / 10),\n    EDUCLVL = if_else(EDUCLVL == 8, NA, EDUCLVL),\n    BIRTHWTREF = if_else(BIRTHWTREF &gt;= 7, NA, BIRTHWTREF)\n  )\n\n\n\n\n\n\n\nCensoring\n\n\n\nIn practice, we would likely want to deal with BIRTHWT values of 9995+, which represent all weights above the 9.995 kg threshold. It’s not fully correct to treat these values as equal to 9.995, since their values are actually unknown. This problem is known as censoring, but we won’t address it in this post to keep this demonstration focused on our core goal of integrating climate data with DHS surveys.\n\n\nResidency\nWhen working with climate data, we want to be attentive to the members of the sample who may have recently moved to a location, as recent arrivals likely didn’t experience the previous climate effects recorded for the area they currently live!\nIPUMS DHS includes the RESIDEINTYR variable to indicate how long survey respondents have lived at their current location.\nUnfortunately, this variable wasn’t collected for the Mali 2012 sample. However, we do have a record of whether a respondent is a visitor to the location or not. At a minimum, we’ll remove records for those who are listed as visitors, since we can’t be confident that they experienced the climate data for the cluster their response was recorded in.\n\nml_dhs &lt;- ml_dhs |&gt; \n  filter(RESIDENT == 1)\n\nData types\nFinally, we want to recode our variables to the appropriate data type. In our case, we just need to convert all labeled variables to factors. BIDX is unlabeled, but represents a factor, so we’ll explicitly convert that variable as well:\n\nml_dhs &lt;- ml_dhs |&gt; \n  mutate(BIDX = as_factor(BIDX)) |&gt; \n  as_factor() # Convert all remaining labeled columns to factors\n\nAttaching data sources\nAt its simplest, we could join our prepared DHS survey data with our extracted heatwave data by matching records based on their cluster and birth month. We can use left_join() from dplyr to do so. This will retain all records in our DHS survey sample and attach temperature values where the birth date (KIDDOBCMC) is equal to the month of temperature data in chirts_clust:\n\n# Join temperature exposure for each child during the month of their birth\nleft_join(\n  ml_dhs,\n  chirts_clust,\n  by = c(\"DHSID\", \"KIDDOBCMC\" = \"CHIRTSCMC\")\n)\n\nHowever, this only attaches a single month of temperature data for each child. What we’d rather have is a time series of values covering the time between conception and birth.\nTo accomplish this, we can define a new variable, KIDCONCEPTCMC, which contains the CMC of the month 9 months before the birth date.\n\n# Make unique ID for each kid using woman's ID and Birth index\nml_dhs &lt;- ml_dhs |&gt;  \n  mutate(KIDCONCEPTCMC = KIDDOBCMC - 9)\n\nNow, we want to join all 9 of the relevant monthly temperature records to each child’s DHS survey record. That is, we want to match records that have the same DHSID, a birthdate that is after the CHIRTS data that will be joined, and a conception date that is before the CHIRTS data that will be joined. We can use join_by() from dplyr to specify these more complex criteria:\n\n# Join 9 months of CHIRTS data to each child record\nml_dhs_chirts &lt;- left_join(\n  ml_dhs,\n  chirts_clust,\n  by = join_by(\n    DHSID == DHSID, # Cluster ID needs to match\n    KIDDOBCMC &gt; CHIRTSCMC, # DOB needs to be after all joined temp. data\n    KIDCONCEPTCMC &lt;= CHIRTSCMC # Conception date needs to be before all joined temp. data\n  )\n)\n\nWe should end up with a dataset that contains 9 records for each child: one for each month between their conception and birth. We can confirm by counting the records for each KIDID:\n\n# Each kid should have 9 rows now\nml_dhs_chirts |&gt; \n  count(KIDID)\n#&gt; # A tibble: 10,300 × 2\n#&gt;    KIDID               n\n#&gt;    &lt;chr&gt;           &lt;int&gt;\n#&gt;  1 46605 1 14 2 1      9\n#&gt;  2 46605 1 16 2 1      9\n#&gt;  3 46605 1 16 5 1      9\n#&gt;  4 46605 1 16 5 2      9\n#&gt;  5 46605 1 4 2 1       9\n#&gt;  6 46605 1 42 2 1      9\n#&gt;  7 46605 1 42 2 2      9\n#&gt;  8 46605 1 50 2 1      9\n#&gt;  9 46605 1 75 12 1     9\n#&gt; 10 46605 1 75 12 2     9\n#&gt; # ℹ 10,290 more rows\n\nWe can pull out a few columns for an individual child to make it more clear how things are being joined:\n\n# Example of what our data look like for one kid:\nml_dhs_chirts |&gt; \n  filter(KIDID == \"46605 1 4 2 1\") |&gt; \n  select(KIDID, DHSID, KIDDOBCMC, CHIRTSCMC, PROPHEATWAVE)\n#&gt; # A tibble: 9 × 5\n#&gt;   KIDID         DHSID          KIDDOBCMC CHIRTSCMC PROPHEATWAVE\n#&gt;   &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 46605 1 4 2 1 ML201200000001      1311      1302       0.822 \n#&gt; 2 46605 1 4 2 1 ML201200000001      1311      1303       0.103 \n#&gt; 3 46605 1 4 2 1 ML201200000001      1311      1304       0     \n#&gt; 4 46605 1 4 2 1 ML201200000001      1311      1305       0.244 \n#&gt; 5 46605 1 4 2 1 ML201200000001      1311      1306       0.787 \n#&gt; 6 46605 1 4 2 1 ML201200000001      1311      1307       0.624 \n#&gt; 7 46605 1 4 2 1 ML201200000001      1311      1308       0     \n#&gt; 8 46605 1 4 2 1 ML201200000001      1311      1309       0.0968\n#&gt; 9 46605 1 4 2 1 ML201200000001      1311      1310       0.741\n\nAs we can see, an individual child (KIDID) is associated with heatwave data (PROPHEATWAVE) for each of the 9 CHIRTS months (CHIRTSCMC) prior to their birth date (KIDDOBCMC).\nFor a different child we’ll get a similar output, but because this child has a different birth date, the specific PROPHEATWAVE values will be different, even if the cluster ID is the same:\n\nml_dhs_chirts |&gt; \n  filter(KIDID == \"46605 1255 2 2\") |&gt; \n  select(KIDID, DHSID, KIDDOBCMC, CHIRTSCMC, PROPHEATWAVE)\n#&gt; # A tibble: 9 × 5\n#&gt;   KIDID          DHSID          KIDDOBCMC CHIRTSCMC PROPHEATWAVE\n#&gt;   &lt;chr&gt;          &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 46605 1255 2 2 ML201200000001      1305      1296        0    \n#&gt; 2 46605 1255 2 2 ML201200000001      1305      1297        0    \n#&gt; 3 46605 1255 2 2 ML201200000001      1305      1298        0.345\n#&gt; 4 46605 1255 2 2 ML201200000001      1305      1299        1    \n#&gt; 5 46605 1255 2 2 ML201200000001      1305      1300        0.999\n#&gt; 6 46605 1255 2 2 ML201200000001      1305      1301        1    \n#&gt; 7 46605 1255 2 2 ML201200000001      1305      1302        0.822\n#&gt; 8 46605 1255 2 2 ML201200000001      1305      1303        0.103\n#&gt; 9 46605 1255 2 2 ML201200000001      1305      1304        0\n\nSimilarly, for a child from a different cluster, we’ll have different PROPHEATWAVE values regardless of which month is under consideration:\n\nml_dhs_chirts |&gt; \n  filter(KIDID == \"46605 518171 3 1\") |&gt; \n  select(KIDID, DHSID, KIDDOBCMC, CHIRTSCMC, PROPHEATWAVE)\n#&gt; # A tibble: 9 × 5\n#&gt;   KIDID            DHSID          KIDDOBCMC CHIRTSCMC PROPHEATWAVE\n#&gt;   &lt;chr&gt;            &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 46605 518171 3 1 ML201200000518      1314      1305       0.379 \n#&gt; 2 46605 518171 3 1 ML201200000518      1314      1306       0.877 \n#&gt; 3 46605 518171 3 1 ML201200000518      1314      1307       0.456 \n#&gt; 4 46605 518171 3 1 ML201200000518      1314      1308       0     \n#&gt; 5 46605 518171 3 1 ML201200000518      1314      1309       0.0213\n#&gt; 6 46605 518171 3 1 ML201200000518      1314      1310       0.505 \n#&gt; 7 46605 518171 3 1 ML201200000518      1314      1311       0.923 \n#&gt; 8 46605 518171 3 1 ML201200000518      1314      1312       1     \n#&gt; 9 46605 518171 3 1 ML201200000518      1314      1313       1\n\nNow that we have the correct 9-month time series for each child, we can further aggregate to get a trimester-specific exposure metric for each child.\nFirst, we’ll create a TRIMESTER variable, which encodes the trimester that each month belongs to for each child:\n\nml_dhs_chirts &lt;- ml_dhs_chirts |&gt; \n  arrange(DHSID, KIDID, CHIRTSCMC) |&gt; # order by CMC to ensure trimesters are in correct order\n  mutate(TRIMESTER = as_factor(rep(c(1, 2, 3), each = 3)), .by = KIDID)\n\nThen, we’ll average the heatwave proportions across trimesters for each child:\n\n# Average proportion of heatwave days within each child and trimester\nml_dhs_tri &lt;- ml_dhs_chirts |&gt; \n  summarize(\n    MEANPROPHEATWAVE = mean(PROPHEATWAVE), \n    .by = c(KIDID, TRIMESTER)\n  )\n\nml_dhs_tri\n#&gt; # A tibble: 30,900 × 3\n#&gt;    KIDID          TRIMESTER MEANPROPHEATWAVE\n#&gt;    &lt;chr&gt;          &lt;fct&gt;                &lt;dbl&gt;\n#&gt;  1 46605 1 14 2 1 1                    0.439\n#&gt;  2 46605 1 14 2 1 2                    0.218\n#&gt;  3 46605 1 14 2 1 3                    0.781\n#&gt;  4 46605 1 16 2 1 1                    0.240\n#&gt;  5 46605 1 16 2 1 2                    0.903\n#&gt;  6 46605 1 16 2 1 3                    0.731\n#&gt;  7 46605 1 16 5 1 1                    0.944\n#&gt;  8 46605 1 16 5 1 2                    0.322\n#&gt;  9 46605 1 16 5 1 3                    0.578\n#&gt; 10 46605 1 16 5 2 1                    0.115\n#&gt; # ℹ 30,890 more rows\n\nWe now have 3 records for each child—one for each trimester. Our temperature metric now represents the average monthly proportion of days spent in a heatwave across the three months of each trimester as well as across the spatial region of each buffered cluster region.\nRecall that in this case we define a heatwave day as any day belonging to a sequence of at least 3 days over 35°C."
  },
  {
    "objectID": "posts/2024-05-31-ts-join/index.html#getting-help",
    "href": "posts/2024-05-31-ts-join/index.html#getting-help",
    "title": "Time-specific temperature exposures for DHS survey respondents",
    "section": "Getting Help",
    "text": "Getting Help\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html",
    "href": "posts/2024-02-04-dhs-chirps/index.html",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "",
    "text": "We know that health outcomes are significantly impacted by individuals’ environmental context: excessive rainfall can flood local infrastructure1; warmer temperatures can expand the range of common disease vectors2; and drought can decimate crop-growing regions.3 However, survey data like those from The DHS Program often provide only a limited view of the environmental trends in which individuals are embedded.\nDeveloping a more holistic understanding of the role of environmental conditions in health outcomes therefore requires integrating external data sources with the survey data from DHS.\nIn this post we’ll demonstrate how to obtain raw precipitation data from the Climate Hazards Center InfraRed Precipitation with Station dataset (CHIRPS) and attach that data to survey responses from IPUMS DHS."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#ipums-dhs-survey-data",
    "href": "posts/2024-02-04-dhs-chirps/index.html#ipums-dhs-survey-data",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "IPUMS DHS survey data",
    "text": "IPUMS DHS survey data\nTo get started, we’ll download a data file (or extract, in IPUMS terms) from IPUMS DHS and load it into R. The extract in this post contains the HWHAZWHO variable (which contains the height-for-age Z-score) along with several pre-selected variables for the 2010 Burkina Faso sample. If you need a refresher on how to download IPUMS DHS data, see the Downloading IPUMS DHS Data post.\n\n\n\n   © IPUMS (MPL-2.0) \nWe’ve downloaded and stored our XML codebook and compressed data file in the data/dhs directory. Be sure to update this path based on your local file setup, so you can follow along.\nTo simplify our output, we’ll select only a subset of the variables included in the extract:\n\nlibrary(ipumsr)\nlibrary(dplyr)\n\n# Load IPUMS DHS extract\ndhs &lt;- read_ipums_micro(\n  ddi = \"data/dhs/idhs_00018.xml\",\n  data_file = \"data/dhs/idhs_00018.dat.gz\",\n  verbose = FALSE\n)\n\n# Select a subset of variables\ndhs &lt;- dhs |&gt; \n  select(SAMPLE, YEAR, IDHSPID, IDHSHID, DHSID, URBAN, HWHAZWHO)\n\ndhs\n#&gt; # A tibble: 15,044 × 7\n#&gt;    SAMPLE                     YEAR IDHSPID      IDHSHID DHSID URBAN   HWHAZWHO  \n#&gt;    &lt;int+lbl&gt;                 &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt; &lt;int+l&gt; &lt;int+lbl&gt; \n#&gt;  1 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -264      \n#&gt;  2 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -113      \n#&gt;  3 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur…  -13      \n#&gt;  4 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -291      \n#&gt;  5 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -211      \n#&gt;  6 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… 9999 [NIU…\n#&gt;  7 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… 9999 [NIU…\n#&gt;  8 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… -185      \n#&gt;  9 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur… 9999 [NIU…\n#&gt; 10 85404 [Burkina Faso 2010]  2010 85404      … 85404 … BF20… 2 [Rur…  -88      \n#&gt; # ℹ 15,034 more rows\n\nThis gives us a tabular data source containing 15,044 individual DHS survey responses for 7 variables. Of particular note is the DHSID variable (which stores the identifier for the location of the survey response)."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#dhs-cluster-coordinates",
    "href": "posts/2024-02-04-dhs-chirps/index.html#dhs-cluster-coordinates",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "DHS cluster coordinates",
    "text": "DHS cluster coordinates\nTo meaningfully attach environmental data to our DHS survey responses, we’ll need to know the location where each survey response was collected. Fortunately, the DHS Program provides GPS coordinates for each surveyed household grouping, or cluster.\n\n\n\n\n\n\nDHS Cluster Displacement\n\n\n\nIt’s important to note that the GPS coordinates provided by The DHS Program do not actually reflect the exact location of the clusters. In fact, the coordinates provided are randomly displaced from their true locations, such that:\n\nurban clusters are displaced up to 2 kilometers in any direction.\n99% of rural clusters are displaced up to 5 kilometers in any direction.\n1% of rural clusters are displaced up to 10 kilometers in any direction.\ndisplaced coordinates do not cross the country boundary and stay within the DHS survey region.\n\nThe current demonstration doesn’t require a precise location of each cluster, so we will ignore this detail. However, for more fine-grained analyses (e.g. road network analyses), you may need to take displacement into account.\nSee the DHS GPS data collection documentation for more details about the DHS cluster point displacement methodology.\n\n\nIPUMS DHS does not yet disseminate DHS cluster coordinates directly. For now, to obtain the GPS coordinates for a specific sample, you’ll have to log into your account from The DHS Program. Specify your country of interest, and, on the line for the appropriate sample year, click the link to download the GPS coordinate data under the heading GPS Datasets. (Again, for this example, we’re using Burkina Faso’s 2010 sample.)\n\n\nIPUMS DHS has recently received permission and funding to distribute DHS GPS data. Stay tuned, as these data will soon be available via IPUMS!\nYou’ll be presented with a new page containing a list of download links. Scroll down to the Geographic Datasets section. You have the option of downloading the file as either a shapefile (.shp) or a comma delimited file (.csv). For our purposes, we will download the shapefile, which contains spatial information in a format that can be easily interpreted by R (as well as by external GIS software).\nFor the Burkina Faso 2010 sample, the file should be named BFGE61FL. If you see a different file name, make sure you’re working with the correct survey year.\nVector data\nOur DHS cluster coordinates are what’s known as vector data. Vector data refer to spatial data that represent geographic features using geometric shapes like points, lines, and polygons.\n\n\nBasic features of the vector data model4\n\nIn R, the sf package provides an intuitive tabular framework for working with vector data. sf objects look much like a familiar tibble or data.frame, but also include an additional geometry column, which stores the spatial features that correspond to each set of observations.\n\n\n\n\n\n\n\n\n   © Edzer Pebesma (GPL-2 | MIT) \nInstalling sf\nsf requires three operating system dependencies:\n\n\nGEOS for geometrical operations on projected coordinates\n\nPRØJ for coordinate reference system conversion and transformation\n\nGDAL for driver options\n\nMake sure to follow these instructions to ensure that you set up GEOS, PRØJ, and GDAL when installing sf. The installation instructions may vary slightly, depending on your operating system. You may also need to update R, and then run install.packages(\"sf\").\nLoading cluster coordinates\nOnce sf is installed, we can use st_read() to load the file of GPS coordinate data we downloaded above. We’ve stored our shapefile in the data/gps directory. Again, make sure to adjust this path based on the location where you saved the file on your own system.\n\n\nA shapefile is actually a collection of several files; the primary file will have the .shp extension. Other files contain relevant metadata about the geometries contained in the .shp file (for instance, projection or index information).\n\nlibrary(sf)\n\n# Load the 2010 BF cluster coordinate shapefile\nbf_gps &lt;- st_read(\"data/gps/BFGE61FL/BFGE61FL.shp\")\n#&gt; Reading layer `BFGE61FL' from data source \n#&gt;   `/Users/robe2037/Documents/projects/dhs-research-hub/posts/2024-02-04-dhs-chirps/data/gps/BFGE61FL/BFGE61FL.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 573 features and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -5.426079 ymin: 5.684342e-14 xmax: 1.957316 ymax: 14.86258\n#&gt; Geodetic CRS:  WGS 84\n\nOur resulting dataset looks something like a tibble, except that it contains a header describing a simple feature collection with 573 features and 20 fields:\n\nbf_gps\n#&gt; Simple feature collection with 573 features and 20 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -5.426079 ymin: 5.684342e-14 xmax: 1.957316 ymax: 14.86258\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;             DHSID DHSCC DHSYEAR DHSCLUST CCFIPS ADM1FIPS ADM1FIPSNA ADM1SALBNA\n#&gt; 1  BF201000000001    BF    2010        1     UV     NULL       NULL       NULL\n#&gt; 2  BF201000000002    BF    2010        2     UV     NULL       NULL       NULL\n#&gt; 3  BF201000000003    BF    2010        3     UV     NULL       NULL       NULL\n#&gt; 4  BF201000000004    BF    2010        4     UV     NULL       NULL       NULL\n#&gt; 5  BF201000000005    BF    2010        5     UV     NULL       NULL       NULL\n#&gt; 6  BF201000000006    BF    2010        6     UV     NULL       NULL       NULL\n#&gt; 7  BF201000000007    BF    2010        7     UV     NULL       NULL       NULL\n#&gt; 8  BF201000000008    BF    2010        8     UV     NULL       NULL       NULL\n#&gt; 9  BF201000000009    BF    2010        9     UV     NULL       NULL       NULL\n#&gt; 10 BF201000000010    BF    2010       10     UV     NULL       NULL       NULL\n#&gt;    ADM1SALBCO ADM1DHS ADM1NAME DHSREGCO          DHSREGNA SOURCE URBAN_RURA\n#&gt; 1        NULL    9999     NULL       13         Sud-Ouest    GPS          R\n#&gt; 2        NULL    9999     NULL        2          Cascades    GPS          R\n#&gt; 3        NULL    9999     NULL       13         Sud-Ouest    GAZ          U\n#&gt; 4        NULL    9999     NULL       10              Nord    GPS          R\n#&gt; 5        NULL    9999     NULL        1 Boucle de Mouhoun    GPS          R\n#&gt; 6        NULL    9999     NULL        6      Centre-Ouest    GPS          R\n#&gt; 7        NULL    9999     NULL       12             Sahel    GPS          R\n#&gt; 8        NULL    9999     NULL        3            Centre    GPS          U\n#&gt; 9        NULL    9999     NULL        4        Centre-Est    GPS          U\n#&gt; 10       NULL    9999     NULL        5       Centre-Nord    GAZ          R\n#&gt;       LATNUM   LONGNUM ALT_GPS ALT_DEM DATUM                   geometry\n#&gt; 1  10.109415 -2.807555     269     269 WGS84 POINT (-2.807555 10.10942)\n#&gt; 2  10.388513 -3.907798     367     362 WGS84 POINT (-3.907798 10.38851)\n#&gt; 3   9.882864 -2.925703    9999     308 WGS84 POINT (-2.925703 9.882864)\n#&gt; 4  13.573418 -2.163120     323     323 WGS84  POINT (-2.16312 13.57342)\n#&gt; 5  12.453299 -3.461899     301     298 WGS84  POINT (-3.461899 12.4533)\n#&gt; 6  12.045308 -2.083828     338     325 WGS84 POINT (-2.083828 12.04531)\n#&gt; 7  14.354198 -0.672096     328     328 WGS84  POINT (-0.672096 14.3542)\n#&gt; 8  12.311034 -1.562071     322     324 WGS84 POINT (-1.562071 12.31103)\n#&gt; 9  11.780763 -0.363904     317     304 WGS84 POINT (-0.363904 11.78076)\n#&gt; 10 13.225114 -1.337994    9999     343 WGS84 POINT (-1.337994 13.22511)\n\nEach feature corresponds to one cluster location for which we have GPS coordinates, and each field represents a variable measured for each cluster. At the end of the output, you’ll also notice the aforementioned geometry column that contains the latitude and longitude for each displaced cluster location.\nSome clusters contain unverified coordinates and have been assigned a location of (0, 0). Since we have no way of linking these clusters to environmental data, we will remove them for this analysis using dplyr’s filter() function:\n\n# Remove empty geographies\nbf_gps &lt;- bf_gps |&gt; \n  filter(LATNUM != 0, LONGNUM != 0)\n\nVisualizing spatial data\nIt’s always worth double-checking that spatial data are being processed as expected, and this is often most easily done visually. We’ll use the ggspatial package to visualize our spatial data. This package is a convenient extension of the popular ggplot2 package.\n\n\n\n   © RStudio, Inc. (MIT) \nAs with ggplot2, ggspatial allows us to think of our plot in layers. The primary difference is that ggspatial’s layer_spatial() automatically maps the plot’s x and y dimensions to the geographic coordinates in the data.\n\n\n\n\n\n\nggplot2 and Themes\n\n\n\nThis post isn’t intended to be an introduction to ggplot2, so don’t worry if some of the plot code is unfamiliar to you—you should still be able to follow along with the other content in the post.\nAlso, we’ve modified some of the default theme elements for use in our plots throughout this post. We won’t cover all the details here, but note that your plots will likely look a bit different if you’re following along. For more about themes, see the associated documentation.\n\n\nTo add some context to our map, we’ll load a shapefile containing the Burkina Faso administrative borders. We’ll use the integrated geographies provided by IPUMS, which can be downloaded by clicking the shapefile link under the Burkina Faso section of this table. We’ve placed this shapefile alongside our DHS coordinate data in our data/gps directory, and can load it with st_read(), as we did before:\n\n# Load administrative boundary shapefile\nbf_borders &lt;- st_read(\n  \"data/gps/geo_bf2003_2010/geo_bf2003_2010.shp\",\n  quiet = TRUE\n)\n\nNow, we can plot the cluster locations (stored in bf_gps) along with the Burkina Faso administrative boundaries (stored in bf_borders):\n\nlibrary(ggplot2)\nlibrary(ggspatial)\n\n# Create map with cluster locations and BF boundaries \nggplot() +\n  layer_spatial(bf_borders, fill = NA) +\n  layer_spatial(bf_gps, size = 2, alpha = 0.4) +\n  labs(\n    title = \"Approximate DHS Cluster Locations\", \n    subtitle = \"Burkina Faso: 2010\",\n    caption = \"Source: DHS Program\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  )\n\n\n\n\n\n\n\nSo far, everything seems to be working as expected—all the cluster points fall within the national borders, and there are noticeably more points in known urban areas, like the capital region of Ouagadougou. We should be safe to continue and load our precipitation data."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#chirps-precipitation-data",
    "href": "posts/2024-02-04-dhs-chirps/index.html#chirps-precipitation-data",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "CHIRPS precipitation data",
    "text": "CHIRPS precipitation data\nFor this post, we will use precipitation data from CHIRPS, a quasi-global gridded rainfall time eries with data from 1981 to the near-present.5 CHIRPS is frequently used in health, agricultural, and hydrological research because of its relatively high spatial and temporal resolution, and validation studies have shown it to perform better than other leading precipitation models across several metrics.6,7\nImportantly, the CHIRPS data are provided as raster data, not vector data. Instead of representing precipitation with a set of polygons or points (as with vector data), raster data represent geographic features in a grid, where each cell in the grid is associated with a particular value. In our case, each cell is associated with a single daily precipitation value (in millimeters).\n\n\nComparison of vector and raster data models4\n\nsf doesn’t provide support for raster data—instead, we’ll turn to the terra package.\nRaster data\nterra has rapidly become the go-to R package for working with raster data, and it is set to supersede the raster package, which was formerly used for raster operations.\n\n\n\n   © Robert J. Hijmans et al. (GPL &gt;=3) \nIn most cases, terra provides much faster processing than does the raster package. The magic behind terra is that it avoids reading many raster images into R at once. Instead, it reads metadata about each image—information about its spatial extent, coordinate reference system, pixel count, and so on.\n\n\n\n\n\n\nraster Package Retirement\n\n\n\nAt the time of writing, the raster package is still available on CRAN, but it will be retired soon. You may see other resources referencing the raster package, but we suggest relying only on terra to ensure that your code is robust to the upcoming retirement.\n\n\nInstalling terra\nYou’ll need the same operating system dependencies required for sf to use terra to its full potential. If you’ve already got those set up for sf, it should be easy to install terra with install.packages(\"terra\"). If not, follow these instructions to set up the package.\nLoading CHIRPS data\nThe complete CHIRPS precipitation data series can be downloaded directly from the UCSB Climate Hazards Center. However, because storing global data can require significant space, most users will likely want to download data for a specific area of interest.\nThere are two primary ways to go about downloading CHIRPS data for a particular area:\n\nSelect data interactively through the CHIRPS API provider: ClimateSERV. ClimateSERV hosts a graphical user interface (GUI) that allows you to select an area of interest on a map.\nAccess data through the CHIRPS API using the API client tools provided by the chirps package.\n\nThe first option has the advantage of being visual and intuitive. However, if you need to update your data for a new area or time range, you’ll have to go through the manual download process again.\nIn contrast, the second option prioritizes reproducibility and adaptability: updating the data used in your analysis is a matter of adjusting just a few lines of code. However, your R session will be occupied while downloading data, which can take a fair amount of time for long time series.\nOption 1: ClimateSERV\nFirst, navigate to the ClimateSERV Map. From there, open the Statistical Query toolbar on the top of the left sidebar and click Select. Click Country under the Select features by dropdown. Then, click on the country of interest—in this case, Burkina Faso (in West Africa).\n\n\n\n\n\n\nWarning\n\n\n\nThe ClimateSERV interface may not work properly on all web browsers (e.g. Firefox). If you have difficulty, try opening the interface in a different browser.\n\n\n\n\n\n\n\n\n\n\nOnce you’ve indicated the country of interest, change the type of request to Download Raw Data and the download format to TIF. Finally, adjust the date range to start on 01/01/2001 and end on 12/31/2010. This will give us a 10-year range running up to the survey year, allowing us to calculate a long-run average.\nFinally, click Submit Query to download the daily rainfall totals. Note that downloading this much data may take a fair amount of time!\nOnce your request has been processed, you’ll receive a compressed (.zip) folder containing one TIF image for each day in the time span: that’s 3652 files containing many megabytes of raster data. We’ve placed the .zip file in the data directory and renamed it chirps.zip. You can unzip the file in R with unzip():\n\n# Unzip and place individual .tif files in a new `data/chirps` directory\nchirps_files &lt;- unzip(\"data/chirps.zip\", exdir = \"data/chirps\")\n\nOr, if you prefer to unzip the file manually, you can do so and then list the individual raster files with list.files():\n\n# Create list of paths to all individual raster files\nchirps_files &lt;- list.files(\"data/chirps\", full.names = TRUE)\n\nWe can load raster files with the rast() function from terra. When providing multiple files, terra will stack them so each input file becomes a single layer in a larger raster stack:\n\nlibrary(terra)\n\n# Load set of .tif files into into a single raster stack\nbf_precip &lt;- rast(chirps_files)\n\nOption 2: The chirps package\nWe can use the get_chirps() function from the chirps package to access CHIRPS data via the CHIRPS API. Since we don’t have a map where we can select our region of interest, we’ll have to provide a pre-loaded region representing the area for which we’d like data.\nWe can use the bf_borders object we created earlier to get data for the Burkina Faso region. We’ll use vect() to convert it to a SpatVector from terra (get_chirps() currently has more robust support for these objects). We’ll also input our desired date range for which to obtain data:\n\n\nThe SpatVector is the main object terra uses to represent vector data. We’ve already shown how to use sf to handle vector data, but terra also provides support for this data model. In general, we find that sf’s vector objects are a little more intuitive, but there are some cases where using terra may make certain operations easier or faster.\n\n# Load the chirps package\nlibrary(chirps)\n\n# Get CHIRPS data from 2001-2010 for Burkina Faso \nbf_precip &lt;- get_chirps(\n  vect(bf_borders),\n  dates = c(\"2001-01-01\",\"2010-12-31\"),\n  server = \"CHC\" # Recommended when obtaining data for multiple dates and/or locations\n)\n\nget_chirps() obtains data for a rectangular region around the country. For consistency with the results you would get from ClimateSERV (which only downloads data within the country borders), we’ll use terra’s mask() function to remove data from outside the Burkina Faso borders:\n\nbf_precip &lt;- mask(bf_precip, bf_borders, touches = FALSE)\n\n\n\n\n\n\n\nClimateSERV vs. chirps\n\n\n\nThere may be some trivial discrepancies between the data obtained via ClimateSERV and those obtained via the chirps package. When creating this post, we used data obtained via ClimateSERV.\nIf you instead obtained data from chirps, your values may differ slightly from those we present below. These differences shouldn’t meaningfully affect any of the operations we demonstrate.\n\n\n\nSpatRaster objects\nRegardless of the method you chose to access the CHIRPS data, you should now have a SpatRaster object from terra containing the raster stack of precipitation data covering the entirety of Burkina Faso from 2001 to 2010:\n\nbf_precip\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 115, 159, 3652  (nrow, ncol, nlyr)\n#&gt; resolution  : 0.05, 0.05  (x, y)\n#&gt; extent      : -5.549997, 2.400003, 9.349999, 15.1  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (EPSG:4326) \n#&gt; sources     : 20010101.tif  \n#&gt;               20010102.tif  \n#&gt;               20010103.tif  \n#&gt;               ... and 3649 more source(s)\n#&gt; names       : 20010101, 20010102, 20010103, 20010104, 20010105, 20010106, ...\n\nAs you can see, this raster layer contains 115 rows and 159 columns of pixels. The value in each pixel represents the rainfall (in millimeters) for an area 0.05 degrees longitude by 0.05 degrees latitude (shown in the resolution field).\nEach of the 3652 layers in the SpatRaster represents the CHIRPS precipitation values for a single day of the 10-year period between 2001 and 2010.\nTo better understand this structure, we can visualize the rainfall patterns for one of these days. We’ll first need to select an individual layer from the raster stack. We can index layers by name or layer position using [[ notation:\n\n# Select a single layer\n# (Layer names will vary depending on how you obtained your CHIRPS data)\nprecip_day &lt;- bf_precip[[\"20010719\"]]\n\nOnce we’ve selected a layer, we can use layer_spatial() to plot it, just as we did for vector objects.\n\nShow plot code# Create map of rainfall for single day\nggplot() +\n  layer_spatial(\n    precip_day, \n    alpha = if_else(values(precip_day) == 0, 0, 0.8), \n    na.rm = TRUE\n  ) +\n  layer_spatial(bf_borders, fill = NA, color = \"#888888\") +\n  layer_spatial(bf_gps,  fill = NA, size = 2, alpha = 0.4) +\n  labs(\n    title = \"Burkina Faso Rainfall: July 19, 2001\",\n    subtitle = \"CHIRPS precipitation data with DHS cluster locations\",\n    fill = \"Rainfall total (mm)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  ) +\n  scale_fill_gradient(low = \"white\", high = \"#00263A\", na.value = NA)\n\n\n\n\n\n\n\nIt can be unwieldy to manage many layers of raster data. In the next section, we’ll introduce methods for simplifying these data across both time and space so they can be more easily attached to our DHS survey."
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#cluster-buffers",
    "href": "posts/2024-02-04-dhs-chirps/index.html#cluster-buffers",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "Cluster buffers",
    "text": "Cluster buffers\nWe might think it logical to extract the mean precipitation values at each cluster’s point location. However, recall that our motivating question centered around the idea that precipitation influences health by way of impacting local crop yields. The farms that serve the population of a given cluster won’t necessarily be situated right at the recorded cluster location. Furthermore, each farm is probably impacted by the precipitation patterns in its surrounding area, not only those that occur at its exact location.\nWith this in mind, it makes sense to generate a summary metric of the precipitation in the general area of each cluster, rather than at the cluster point locations. We can accomplish this by creating buffer zones around each cluster coordinate.\nWe can use st_buffer() to create an appropriately sized buffer zone around the displaced GPS coordinates for each cluster. However, sf’s geometrical operations (powered by GEOS) assume that the input data are in meters, and our data are currently in degrees.\nTherefore, we first need to project our GPS coordinates into a new coordinate reference system, or CRS. We will use the UTM Coordinate System Zone 30N for our projection, which provides limited distortion for Burkina Faso. The EPSG code for this UTM zone is 32630, which we can provide to st_transform() to project our data:\n\n\nMap projections constitute an entire topic on their own. To learn more about projected coordinate systems, there are countless resources online, like this treatment from ESRI\n\n# Project cluster locations to the UTM 30N reference system\nbf_gps &lt;- st_transform(bf_gps, crs = 32630)\n\nWe can use st_geometry() to see a summary of our sf object’s geometry. Note that the Projected CRS now shows UTM zone 30N, and that our data are now stored in meters, rather than degrees:\n\nst_geometry(bf_gps)\n#&gt; Geometry set for 541 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 234659.3 ymin: 1092462 xmax: 1039190 ymax: 1645174\n#&gt; Projected CRS: WGS 84 / UTM zone 30N\n#&gt; First 5 geometries:\n#&gt; POINT (521084.1 1117516)\n#&gt; POINT (400626 1148510)\n#&gt; POINT (508145.5 1092462)\n#&gt; POINT (590542.4 1500704)\n#&gt; POINT (449803.2 1376723)\n\n\n\nIf you need to check your work, you can always access the current CRS of your data with st_crs().\nNow that we’re working in meters, we can buffer our points using st_buffer(). The appropriate buffer distance will depend on your specific research question of interest. For this demonstration, we’ll create a 10-kilometer (10,000 meter) buffer by setting dist = 10000:\n\nbf_gps &lt;- st_buffer(bf_gps, dist = 10000)\n\nWe see that the geometry column now describes POLYGON geometries rather than POINT geometries. Each polygon is defined by a series of points that form the circumference of the buffer zone.\n\nst_geometry(bf_gps)\n#&gt; Geometry set for 541 features \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 224659.3 ymin: 1082462 xmax: 1049190 ymax: 1655174\n#&gt; Projected CRS: WGS 84 / UTM zone 30N\n#&gt; First 5 geometries:\n#&gt; POLYGON ((531084.1 1117516, 531070.3 1116993, 5...\n#&gt; POLYGON ((410626 1148510, 410612.3 1147987, 410...\n#&gt; POLYGON ((518145.5 1092462, 518131.8 1091939, 5...\n#&gt; POLYGON ((600542.4 1500704, 600528.7 1500181, 6...\n#&gt; POLYGON ((459803.2 1376723, 459789.5 1376200, 4...\n\nWe’re finished measuring distance in meters, so we’ll again use st_transform() to revert back to degrees of latitude and longitude (EPSG 4326) for consistency with our other maps.\n\nbf_gps &lt;- st_transform(bf_gps, crs = 4326)\n\n\nShow plot codeggplot() + \n  layer_spatial(bf_borders, fill = NA) +\n  layer_spatial(bf_gps, fill = \"black\", alpha = 0.2) +\n  labs(\n    title = \"Buffered DHS Cluster Locations\",\n    subtitle = \"Burkina Faso: 2010\",\n    caption = \"Source: DHS Program\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nHandling cluster displacement\n\n\n\nPreviously, we discussed the fact that the cluster coordinates provided by The DHS Program are displaced from their true locations. While we don’t need to address this for our precipitation data, there may be other cases where it is useful to obtain a more precise location for each cluster (for instance, if working with road networks or other fine-grained data).\nIn these cases, you can certainly buffer points conditionally on other values in the data by providing a conditional statement to the buffer dist argument. For instance, to buffer urban clusters (URBAN_RURA == \"U\") by 2000 meters and other clusters by 5000 meters, use an if_else() statement:\n\nst_buffer(\n  bf_gps, \n  dist = if_else(bf_gps$URBAN_RURA == \"U\", 2000, 5000)\n)"
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#aggregating-rainfall-within-cluster-regions",
    "href": "posts/2024-02-04-dhs-chirps/index.html#aggregating-rainfall-within-cluster-regions",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "Aggregating rainfall within cluster regions",
    "text": "Aggregating rainfall within cluster regions\nNow that we have buffered cluster regions, we can focus on combining our two data sources. Let’s zoom in on a single cluster as an example. We’ll use the cluster with DHSCLUST number 160:\n\nclust &lt;- bf_gps |&gt; \n  filter(DHSCLUST == 160)\n\nTo simplify this example, we’ll use terra’s crop() to restrict our raster to the cells surrounding this cluster:\n\nprecip_clust &lt;- crop(bf_precip_mean, clust, snap = \"out\")\n\n\nShow plot codeggplot() + \n  layer_spatial(precip_clust, alpha = 0.8) + \n  layer_spatial(clust, alpha = 0, linewidth = 0.5, color = \"black\") +\n  labs(\n    title = \"Average Rainfall: 2001-2010\",\n    subtitle = \"Single DHS cluster\",\n    fill = \"10-year average rainfall (mm/day)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  scale_fill_gradient(\n    low = \"#FAEFD1\", \n    high = \"#00263A\", \n    na.value = NA, \n    limits = c(2.3, 2.5)\n  )\n\n\n\n\n\n\n\n\n\nFor demonstration purposes, we’ve used different limits for the color scale in this plot and the next. In general, we’d want to ensure that the colors in this plot map to the same values used in other plots with the same color scheme.\nWe see a range of rainfall totals across the 20 pixels in this area. How can we summarize the information contained in these individual pixels into a single estimate of the rainfall for this buffered cluster region?\nThe most straightforward approach might be to take the mean of all the pixel values that overlap our buffer region. First, we need to obtain the values of the overlapping raster cells. We can use terra’s extract() to obtain the raster cell values from the region defined by an overlapping vector data source (in this case, our example cluster, clust):\n\nextract(bf_precip_mean, clust)\n#&gt;    ID     mean\n#&gt; 1   1 2.337887\n#&gt; 2   1 2.329431\n#&gt; 3   1 2.344792\n#&gt; 4   1 2.317271\n#&gt; 5   1 2.343546\n#&gt; 6   1 2.329051\n#&gt; 7   1 2.381871\n#&gt; 8   1 2.367312\n#&gt; 9   1 2.395538\n#&gt; 10  1 2.395361\n#&gt; 11  1 2.402505\n#&gt; 12  1 2.415130\n\nNote that this provides mean daily precipitation values for 12 different cells. By default, extract() will only extract values from the cells whose center point lies within the overlaid polygon:\n\nShow plot code# New SpatRaster with same extent as bf_precip_mean, but with empty values\next_cells &lt;- rast(bf_precip_mean, vals = NA)\n\n# ID cell locations for each cell that is extracted\ncell_idx &lt;- extract(ext_cells, clust, cells = TRUE)$cell\n\next_cells[cell_idx] &lt;- bf_precip_mean[cell_idx]\n\nggplot() + \n  layer_spatial(\n    crop(ext_cells, clust, snap = \"out\"), \n    alpha = 0.8, \n    na.rm = TRUE\n  ) +\n  layer_spatial(clust, alpha = 0, linewidth = 0.5, color = \"black\") +\n  labs(\n    title = \"Average Rainfall: 2001-2010\",\n    subtitle = \"Single DHS cluster\",\n    fill = \"10-year average rainfall (mm/day)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  scale_fill_gradient(\n    low = \"#FAEFD1\", \n    high = \"#00263A\", \n    na.value = NA, \n    limits = c(2.3, 2.5)\n  )\n\n\n\n\n\n\n\nTo instead include all cells that intersect the polygon, set touches = TRUE:\n\nclust_precip &lt;- extract(bf_precip_mean, clust, touches = TRUE)\n\nclust_precip\n#&gt;    ID     mean\n#&gt; 1   1 2.350189\n#&gt; 2   1 2.319925\n#&gt; 3   1 2.385132\n#&gt; 4   1 2.337887\n#&gt; 5   1 2.329431\n#&gt; 6   1 2.328763\n#&gt; 7   1 2.344792\n#&gt; 8   1 2.317271\n#&gt; 9   1 2.343546\n#&gt; 10  1 2.329051\n#&gt; 11  1 2.381871\n#&gt; 12  1 2.367312\n#&gt; 13  1 2.395538\n#&gt; 14  1 2.395361\n#&gt; 15  1 2.393321\n#&gt; 16  1 2.402505\n#&gt; 17  1 2.415130\n#&gt; 18  1 2.395817\n\nThis operation provides the mean daily precipitation values for each of the 18 cells that intersect our cluster region. However, some cells cover a much larger portion of the cluster region than others. To account for the heterogeneity in size, we can set weights = TRUE to include a weight column, which records the proportion of each cell that is covered by the polygon:\n\n\nweights = TRUE automatically includes all intersected cells, but those with negligible weights will be removed.\n\nclust_precip &lt;- extract(bf_precip_mean, clust, weights = TRUE)\n\nclust_precip\n#&gt;    ID     mean weight\n#&gt; 1   1 2.385132   0.20\n#&gt; 2   1 2.337887   0.90\n#&gt; 3   1 2.329431   0.94\n#&gt; 4   1 2.328763   0.33\n#&gt; 5   1 2.344792   0.70\n#&gt; 6   1 2.317271   1.00\n#&gt; 7   1 2.343546   1.00\n#&gt; 8   1 2.329051   0.86\n#&gt; 9   1 2.381871   0.57\n#&gt; 10  1 2.367312   1.00\n#&gt; 11  1 2.395538   1.00\n#&gt; 12  1 2.395361   0.74\n#&gt; 13  1 2.393321   0.03\n#&gt; 14  1 2.402505   0.47\n#&gt; 15  1 2.415130   0.53\n#&gt; 16  1 2.395817   0.09\n\nWe can then provide the weights to the weighted.mean() function to calculate a weighted mean that is shifted toward the cell values that form a larger proportion of the polygon area.\n\nweighted.mean(clust_precip$mean, clust_precip$weight)\n#&gt; [1] 2.358508\n\nBecause this kind of workflow is so common, terra provides a quicker shorthand in extract(). You can use the fun argument to specify an aggregation function at the same time that you extract the raster cell values:\n\n# Extract area-weighted average for all cells covered by `clust`\nextract(\n  bf_precip_mean,\n  clust,\n  fun = \"mean\", # Average all extracted values\n  weights = TRUE # Use area weights\n)\n#&gt;   ID     mean\n#&gt; 1  1 2.358508\n\nThe 2.358508 value represents the area-weighted mean of the 10-year mean precipitation values (in millimeters per day) within the given cluster region.\nScaling up for all clusters\nTo aggregate the rainfall data for each DHS cluster in the sample, we simply scale up. We again use extract(), but we provide our raster of precipitation values for the entire country and the polygons for all of our buffered clusters.\n\n# Average the mean CHIRPS values within each DHS cluster\nclust_means &lt;- extract(\n  bf_precip_mean,\n  bf_gps, # All clusters included\n  fun = \"mean\",\n  weights = TRUE,\n  na.rm = TRUE, # Ignore missing CHIRPS values\n  bind = TRUE # Recombine with original `bf_gps` data\n)\n\nBy setting bind = TRUE, we can automatically recombine the extracted means with the other cluster data in bf_gps.\nWe also set na.rm = TRUE to ensure we don’t get missing values for clusters whose regions expand beyond the country borders (where we haven’t downloaded any CHIRPS data). This decision may be reasonable if our interest is in rainfall over local crop-producing regions, and we don’t believe that clusters in border regions obtain substantial inputs from farms outside of the country. (If we wanted to include the rainfall outside of the country borders, we would simply need to expand our bounding box when downloading our CHIRPS data.)\nFinally, we’ll convert our cluster means back to an sf object. Notice that there now exists a mean column that stores the weighted mean values for each cluster:\n\n# Convert to sf object\nclust_means &lt;- st_as_sf(clust_means)\n\nclust_means |&gt; \n  select(DHSID, mean, geometry)\n#&gt; Simple feature collection with 541 features and 2 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -5.517451 ymin: 9.792417 xmax: 2.049008 ymax: 14.95288\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;             DHSID     mean                       geometry\n#&gt; 1  BF201000000001 2.758146 POLYGON ((-2.71628 10.10935...\n#&gt; 2  BF201000000002 2.782180 POLYGON ((-3.816453 10.3887...\n#&gt; 3  BF201000000003 3.004422 POLYGON ((-2.834491 9.88283...\n#&gt; 4  BF201000000004 1.678181 POLYGON ((-2.070699 13.5730...\n#&gt; 5  BF201000000005 2.129466 POLYGON ((-3.369883 12.4534...\n#&gt; 6  BF201000000006 2.203529 POLYGON ((-1.991964 12.0449...\n#&gt; 7  BF201000000007 1.113443 POLYGON ((-0.579432 14.3532...\n#&gt; 8  BF201000000008 2.110246 POLYGON ((-1.470133 12.3105...\n#&gt; 9  BF201000000009 2.281224 POLYGON ((-0.2722169 11.779...\n#&gt; 10 BF201000000010 1.722371 POLYGON ((-1.245736 13.2245...\n\nNow that we have aggregated rainfall across all clusters, we can plot the average rainfall for each of the 10-kilometer buffered cluster regions. Each cluster region is associated with a single 10-year average rainfall value.\n\nShow plot codeggplot() + \n  layer_spatial(bf_borders, fill = NA) +\n  layer_spatial(clust_means, aes(fill = mean), alpha = 0.8) +\n  labs(\n    title = \"Average Rainfall within Clusters: 2001-2010\",\n    subtitle = \"Burkina Faso: 2010 Clusters\",\n    fill = \"10-year average rainfall (mm/day)\",\n    caption = \"Source: DHS Program and Climate Hazards Center InfraRed Precipitation with Station (CHIRPS)\"\n  ) +\n  annotation_scale(\n    aes(style = \"ticks\", location = \"br\"), \n    text_col = \"#999999\",\n    line_col = \"#999999\"\n  ) +\n  scale_fill_steps(low = \"#FAEFD1\", high = \"#00263A\", n.breaks = 4) +\n  theme(legend.ticks = element_blank())"
  },
  {
    "objectID": "posts/2024-02-04-dhs-chirps/index.html#getting-help",
    "href": "posts/2024-02-04-dhs-chirps/index.html#getting-help",
    "title": "Attaching CHIRPS Precipitation Data to DHS Surveys",
    "section": "Getting Help",
    "text": "Getting Help\nQuestions or comments? Check out the IPUMS User Forum or reach out to IPUMS User Support at ipums@umn.edu."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Latest Posts",
    "section": "",
    "text": "Time-specific temperature exposures for DHS survey respondents\n\n\n\n\n\n\nCHIRTS\n\n\nTemperature\n\n\nHeat\n\n\nLongitudinal Analysis\n\n\nTime series\n\n\nMultilevel Modeling\n\n\nClimate\n\n\nterra\n\n\nlme4\n\n\ntidyr\n\n\nR\n\n\n\nFine-tune an analysis by incorporating temporal information from a DHS survey with longitudinal climate data \n\n\n\n\n\nMay 31, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nFlexible Workflows with CHIRTS Temperature Data\n\n\n\n\n\n\nCHIRTS\n\n\nTemperature\n\n\nHeat\n\n\nClimate\n\n\nTime series\n\n\nReproducible workflows\n\n\nFunctional programming\n\n\nterra\n\n\nsf\n\n\nggspatial\n\n\nR\n\n\n\nGeneralize your code to more easily aggregate time-varying climate data \n\n\n\n\n\nApr 15, 2024\n\n\nFinn Roberts\n\n\n\n\n\n\n\n\n\n\n\n\nDemystifying Spatially Harmonized Geography in IPUMS DHS\n\n\n\n\n\n\nIntegrated geography\n\n\nSpatial harmonization\n\n\nLongitudinal analysis\n\n\nMethodology\n\n\nResearch concepts\n\n\nCartography\n\n\n\nA demonstration of data and techniques to handle shifting geographic boundaries in longitudinal research \n\n\n\n\n\nApr 9, 2024\n\n\nSula Sarkar, Devon Kristiansen, Miriam King\n\n\n\n\n\n\n\n\n\n\n\n\nDroplets of Insights for Integrating DHS and Rainfall Data\n\n\n\n\n\n\nClimate\n\n\nPrecipitation\n\n\nCHIRPS\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nSome factors to consider when obtaining and preparing precipitation data for use with DHS surveys \n\n\n\n\n\nMar 7, 2024\n\n\nAudrey Dorélien, Molly Brown\n\n\n\n\n\n\n\n\n\n\n\n\nAttaching CHIRPS Precipitation Data to DHS Surveys\n\n\n\n\n\n\nCHIRPS\n\n\nPrecipitation\n\n\nClimate\n\n\nAgriculture\n\n\nterra\n\n\nsf\n\n\nggspatial\n\n\nR\n\n\n\nRemotely sensed precipitation data can provide important context for understanding the health outcomes reported in the DHS \n\n\n\n\n\nFeb 4, 2024\n\n\nFinn Roberts, Matt Gunther\n\n\n\n\n\n\n\n\n\n\n\n\nDeveloping Robust Conceptual Models in Climate Change and Health Research\n\n\n\n\n\n\nClimate\n\n\nMethodology\n\n\nResearch concepts\n\n\n\nIntroducing the theoretical foundations for effective research on climate change and health \n\n\n\n\n\nFeb 3, 2024\n\n\nElizabeth Heger Boyle, Kathryn Grace, Audrey Dorélien, Devon Kristiansen\n\n\n\n\n\n\n\n\n\n\n\n\nObtaining Data from IPUMS DHS\n\n\n\n\n\n\nTips\n\n\nImporting data\n\n\nR\n\n\n\nIdentify and download data from the IPUMS DHS website and load it into R \n\n\n\n\n\nFeb 2, 2024\n\n\nFinn Roberts, Matt Gunther\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to R\n\n\n\n\n\n\nTips\n\n\nR\n\n\n\nGetting started with R and RStudio\n\n\n\n\n\nFeb 1, 2024\n\n\nFinn Roberts, Matt Gunther\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this Blog",
    "section": "",
    "text": "IPUMS DHS data provide a critical resource to help illuminate the relationship between climate change and population health. To support such research, the IPUMS Global Health team received a 2023 supplemental grant from the Eunice Kennedy Shriver National Institute of Child Health and Human Development, or NICHD (3R01HD069471-12S1).\nThis blog is designed to be a resource for researchers who are familiar with population health survey data but new to research using spatial data sources. The blog will introduce spatial data concepts and demonstrate basic spatial data processing techniques in R.\nAdditionally, the supplemental grant supports:\n\nthe integration and dissemination of DHS malaria monitoring data (MIS surveys) in IPUMS DHS.\nthe dissemination of displaced GPS coordinates for DHS primary sampling units and the addition of new contextual variables to the IPUMS data extract system.\n\nThese three elements will make it easier to add environmental context to research using IPUMS DHS surveys and will provide additional data relevant to disease vector transmission. Combined, they will enhance researchers’ ability to use IPUMS DHS data to understand the impacts of climate change on health.\n\n\nThis blog is developed in collaboration with the Climate Hazards Center at the University of California, Santa Barbara and is supported by USAID Cooperative Agreement 72DFFP19CA00001.\nThe IPUMS research team also receives support as members of the Minnesota Population Center through a grant from the NICHD Population Research Infrastructure Program (P2C HD041023)."
  },
  {
    "objectID": "about.html#supporting-climate-and-health-research",
    "href": "about.html#supporting-climate-and-health-research",
    "title": "About this Blog",
    "section": "",
    "text": "IPUMS DHS data provide a critical resource to help illuminate the relationship between climate change and population health. To support such research, the IPUMS Global Health team received a 2023 supplemental grant from the Eunice Kennedy Shriver National Institute of Child Health and Human Development, or NICHD (3R01HD069471-12S1).\nThis blog is designed to be a resource for researchers who are familiar with population health survey data but new to research using spatial data sources. The blog will introduce spatial data concepts and demonstrate basic spatial data processing techniques in R.\nAdditionally, the supplemental grant supports:\n\nthe integration and dissemination of DHS malaria monitoring data (MIS surveys) in IPUMS DHS.\nthe dissemination of displaced GPS coordinates for DHS primary sampling units and the addition of new contextual variables to the IPUMS data extract system.\n\nThese three elements will make it easier to add environmental context to research using IPUMS DHS surveys and will provide additional data relevant to disease vector transmission. Combined, they will enhance researchers’ ability to use IPUMS DHS data to understand the impacts of climate change on health.\n\n\nThis blog is developed in collaboration with the Climate Hazards Center at the University of California, Santa Barbara and is supported by USAID Cooperative Agreement 72DFFP19CA00001.\nThe IPUMS research team also receives support as members of the Minnesota Population Center through a grant from the NICHD Population Research Infrastructure Program (P2C HD041023)."
  },
  {
    "objectID": "about.html#what-is-ipums-dhs",
    "href": "about.html#what-is-ipums-dhs",
    "title": "About this Blog",
    "section": "What is IPUMS DHS?",
    "text": "What is IPUMS DHS?\nIPUMS provides census and survey data from around the world integrated across time and space. IPUMS integration and documentation makes it easy to study change, conduct comparative research, merge information across data types, and analyze individuals within family and community context.\nIPUMS is comprised of several individual products, each with different data sources and content areas. IPUMS DHS is one of several IPUMS Global Health products—representative household surveys, primarily from low- and middle-income countries, that gather extensive information on health, family planning, living conditions, and more.\nIPUMS DHS specifically facilitates analysis of data provided by The Demographic and Health Surveys Program (DHS), which has been administered in low- and middle-income countries since the 1980s.\nIPUMS DHS harmonizes DHS variables over time and provides comprehensive cross-survey documentation to make it easier to find and understand DHS data.\nIPUMS DHS data are available free of charge, but users must register with The DHS Program to gain access. Registered DHS users can enter their DHS username and password to access data from IPUMS DHS. For more information about downloading IPUMS DHS data, see the associated post."
  },
  {
    "objectID": "about.html#how-to-cite",
    "href": "about.html#how-to-cite",
    "title": "About this Blog",
    "section": "How to cite",
    "text": "How to cite\nFor a comprehensive guide to citing IPUMS DHS, see the citation page on the IPUMS DHS website.\nTo cite an individual post on this blog, we suggest the following format:\n\nRoberts, F., & Gunther, M. (2024, Februrary 4). Attaching CHIRPS precipitation data to DHS surveys. IPUMS DHS Climate Change and Health Research Hub. https://tech.popdata.org/dhs-research-hub/posts/2024-02-04-dhs-chirps/index.html"
  },
  {
    "objectID": "about.html#getting-help",
    "href": "about.html#getting-help",
    "title": "About this Blog",
    "section": "Getting help",
    "text": "Getting help\nIPUMS users can find help on the IPUMS User Forum or by contacting IPUMS user support at ipums@umn.edu."
  },
  {
    "objectID": "about.html#related-work",
    "href": "about.html#related-work",
    "title": "About this Blog",
    "section": "Related work",
    "text": "Related work\nIPUMS Global Health also houses the IPUMS PMA Data Analysis Hub, a similar blog focused on working with IPUMS PMA data in R."
  }
]