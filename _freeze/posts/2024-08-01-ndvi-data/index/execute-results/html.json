{
  "hash": "2cc78f50e2f9164b742af4da8263b9a5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"An Iterative Workflow for Loading NDVI Data in R\"\ndescription: \"Use purrr and terra to efficiently integrate NDVI data across multiple spatial extents\"\nauthor: \n  - name: \"Finn Roberts\"\n    affiliation: \"IPUMS Senior Data Analyst\"\ndate: 08-01-2024\ncategories:\n  - NDVI\n  - Agriculture\n  - Extreme weather\n  - Food production\n  - Importing Data\n  - R\n  - Reproducible workflows\n  - Time series\n  - stringr\n  - purrr\n  - terra\n  - ggspatial\n  - sf\nfig-width: 10\nfig-height: 8\nimage: index_files/figure-html/listing-img-1.png\nopen-graph:\n  title: |\n    An Iterative Workflow for Loading NDVI Data in R\n  description: |\n    Use purrr and terra to efficiently integrate NDVI data across multiple\n    spatial extents\n  image: index_files/figure-html/listing-img-1.png\ntwitter-card:\n  title: |\n    An Iterative Workflow for Loading NDVI Data in R\n  description: |\n    Use purrr and terra to efficiently integrate NDVI data across multiple\n    spatial extents\n  image: index_files/figure-html/listing-img-1.png\n---\n\n::: {.cell}\n\n:::\n\n\n\nIn our [previous post](../2024-07-02-ndvi-concepts), we introduced NDVI\nas a remote sensing tool that can be used to study the effects of\nenvironmental conditions on malnourishment and other health outcomes. In\nthis post, we'll show how to access and load NDVI data using R.\n\n# NDVI vs. other environmental data sources\n\nLike the other environmental metrics we've used on this blog\n([CHIRPS](../2024-02-04-dhs-chirps) and\n[CHIRTS](../2024-04-15-chirts-metrics)), NDVI data are also provided in\nraster format. However, there are some key differences.\n\n1.  First, NDVI is a *calculated value*, whereas our previous metrics\n    were *modeled values*. That is, NDVI is calculated directly from the\n    reflectance of different light wavelengths as measured via\n    satellite. In contrast, CHIRPS and CHIRTS integrate ground-level\n    station data with satellite observations to create a **model** of\n    rainfall and temperature across the surface.\n\n2.  NDVI is also provided at higher resolutions than CHIRPS or CHIRTS.\n    For population health research, this resolution typically exceeds\n    what is useful since we will be linking NDVI data to survey data\n    that has far more geographic uncertainty. Still, this difference can\n    introduce new challenges as NDVI raster files can be much larger\n    than the files for the other metrics we have worked with so far.\n\n3.  Depending on the data source, NDVI data may be distributed for\n    specific spatial extents and at multi-week time intervals. Whereas\n    daily time series of CHIRPS and CHIRTS could be downloaded for large\n    spatial extents and cropped to an area of interest, NDVI often\n    requires more manual work to integrate data from different spatial\n    extents and time intervals.\n\nThis post will primarily focus on this last point. We'll demonstrate\nsome of the additional steps required to produce a usable NDVI time\nseries for a particular area of interest. In the process, we'll\nintroduce new R techniques that can assist in this workflow.\n\n# Obtaining NDVI Data\n\nNot only are there multiple sources that provide NDVI data, there are\nalso multiple satellites that collect the images used to calculate NDVI.\nFor this post, we'll use data from NASA's\n[MODIS](https://terra.nasa.gov/about/terra-instruments/modis) satellite,\nwhich provides data at a 250 meter resolution.\n\n::: column-margin\nWhile this resolution exceeds our typical needs, we use 250 meter data\nfor this post and the next to help demonstrate some of the relevant\nconsiderations for working with NDVI data in a population health\ncontext.\n:::\n\n::: callout-note\n## MODIS Retirement\n\nRemote sensing instruments exist in a constant state of evolution as\nsatellites are launched and retired. This means that researchers relying\non remotely sensed data—like NDVI—often need to develop fluency with\nmultiple data products that may cover different time periods, spatial\nranges, or provide different types of imaging data.\n\nIn fact, NASA's Terra satellite—which houses MODIS instruments—will be\nretired in the near future. Because of this, many researchers now use\nNASA's [VIIRS](https://www.earthdata.nasa.gov/sensors/viirs) product to\nobtain NDVI data.\n\nWe're still using MODIS for the demonstrations in this post because\nthese data are widely familiar and may still be valuable for certain\ntime periods before newer products became available.\n\nIn the future, we hope to release more posts highlighting additional\ndata products!\n:::\n\n## Earthdata Search\n\nWe'll use NASA's Earthdata Search web interface to select and download\ndata. Earthdata Search provides access to NASA's immense archive of\nsatellite data.\n\nBefore you can access MODIS register for access to Earthdata Search,\nyou'll need to [register](https://urs.earthdata.nasa.gov/home) for a\nfree account.\n\n### Find data\n\nOnce you've registered for an account, you can open up the [Earthdata\nSearch interface](https://search.earthdata.nasa.gov/search), which\nshould look something like this:\n\n::: column-page\n![](images/ed_search.png)\n:::\n\nFrom this interface, you can search for data across many NASA products.\n\n#### Selecting a product\n\nAs mentioned above, we'll use data from MODIS for this post. MODIS\nprovides several different [NDVI\nproducts](https://modis.gsfc.nasa.gov/data/dataprod/mod13.php) that vary\nin their temporal and spatial resolution. Each product has a specific\ncode used to reference it (see the previous link for a table including\nthese codes).\n\nSome products are provided at the monthly level. If you don't need finer\ntemporal resolution, it's preferable to download the monthly data\nprovided directly by NASA. This is because NASA performs additional\nquality assurance steps when producing monthly data, including adding\ncorrections for pixels obscured due to cloud cover and other similar\ntweaks. NASA's processing will generally be better than any\npreprocessing we would do on our own.\n\nOf course, for more finely-tuned temporal analyses (e.g. start-of-season\nanalyses), it may be more appropriate to download data from one of the\n16-day MODIS products.\n\nEither way, you can search for a particular product by typing its code\nin the search bar in the top left of the Earthdata Search interface, as\nshown below.\n\n::: column-page\n![](images/ed_search_mod13q1.png)\n:::\n\nIn our case, we've selected the MOD13Q1 product, which provides data at\na 250 meter resolution in 16-day increments.\n\nYou can also select a geographic region of interest using the polygon\ndrawing tools on the right side of the screen. For instance, to identify\ndata for Kenya, we can draw a polygon around the country. Selecting a\nregion will highlight the different areas, or *tiles*, for which we can\ndownload data.\n\nIn the case of Kenya, the country spans 4 different MODIS tiles. To\nobtain full coverage of the entire country, we'll need to download each\ntile and stich them together.\n\n::: column-page\n![](images/ed_search_ke_bbox.png)\n:::\n\nTo narrow down further, you can filter the time range of data by\nentering start and end dates on the left side of the interface.\n\nOnce you've selected the area and time of interest, you can click the\n**Download** button to proceed to the download page, as shown below.\n\n::: column-page\n![](images/ed_search_time.png)\n:::\n\n### Download\n\nDepending on how many tiles and time increments you'll be downloading,\nyou'll notice that the estimated data storage size will be quite large.\nThis is both a product of the resolution of this data as well as the\nfact that we must download full tiles of data. That is, we can't\ndownload data just for our region of interest alone; instead, we have to\ndownload data for the MODIS tile(s) that contain that region and then\ncrop the resulting data.\n\n#### Data storage concerns\n\nData storage can therefore be a significant barrier with working with\nlong time series of NDVI data. This is why most institutions and\nresearchers who work extensively with raster data have dedicated server\nspace to use when running raster operations.\n\nHowever, if your area of interest is small enough, it may be possible to\nbuild up a dataset by downloading and cropping data to that area\nincrementally, preventing you from needing to store large image files\nindefinitely.\n\nThere are also cloud-based solutions available, [Google Earth\nEngine](https://earthengine.google.com/) and [Amazon Web\nServices](https://aws.amazon.com/earth/). Google Earth Engine in\nparticular has become a popular interface that combines an interactive\nmap and documentation with a JavaScript library that allows you to write\nscripts for geoprocessing on Google's servers. We won't discuss these\noptions extensively here, but if you begin working on more\ndata-intensive spatial projects, it may be worth exploring what they\nhave to offer.\n\n#### Downloaded files\n\nAfter clicking the **Download** link, you should be redirected to a\ndownload page that shows the individual files that fit your selection\ncriteria. The page will also include a `download.sh` script that will\nperform a batch download. This can be useful if you're downloading data\nfor many files at once. See the [Earthdata\ndocumentation](https://wiki.earthdata.nasa.gov/display/EDSC/How+To%3A+Use+the+Download+Access+Script)\nfor more details on how to use this script to automate the download of\nmany files.\n\nYou'll get a separate HDF file for each tile and time increment.\nFortunately, `{terra}` is able to load HDF files, so we don't need to\nlearn any new R tools yet. However, because each tile represents a\n*different* spatial extent, we need to use a new technique to combine\nthem into a single source, called a **mosaic**.\n\n::: column-margin\n`<div class=\"hex\">\n<a href=\"https://rspatial.github.io/terra/index.html\" class=\"hex\">\n<img src=\"../../images/hex/terra.png\" class=\"hex-img\"/>\n</a>\n<p class=\"hex-cap\">© Robert J. Hijmans et al. (GPL &gt;=3)</p>\n</div>`{=html}\n:::\n\n# Preparing NDVI data\n\nNow that we have our data downloaded, we'll shift our focus to loading\nthose data into R. For this demonstration, we've downloaded data for the\n4 tiles that cover the entirety of Kenya for 2 time intervals in 2014.\nThat is, we have 8 total files downloaded.\n\nIn practice, you'd likely include a longer time series; we've opted to\nuse only 2 dates to limit the processing time required for this demo.\nRegardless, we will demonstrate a workflow that can be easily scaled to\nload more files.\n\nBefore getting started, we'll load some of the familiar packages that\nhave been introduced previously on the blog:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(terra)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(ggspatial)\n```\n:::\n\n\n\n## NASA image files\n\nNASA provides its files in HDF format. Each HDF file is identified by\nits tile code (e.g. `h21v08`) and its timestamp code (e.g. `A2014001`).\nTo mosaic our tiles, we'll need to create a separate raster stack for\neach tile across all of our timestamps.\n\n::: column-margin\nHDF stands for Hierarchical Data Format, yet another common raster data\nfile format.\n:::\n\nFirst, we'll list all our files, which we've saved in a directory we've\nnamed `data_local/MOD13Q1`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles <- list.files(\"data_local/MOD13Q1\", full.names = TRUE)\n```\n:::\n\n\n\nYou'll notice that each file contains the tile code and timestamp in the\nfile name.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles\n#> [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v08.061.2021246163834.hdf\"\n#> [2] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v09.061.2021246163703.hdf\"\n#> [3] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v08.061.2021246164307.hdf\"\n#> [4] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v09.061.2021246155251.hdf\"\n#> [5] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v08.061.2021260011612.hdf\"\n#> [6] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v09.061.2021260011337.hdf\"\n#> [7] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v08.061.2021260023343.hdf\"\n#> [8] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v09.061.2021260023455.hdf\"\n```\n:::\n\n\n\nUltimately, we'll need to group each of these files such that we can\ncreate a single raster stack for each **tile**. The layers in that stack\nshould correspond to the data's **timestamps**. This will produce a time\nseries of NDVI data for each spatial region which can later be combined\nwith the adjacent regions.\n\nTo achieve this, we'll need to leverage the tile and timestamp codes\nthat NASA provides in its file names.\n\n## Regular expressions\n\nRight now, all our files are in one large list. But because they\nrepresent different spatial extents, we can't load them all at once with\nterra:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrast(files)\n#> Error: [rast] extents do not match\n```\n:::\n\n\n\nInstead, we need to first organize our files by their tile codes. We\ncould go ahead and manually store the codes for later access like so:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntile_codes <- c(\"h21v08\", \"h21v09\", \"h22v08\", \"h22v09\")\ntimestamps <- c(\"2014001\", \"2014209\")\n```\n:::\n\n\n\nHowever, this wouldn't produce a very flexible workflow: if we ever\ndecided to perform a similar analysis for different times or spatial\nregions, we'd have to modify these codes manually.\n\nAn alternative approach would be to dynamically extract the codes using\n**regular expressions**. The term *regular expressions* refers to a\nspecific syntax designed to identify and extract sequences of characters\nin a set of text.\n\nWe'll use the `{stringr}` package to demonstrate the basics of regular\nexpressions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(stringr)\n```\n:::\n\n\n\n::: column-margin\n`<div class=\"hex\">\n<a href=\"https://stringr.tidyverse.org/index.html\" class=\"hex\">\n<img src=\"../../images/hex/stringr.png\" class=\"hex-img\"/>\n</a>\n<p class=\"hex-cap\">© RStudio, Inc. (MIT)</p>\n</div>`{=html}\n:::\n\nAt their simplest, regular expressions allow you to extract specific\ncharacters from a string:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# str_detect() tells us whether a search pattern exists in the target string\n\n# \"b\" does exist in the string\nstr_detect(\"abc123\", \"b\")\n#> [1] TRUE\n\n# \"z\" does not exist in the string:\nstr_detect(\"abc123\", \"z\")\n#> [1] FALSE\n```\n:::\n\n\n\nHowever, regular expressions may also include special characters, which\nallow you to search more flexibly. For instance, `[` brackets allow us\nto define a *character class*, which will detect whether any of the\nenclosed characters is present.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_detect(\"abc123\", \"[ahz]\")\n#> [1] TRUE\n```\n:::\n\n\n\nWe can use a similar approach to search for any numeric digits in our\nstring:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_detect(\"abc123\", \"[0-9]\")\n#> [1] TRUE\n```\n:::\n\n\n\nCurly braces can be used to detect a certain number of sequential\ncharacters. For instance, to determine whether the string contains 3\nconsecutive digits:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_detect(\"abc123\", \"[0-9]{3}\")\n#> [1] TRUE\n```\n:::\n\n\n\nThis returns `TRUE`, but if we searched for a longer sequence, we'd no\nlonger be able to detect a match for the pattern:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_detect(\"abc123\", \"[0-9]{4}\")\n#> [1] FALSE\n```\n:::\n\n\n\nThe special characters `^` and `$` represent the start and end of the\nstring, respectively. For instance, to determine if the string starts\nwith a digit:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_detect(\"abc123\", \"^[0-9]\")\n#> [1] FALSE\n```\n:::\n\n\n\nOr to determine if it ends with a digit:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_detect(\"abc123\", \"[0-9]$\")\n#> [1] TRUE\n```\n:::\n\n\n\nThere are far more [available\noptions](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions/Cheatsheet)\nin the regular expression language, and we won't be able to cover them\nall here. The stringr package includes a helpful\n[introduction](https://stringr.tidyverse.org/articles/regular-expressions.html?q=:digit:#matching-multiple-characters)\nto help you get started.\n\nIn our case, we can use the basic syntax we just introduced to extract\nthe tile code from our file names:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract h00v00 tile code pattern\ntile_codes <- unique(str_extract(files, \"h[0-9]{2}v[0-9]{2}\"))\n\ntile_codes\n#> [1] \"h21v08\" \"h21v09\" \"h22v08\" \"h22v09\"\n```\n:::\n\n\n\nImportantly, if we ever download new files for different tiles, this\ncode will *still extract their tile codes* without any modification\n(that is, unless NASA changes their file naming conventions\nunexpectedly).\n\nRegular expressions therefore allow us to implement a **generalized**\napproach that is more robust to future changes in our analysis.\n\n## An iterative R workflow\n\nNext, we need to group the files so that all files for a single tile are\nplaced together. That way, all the timestamps for each tile will be\nloaded into a raster stack with a single spatial extent. Each layer will\ncorrespond to a different timestamp.\n\nBecause we have multiple tiles and multiple files for each tile, we will\nlikely need to apply the same set of processing operations many times\nover. While we could copy and paste our code to group each tile's files,\nthis would quickly become tedious. Furthermore, hard-coding our file\ngroups would make it harder to make modifications if we were to add more\nfiles to our analysis or adapt our code to work with a different region.\n\nInstead, we can build an *iterative* workflow that will automatically\napply our processing steps for each tile.\n\n### The purrr package\n\nWe'll use the `{purrr}` package to build our workflow. The purrr package\nprovides a concise syntax for iterating through multiple inputs. This is\nprimarily facilitated by purrr's `map()` function.\n\n::: column-margin\n`<div class=\"hex\">\n<a href=\"https://purrr.tidyverse.org/index.html\" class=\"hex\">\n<img src=\"../../images/hex/purrr.png\" class=\"hex-img\"/>\n</a>\n<p class=\"hex-cap\">© RStudio, Inc. (GPL-3)</p>\n</div>`{=html}\n:::\n\n::: callout-note\n## Iterating in base R\n\nYou may already be familiar with the `for` loop or with `lapply()`, both\nof which are options for iterating in base R (`lapply()` is most similar\nto purrr's `map()`).\n\nIf you're more comfortable with these tools, the code we introduce below\ncan certainly be adapted for a `for` loop or `lapply()` workflow.\n:::\n\n`map()` allows us to iterate by providing two things:\n\n-   The list (or vector) of objects we want to iterate over\n-   A function describing the operation we want to do for each element\n    in the list\n\nFor instance, take this list of letters:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nl <- list(\n  c(\"A\", \"B\"),\n  c(\"C\", \"D\", \"E\"), \n  \"F\"\n)\n\nl\n#> [[1]]\n#> [1] \"A\" \"B\"\n#> \n#> [[2]]\n#> [1] \"C\" \"D\" \"E\"\n#> \n#> [[3]]\n#> [1] \"F\"\n```\n:::\n\n\n\nLet's say we want to find out the length of each element in our list.\nMany R functions are vectorized, so we might think to simply use\n`length()` on our list:\n\n::: column-margin\nVectorized functions automatically operate on each individual element of\na vector. For instance, `nchar()` can count characters in a single\nstring or for each string in a vector of strings.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(l)\n#> [1] 3\n```\n:::\n\n\n\nHowever, this gives us the length of our *input* list `l`, which isn't\nwhat we want. Instead, we can use purrr to **map** the `length()`\nfunction to each element in our list `l`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)\n\n# For each element in `l`, apply the `length()` function\nmap(l, length)\n#> [[1]]\n#> [1] 2\n#> \n#> [[2]]\n#> [1] 3\n#> \n#> [[3]]\n#> [1] 1\n```\n:::\n\n\n\n::: callout-tip\n## Anonymous function syntax\n\nWe can also provide the function in R's anonymous function syntax, which\nwe introduced\n[previously](../2024-04-15-chirts-metrics/index.html#scaling-up). For\ninstance:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap(l, function(x) length(x))\n\n# Or, for R 4.1+\nmap(l, \\(x) length(x))\n```\n:::\n\n\n\nIf you want to modify other arguments of the function being mapped,\nyou'll need to use anonymous function syntax instead of the function\nname itself.\n:::\n\nWe see that `map()` returns a **list** containing the result we get when\nwe apply the `length()` function to each individual element in `l`.\n\nWe can do a similar process with our raster tiles. For each tile code,\nwe want to detect the files that contain that tile code in their name.\nThis way, we can group those files together.\n\nHere, we use `str_detect()` to detect the presence of the input\n`tile_code` string in our set of files. For each tile code, we'll get a\nset of logical values indicating the position of the files that contain\nthat code.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For each tile code, identify the file indexes that contain that code\nmap(\n  tile_codes,\n  function(code) str_detect(files, code)\n)\n#> [[1]]\n#> [1]  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n#> \n#> [[2]]\n#> [1] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE\n#> \n#> [[3]]\n#> [1] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE\n#> \n#> [[4]]\n#> [1] FALSE FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE\n```\n:::\n\n\n\nWe can use these logical values to subset the file names that correspond\nto each tile code. Remember that we'll have multiple files for each tile\nbecause we're working with multiple timestamps:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntiles <- map(\n  tile_codes,\n  function(code) files[str_detect(files, code)]\n)\n\ntiles\n#> [[1]]\n#> [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v08.061.2021246163834.hdf\"\n#> [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v08.061.2021260011612.hdf\"\n#> \n#> [[2]]\n#> [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h21v09.061.2021246163703.hdf\"\n#> [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h21v09.061.2021260011337.hdf\"\n#> \n#> [[3]]\n#> [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v08.061.2021246164307.hdf\"\n#> [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v08.061.2021260023343.hdf\"\n#> \n#> [[4]]\n#> [1] \"data_local/MOD13Q1/MOD13Q1.A2014001.h22v09.061.2021246155251.hdf\"\n#> [2] \"data_local/MOD13Q1/MOD13Q1.A2014209.h22v09.061.2021260023455.hdf\"\n```\n:::\n\n\n\n<!-- :::{.callout-note} -->\n\n<!-- purrr syntax takes a little bit of getting used to, but it ultimately can make -->\n\n<!-- it much easier to do recurring operations across a large set of inputs. -->\n\n<!-- In the long run, learning to add iteration to your data processing workflows  -->\n\n<!-- will save you time and make your processing scripts more robust. -->\n\n<!-- ::: -->\n\nAt this point we can load each of our tiles into a separate raster stack\nusing terra's `rast()`. Each of these files contains several layers with\ndifferent measures. We can peek at the layers by loading the first file.\nWe're only interested in the layer of NDVI values, which we see is the\nfirst layer:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(rast(files[1]))\n#>  [1] \"\\\"250m 16 days NDVI\\\"\"                     \n#>  [2] \"\\\"250m 16 days EVI\\\"\"                      \n#>  [3] \"\\\"250m 16 days VI Quality\\\"\"               \n#>  [4] \"\\\"250m 16 days red reflectance\\\"\"          \n#>  [5] \"\\\"250m 16 days NIR reflectance\\\"\"          \n#>  [6] \"\\\"250m 16 days blue reflectance\\\"\"         \n#>  [7] \"\\\"250m 16 days MIR reflectance\\\"\"          \n#>  [8] \"\\\"250m 16 days view zenith angle\\\"\"        \n#>  [9] \"\\\"250m 16 days sun zenith angle\\\"\"         \n#> [10] \"\\\"250m 16 days relative azimuth angle\\\"\"   \n#> [11] \"\\\"250m 16 days composite day of the year\\\"\"\n#> [12] \"\\\"250m 16 days pixel reliability\\\"\"\n```\n:::\n\n\n\nBecause data at this resolution can be memory-intensive, we can load\nonly the layer of NDVI data by using the `lyrs` argument to `rast()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrast(files[1], lyrs = 1)\n#> class       : SpatRaster \n#> dimensions  : 4800, 4800, 1  (nrow, ncol, nlyr)\n#> resolution  : 231.6564, 231.6564  (x, y)\n#> extent      : 3335852, 4447802, 0, 1111951  (xmin, xmax, ymin, ymax)\n#> coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#> source      : MOD13Q1.A2014001.h21v08.061.2021246163834.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI \n#> varname     : MOD13Q1.A2014001.h21v08.061.2021246163834 \n#> name        : \"250m 16 days NDVI\"\n```\n:::\n\n\n\nHowever, we'll be loading multiple files (one for each timestamp) into\neach raster stack for each tile. Therefore, we have to select the first\nlayer *of each raster file*. The `lyrs` argument considers all the files\ntogether when loading them. That is, `lyrs = 1` doesn't refer to the\nfirst layer of *each* file. Instead, it will extract the very first\nlayer of the first file only.\n\nFortunately, each file is organized identically, so we know that NDVI\nwill always be the first layer. Because there are 12 layers in each\nfile, we just need to load every 12th layer. We can easily build a\nsequence that contains the correct layer numbers:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Maximum layer value will be the first layer in the very last file being loaded\nmax_lyrs <- 12 * length(tiles[[1]])\n\n# Create sequence in increments of 12 up to the maximum layer index needed\nndvi_layers <- seq(1, max_lyrs, by = 12)\n\nndvi_layers\n#> [1]  1 13\n```\n:::\n\n\n\nNow, we can again `map()` over our file groups to load each set of\nfiles, selecting out only the NDVI layers:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load NDVI layers for each set of files corresponding to a particular tile\nndvi_tiles <- map(\n  tiles, \n  function(x) rast(x, lyrs = ndvi_layers)\n)\n```\n:::\n\n\n\nWe see that we now have a list of 4 separate raster stacks, because we\nloaded each entry in `tiles` separately:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nndvi_tiles\n#> [[1]]\n#> class       : SpatRaster \n#> dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#> resolution  : 231.6564, 231.6564  (x, y)\n#> extent      : 3335852, 4447802, 0, 1111951  (xmin, xmax, ymin, ymax)\n#> coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#> sources     : MOD13Q1.A2014001.h21v08.061.2021246163834.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#>               MOD13Q1.A2014209.h21v08.061.2021260011612.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#> varnames    : MOD13Q1.A2014001.h21v08.061.2021246163834 \n#>               MOD13Q1.A2014209.h21v08.061.2021260011612 \n#> names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#> \n#> [[2]]\n#> class       : SpatRaster \n#> dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#> resolution  : 231.6564, 231.6564  (x, y)\n#> extent      : 3335852, 4447802, -1111951, 0  (xmin, xmax, ymin, ymax)\n#> coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#> sources     : MOD13Q1.A2014001.h21v09.061.2021246163703.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#>               MOD13Q1.A2014209.h21v09.061.2021260011337.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#> varnames    : MOD13Q1.A2014001.h21v09.061.2021246163703 \n#>               MOD13Q1.A2014209.h21v09.061.2021260011337 \n#> names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#> \n#> [[3]]\n#> class       : SpatRaster \n#> dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#> resolution  : 231.6564, 231.6564  (x, y)\n#> extent      : 4447802, 5559753, 0, 1111951  (xmin, xmax, ymin, ymax)\n#> coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#> sources     : MOD13Q1.A2014001.h22v08.061.2021246164307.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#>               MOD13Q1.A2014209.h22v08.061.2021260023343.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#> varnames    : MOD13Q1.A2014001.h22v08.061.2021246164307 \n#>               MOD13Q1.A2014209.h22v08.061.2021260023343 \n#> names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#> \n#> [[4]]\n#> class       : SpatRaster \n#> dimensions  : 4800, 4800, 2  (nrow, ncol, nlyr)\n#> resolution  : 231.6564, 231.6564  (x, y)\n#> extent      : 4447802, 5559753, -1111951, 0  (xmin, xmax, ymin, ymax)\n#> coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#> sources     : MOD13Q1.A2014001.h22v09.061.2021246155251.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#>               MOD13Q1.A2014209.h22v09.061.2021260023455.hdf:MODIS_Grid_16DAY_250m_500m_VI:250m 16 days NDVI  \n#> varnames    : MOD13Q1.A2014001.h22v09.061.2021246155251 \n#>               MOD13Q1.A2014209.h22v09.061.2021260023455 \n#> names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\"\n```\n:::\n\n\n\nTo confirm that each raster stack comes from a different tile, we note\nthat each has a different spatial extent:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmap(ndvi_tiles, ext)\n#> [[1]]\n#> SpatExtent : 3335851.559, 4447802.078667, 0, 1111950.519667 (xmin, xmax, ymin, ymax)\n#> \n#> [[2]]\n#> SpatExtent : 3335851.559, 4447802.078667, -1111950.519667, 0 (xmin, xmax, ymin, ymax)\n#> \n#> [[3]]\n#> SpatExtent : 4447802.078667, 5559752.598333, 0, 1111950.519667 (xmin, xmax, ymin, ymax)\n#> \n#> [[4]]\n#> SpatExtent : 4447802.078667, 5559752.598333, -1111950.519667, 0 (xmin, xmax, ymin, ymax)\n```\n:::\n\n\n\n## Crop tiles\n\nNext, we'll crop our tiles to our area of interest (in this case, using\nthe Kenya national borders). This will reduce the size of our files\ngoing forward and speed up any subsequent processing.\n\nTo crop the tiles to our area of interest, we'll need country borders.\nWe'll load the integrated boundaries from IPUMS and dissolve them (with\n`st_union()`) to get the national border.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nke_borders <- ipumsr::read_ipums_sf(\"data/geo_ke1989_2014.zip\") |> \n  st_make_valid() |> # Fix minor border inconsistencies\n  st_union()\n```\n:::\n\n\n\nWe need to make sure our borders are in the same coordinate system as\nour raster before we can crop to its extent, so we'll extract the CRS\nfrom one of our rasters and use it with `st_transform()` to convert our\nborders' CRS. (We've introduced `st_transform()` [in the\npast](https://tech.popdata.org/dhs-research-hub/posts/2024-02-04-dhs-chirps/index.html#cluster-buffers).)\n\n::: callout-caution\n## Raster projection\n\nIn general, if you're working with both raster and vector data, it's\nbest to project the vector data to match your raster data rather than\nthe reverse.\n\nProjecting a raster distorts the cells, but because rasters necessarily\nare represented as a uniform grid, the values in the new cells must be\nresampled from the original grid, as the new cell locations may overlap\nwith multiple input cell locations. This adds an additional later of\nuncertainty to the data.\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Obtain CRS of NDVI raster data\nndvi_crs <- crs(ndvi_tiles[[1]])\n\n# Transform borders to same CRS as NDVI\nke_borders <- st_transform(ke_borders, crs = ndvi_crs)\n```\n:::\n\n\n\nNow, we can use our borders to crop each tile, removing the values\noutside of the border area. Note that we still need to use `map()`\nbecause our rasters are still separate elements in the `ndvi_tiles`\nlist:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nndvi_tiles <- map(\n  ndvi_tiles, \n  function(x) crop(x, ke_borders)\n)\n```\n:::\n\n\n\nNow we're ready to join our tiles together!\n\n## Mosaic tiles\n\nWe can mosaic our cropped tiles together with terra's `mosaic()`\nfunction. Typically `mosaic()` only takes two rasters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Mosaic two of our tiles together\nndvi_mosaic <- mosaic(ndvi_tiles[[1]], ndvi_tiles[[3]])\n```\n:::\n\n\n\nHowever, we need to mosaic all 4 tiles to cover Kenya in its entirety:\n\n\n\n::: {.cell .column-page}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show plot code\"}\nndvi_pal <- list(\n  pal = c(\n    \"#fdfbdc\",\n    \"#f1f4b7\",\n    \"#d3ef9f\",\n    \"#a5da8d\",\n    \"#6cc275\",\n    \"#51a55b\",\n    \"#397e43\",\n    \"#2d673a\",\n    \"#1d472e\" \n  ),\n  values = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 1)\n)\n\nggplot() +\n  layer_spatial(ndvi_mosaic[[1]]) +\n  layer_spatial(st_simplify(ke_borders, dTolerance = 1000), fill = NA) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 100000000),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    title = \"NDVI: Kenya\",\n    subtitle = \"January 1-16, 2014\",\n    fill = \"NDVI\",\n    caption = \"Source: NASA MOD13Q1\"\n  ) +\n  theme_dhs_map()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-32-1.png){width=100%}\n:::\n:::\n\n\n\nWe could manually mosaic all the tiles together, but purrr actually\nprovides a more efficient solution. The `reduce()` function allows us to\ncollapse a set of objects stored in a list by repeatedly applying the\nsame operation to each pair of list elements.\n\nThat is, `reduce()` allows us to mosaic the first two entries in our\n`ndvi_tiles` list, then mosaic that output with the next entry in the\nlist, and so on. We simply need to provide the list of our raster tiles\nand the function that we want to use to collapse the list. In this case,\nwe want to `mosaic()` the elements in the list together into a single\nraster output.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nke_ndvi <- reduce(ndvi_tiles, mosaic)\n```\n:::\n\n\n\nNext, we'll rescale the NDVI values, which should range from -1 to 1:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nke_ndvi <- ke_ndvi / 100000000\n```\n:::\n\n::: {.cell .column-page}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show plot code\"}\nggplot() +\n  layer_spatial(ke_ndvi[[1]]) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    title = \"NDVI: Kenya\",\n    subtitle = \"January 1-16, 2014\",\n    fill = \"NDVI\",\n    caption = \"Source: NASA MOD13Q1\"\n  ) +\n  theme_dhs_map()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-35-1.png){width=100%}\n:::\n:::\n\n\n\nNotice that no time information is attached to our output raster:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntime(ke_ndvi)\n#> [1] NA NA\n```\n:::\n\n\n\nThe time information is stored in a `A0000000` format string in the file\nname. The first 4 digits after the `A` contain the data year, and the\nnext 3 contain the day of the year.\n\nWe can again use a regular expression to extract the timestamps using\nthis pattern:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract 7-digit sequence following an \"A\"\ntimestamps <- unique(stringr::str_extract(files, \"(?<=A)[0-9]{7}\"))\n\ntimestamps\n#> [1] \"2014001\" \"2014209\"\n```\n:::\n\n\n\n::: column-margin\n`\"(?<=A)\"` is referred to as a *lookbehind assertion*. It will match\nelements of a string only if they follow the text in the assertion (in\nthis case, `\"A\"`). For more about assertions, check out this\n[cheatsheet](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_expressions/Cheatsheet#other_assertions).\n:::\n\nThese timestamps are in year + day of year format. We can parse this\nformat using `{lubridate}`, using `\"yj\"` format.\n\n::: column-margin\n`<div class=\"hex\">\n<a href=\"https://lubridate.tidyverse.org/index.html\" class=\"hex\">\n<img src=\"../../images/hex/lubridate.png\" class=\"hex-img\"/>\n</a>\n<p class=\"hex-cap\">© Garrett Grolemund, Hadley Wickham (GPL &gt;= 2)</p>\n</div>`{=html}\n:::\n\n::: column-margin\nLearn more about the different codes used to specify time formats\n[here](https://rdrr.io/r/base/strptime.html).\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parse time format and attach to our output raster\ntime(ke_ndvi) <- unique(lubridate::parse_date_time(timestamps, \"yj\"))\n```\n:::\n\n\n\n::: callout-caution\nNote that by manually setting the time after combining our tiles\ntogether, we may miss the possibility that certain tiles may have been\nrecorded at a different time than others. MODIS data are fairly reliable\ntime-wise, so these files do all share the same timestamps, but when\ncombining files, it's always worth confirming that each raster was\nrecorded for the expected time range.\n:::\n\nNow, we finally have a full set of NDVI data for our area of interest.\nWe have full spatial coverage for Kenya, and we've got our timestamps\nrepresented in distinct layers of our raster:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nke_ndvi\n#> class       : SpatRaster \n#> dimensions  : 4662, 3795, 2  (nrow, ncol, nlyr)\n#> resolution  : 231.6564, 231.6564  (x, y)\n#> extent      : 3769512, 4648648, -520300.2, 559681.8  (xmin, xmax, ymin, ymax)\n#> coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#> source      : spat_100574e7581d4_65623.tif \n#> names       : \"250m 16 days NDVI\", \"250m 16 days NDVI\" \n#> min values  :             -0.2000,             -0.2000 \n#> max values  :              0.9995,              0.9995 \n#> time        : 2014-01-01 to 2014-07-28 UTC\n```\n:::\n\n::: {.cell .column-page}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show plot code\"}\nlibrary(patchwork)\n#> \n#> Attaching package: 'patchwork'\n#> The following object is masked from 'package:terra':\n#> \n#>     area\n\nke_ndvi_mask <- mask(ke_ndvi, vect(ke_borders))\n\np1 <- ggplot() +\n  layer_spatial(ke_ndvi_mask[[1]]) +\n  layer_spatial(ke_borders, fill = NA) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    subtitle = \"January 1-16, 2014\",\n    fill = \"NDVI\"\n  ) +\n  theme_dhs_map(show_scale = FALSE) +\n  theme_dhs_base()\n\np2 <- ggplot() +\n  layer_spatial(ke_ndvi_mask[[2]]) +\n  layer_spatial(ke_borders, fill = NA) +\n  scale_fill_gradientn(\n    colors = ndvi_pal$pal,\n    values = ndvi_pal$values,\n    limits = c(0, 1),\n    na.value = \"transparent\"\n  ) +\n  labs(\n    subtitle = \"July 28-August 12, 2014\",\n    fill = \"NDVI\"\n  ) +\n  theme_dhs_map() +\n  theme_dhs_base()\n\np1 + p2 +\n  plot_layout(guides = \"collect\", ncol = 2) +\n  plot_annotation(\n    title = \"Seasonal NDVI Comparison: Kenya\",\n    caption = \"Source: NASA MOD13Q1\"\n  ) &\n  theme(legend.position='bottom')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/listing-img-1.png){width=100%}\n:::\n:::\n\n\n\n# Up next\n\nIn this post, we've introduced some approaches to build generalized\nworkflows when loading more complicated raster data structures. Using\ntools like regular expressions and iteration help you produce code that\nis robust when your data or analyses change. The up-front time\ninvestment required for learning these tools often pays off in the long\nrun, especially if you frequently use similar data analysis approaches.\n\nAs we can see in our maps above, NDVI differs significantly across the\nyear. In our next technical post, we'll take a closer look at this idea\nof seasonality and consider ways we can adapt our previous work with\nother environmental metrics to incorporate seasons more explicitly.\n\n## Getting Help {.appendix}\n\nQuestions or comments? Check out the [IPUMS User\nForum](https://forum.ipums.org) or reach out to IPUMS User Support at\nipums\\@umn.edu.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}